diff -Nur -X diffexclude a/drivers/staging/android/ion/Kconfig b/drivers/staging/android/ion/Kconfig
--- a/drivers/staging/android/ion/Kconfig	2019-11-24 15:17:01.000000000 +0800
+++ b/drivers/staging/android/ion/Kconfig	2023-06-09 12:02:57.288923383 +0800
@@ -1,27 +1,10 @@
-# SPDX-License-Identifier: GPL-2.0
-menuconfig ION
-	bool "Ion Memory Manager"
-	depends on HAS_DMA && MMU
+config VIDEO_SUNXI_CEDAR_ION
+	tristate "Allwinner CedarX Ion Driver"
+	depends on ARCH_SUNXI && OF_ADDRESS && HAS_DMA && MMU
 	select GENERIC_ALLOCATOR
 	select DMA_SHARED_BUFFER
+	select CMA
+	select OF_RESERVED_MEM
+	select DMA_CMA
 	help
-	  Choose this option to enable the ION Memory Manager,
-	  used by Android to efficiently allocate buffers
-	  from userspace that can be shared between drivers.
-	  If you're not using Android its probably safe to
-	  say N here.
-
-config ION_SYSTEM_HEAP
-	bool "Ion system heap"
-	depends on ION
-	help
-	  Choose this option to enable the Ion system heap. The system heap
-	  is backed by pages from the buddy allocator. If in doubt, say Y.
-
-config ION_CMA_HEAP
-	bool "Ion CMA heap support"
-	depends on ION && DMA_CMA
-	help
-	  Choose this option to enable CMA heaps with Ion. This heap is backed
-	  by the Contiguous Memory Allocator (CMA). If your system has these
-	  regions, you should say Y here.
+	  Allwinner libcdc compatible Ion driver.
diff -Nur -X diffexclude a/drivers/staging/android/ion/Makefile b/drivers/staging/android/ion/Makefile
--- a/drivers/staging/android/ion/Makefile	2019-11-24 15:17:01.000000000 +0800
+++ b/drivers/staging/android/ion/Makefile	2023-06-09 12:02:57.288923383 +0800
@@ -1,4 +1,8 @@
-# SPDX-License-Identifier: GPL-2.0
-obj-$(CONFIG_ION) += ion.o ion_heap.o
-obj-$(CONFIG_ION_SYSTEM_HEAP) += ion_system_heap.o ion_page_pool.o
-obj-$(CONFIG_ION_CMA_HEAP) += ion_cma_heap.o
+obj-$(CONFIG_VIDEO_SUNXI_CEDAR_ION) +=	ion.o ion-ioctl.o ion_heap.o \
+			ion_page_pool.o ion_system_heap.o \
+			ion_carveout_heap.o ion_chunk_heap.o ion_cma_heap.o \
+			ion_of.o
+ifdef CONFIG_COMPAT
+obj-$(CONFIG_VIDEO_SUNXI_CEDAR_ION) += compat_ion.o
+endif
+obj-$(CONFIG_VIDEO_SUNXI_CEDAR_ION) += sunxi/
\ No newline at end of file
diff -Nur -X diffexclude a/drivers/staging/android/ion/compat_ion.c b/drivers/staging/android/ion/compat_ion.c
--- a/drivers/staging/android/ion/compat_ion.c	1970-01-01 08:00:00.000000000 +0800
+++ b/drivers/staging/android/ion/compat_ion.c	2023-06-09 12:02:57.288923383 +0800
@@ -0,0 +1,195 @@
+/*
+ * drivers/staging/android/ion/compat_ion.c
+ *
+ * Copyright (C) 2013 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#include <linux/compat.h>
+#include <linux/fs.h>
+#include <linux/uaccess.h>
+
+#include "ion.h"
+#include "compat_ion.h"
+
+/* See drivers/staging/android/uapi/ion.h for the definition of these structs */
+struct compat_ion_allocation_data {
+	compat_size_t len;
+	compat_size_t align;
+	compat_uint_t heap_id_mask;
+	compat_uint_t flags;
+	compat_int_t handle;
+};
+
+struct compat_ion_custom_data {
+	compat_uint_t cmd;
+	compat_ulong_t arg;
+};
+
+struct compat_ion_handle_data {
+	compat_int_t handle;
+};
+
+#define COMPAT_ION_IOC_ALLOC	_IOWR(ION_IOC_MAGIC, 0, \
+				      struct compat_ion_allocation_data)
+#define COMPAT_ION_IOC_FREE	_IOWR(ION_IOC_MAGIC, 1, \
+				      struct compat_ion_handle_data)
+#define COMPAT_ION_IOC_CUSTOM	_IOWR(ION_IOC_MAGIC, 6, \
+				      struct compat_ion_custom_data)
+
+static int compat_get_ion_allocation_data(
+			struct compat_ion_allocation_data __user *data32,
+			struct ion_allocation_data __user *data)
+{
+	compat_size_t s;
+	compat_uint_t u;
+	compat_int_t i;
+	int err;
+
+	err = get_user(s, &data32->len);
+	err |= put_user(s, &data->len);
+	err |= get_user(s, &data32->align);
+	err |= put_user(s, &data->align);
+	err |= get_user(u, &data32->heap_id_mask);
+	err |= put_user(u, &data->heap_id_mask);
+	err |= get_user(u, &data32->flags);
+	err |= put_user(u, &data->flags);
+	err |= get_user(i, &data32->handle);
+	err |= put_user(i, &data->handle);
+
+	return err;
+}
+
+static int compat_get_ion_handle_data(
+			struct compat_ion_handle_data __user *data32,
+			struct ion_handle_data __user *data)
+{
+	compat_int_t i;
+	int err;
+
+	err = get_user(i, &data32->handle);
+	err |= put_user(i, &data->handle);
+
+	return err;
+}
+
+static int compat_put_ion_allocation_data(
+			struct compat_ion_allocation_data __user *data32,
+			struct ion_allocation_data __user *data)
+{
+	compat_size_t s;
+	compat_uint_t u;
+	compat_int_t i;
+	int err;
+
+	err = get_user(s, &data->len);
+	err |= put_user(s, &data32->len);
+	err |= get_user(s, &data->align);
+	err |= put_user(s, &data32->align);
+	err |= get_user(u, &data->heap_id_mask);
+	err |= put_user(u, &data32->heap_id_mask);
+	err |= get_user(u, &data->flags);
+	err |= put_user(u, &data32->flags);
+	err |= get_user(i, &data->handle);
+	err |= put_user(i, &data32->handle);
+
+	return err;
+}
+
+static int compat_get_ion_custom_data(
+			struct compat_ion_custom_data __user *data32,
+			struct ion_custom_data __user *data)
+{
+	compat_uint_t cmd;
+	compat_ulong_t arg;
+	int err;
+
+	err = get_user(cmd, &data32->cmd);
+	err |= put_user(cmd, &data->cmd);
+	err |= get_user(arg, &data32->arg);
+	err |= put_user(arg, &data->arg);
+
+	return err;
+};
+
+long compat_ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	long ret;
+
+	if (!filp->f_op->unlocked_ioctl)
+		return -ENOTTY;
+
+	switch (cmd) {
+	case COMPAT_ION_IOC_ALLOC:
+	{
+		struct compat_ion_allocation_data __user *data32;
+		struct ion_allocation_data __user *data;
+		int err;
+
+		data32 = compat_ptr(arg);
+		data = compat_alloc_user_space(sizeof(*data));
+		if (!data)
+			return -EFAULT;
+
+		err = compat_get_ion_allocation_data(data32, data);
+		if (err)
+			return err;
+		ret = filp->f_op->unlocked_ioctl(filp, ION_IOC_ALLOC,
+							(unsigned long)data);
+		err = compat_put_ion_allocation_data(data32, data);
+		return ret ? ret : err;
+	}
+	case COMPAT_ION_IOC_FREE:
+	{
+		struct compat_ion_handle_data __user *data32;
+		struct ion_handle_data __user *data;
+		int err;
+
+		data32 = compat_ptr(arg);
+		data = compat_alloc_user_space(sizeof(*data));
+		if (!data)
+			return -EFAULT;
+
+		err = compat_get_ion_handle_data(data32, data);
+		if (err)
+			return err;
+
+		return filp->f_op->unlocked_ioctl(filp, ION_IOC_FREE,
+							(unsigned long)data);
+	}
+	case COMPAT_ION_IOC_CUSTOM: {
+		struct compat_ion_custom_data __user *data32;
+		struct ion_custom_data __user *data;
+		int err;
+
+		data32 = compat_ptr(arg);
+		data = compat_alloc_user_space(sizeof(*data));
+		if (!data)
+			return -EFAULT;
+
+		err = compat_get_ion_custom_data(data32, data);
+		if (err)
+			return err;
+
+		return filp->f_op->unlocked_ioctl(filp, ION_IOC_CUSTOM,
+							(unsigned long)data);
+	}
+	case ION_IOC_SHARE:
+	case ION_IOC_MAP:
+	case ION_IOC_IMPORT:
+	case ION_IOC_SYNC:
+		return filp->f_op->unlocked_ioctl(filp, cmd,
+						(unsigned long)compat_ptr(arg));
+	default:
+		return -ENOIOCTLCMD;
+	}
+}
diff -Nur -X diffexclude a/drivers/staging/android/ion/compat_ion.h b/drivers/staging/android/ion/compat_ion.h
--- a/drivers/staging/android/ion/compat_ion.h	1970-01-01 08:00:00.000000000 +0800
+++ b/drivers/staging/android/ion/compat_ion.h	2023-06-09 12:02:57.288923383 +0800
@@ -0,0 +1,29 @@
+/*
+ * drivers/staging/android/ion/compat_ion.h
+ *
+ * Copyright (C) 2013 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#ifndef _LINUX_COMPAT_ION_H
+#define _LINUX_COMPAT_ION_H
+
+#if IS_ENABLED(CONFIG_COMPAT)
+
+long compat_ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);
+
+#else
+
+#define compat_ion_ioctl  NULL
+
+#endif /* CONFIG_COMPAT */
+#endif /* _LINUX_COMPAT_ION_H */
diff -Nur -X diffexclude a/drivers/staging/android/ion/dma-contiguous.h b/drivers/staging/android/ion/dma-contiguous.h
--- a/drivers/staging/android/ion/dma-contiguous.h	1970-01-01 08:00:00.000000000 +0800
+++ b/drivers/staging/android/ion/dma-contiguous.h	2023-06-09 12:02:57.288923383 +0800
@@ -0,0 +1,176 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+#ifndef __LINUX_CMA_H
+#define __LINUX_CMA_H
+
+/*
+ * Contiguous Memory Allocator for DMA mapping framework
+ * Copyright (c) 2010-2011 by Samsung Electronics.
+ * Written by:
+ *	Marek Szyprowski <m.szyprowski@samsung.com>
+ *	Michal Nazarewicz <mina86@mina86.com>
+ */
+
+/*
+ * Contiguous Memory Allocator
+ *
+ *   The Contiguous Memory Allocator (CMA) makes it possible to
+ *   allocate big contiguous chunks of memory after the system has
+ *   booted.
+ *
+ * Why is it needed?
+ *
+ *   Various devices on embedded systems have no scatter-getter and/or
+ *   IO map support and require contiguous blocks of memory to
+ *   operate.  They include devices such as cameras, hardware video
+ *   coders, etc.
+ *
+ *   Such devices often require big memory buffers (a full HD frame
+ *   is, for instance, more then 2 mega pixels large, i.e. more than 6
+ *   MB of memory), which makes mechanisms such as kmalloc() or
+ *   alloc_page() ineffective.
+ *
+ *   At the same time, a solution where a big memory region is
+ *   reserved for a device is suboptimal since often more memory is
+ *   reserved then strictly required and, moreover, the memory is
+ *   inaccessible to page system even if device drivers don't use it.
+ *
+ *   CMA tries to solve this issue by operating on memory regions
+ *   where only movable pages can be allocated from.  This way, kernel
+ *   can use the memory for pagecache and when device driver requests
+ *   it, allocated pages can be migrated.
+ *
+ * Driver usage
+ *
+ *   CMA should not be used by the device drivers directly. It is
+ *   only a helper framework for dma-mapping subsystem.
+ *
+ *   For more information, see kernel-docs in kernel/dma/contiguous.c
+ */
+
+#ifdef __KERNEL__
+
+#include <linux/device.h>
+#include <linux/mm.h>
+
+struct cma;
+struct page;
+
+#ifdef CONFIG_DMA_CMA
+
+extern struct cma *dma_contiguous_default_area;
+
+static inline struct cma *dev_get_cma_area(struct device *dev)
+{
+	if (dev && dev->cma_area)
+		return dev->cma_area;
+	return dma_contiguous_default_area;
+}
+
+static inline void dev_set_cma_area(struct device *dev, struct cma *cma)
+{
+	if (dev)
+		dev->cma_area = cma;
+}
+
+static inline void dma_contiguous_set_default(struct cma *cma)
+{
+	dma_contiguous_default_area = cma;
+}
+
+void dma_contiguous_reserve(phys_addr_t addr_limit);
+
+int __init dma_contiguous_reserve_area(phys_addr_t size, phys_addr_t base,
+				       phys_addr_t limit, struct cma **res_cma,
+				       bool fixed);
+
+/**
+ * dma_declare_contiguous() - reserve area for contiguous memory handling
+ *			      for particular device
+ * @dev:   Pointer to device structure.
+ * @size:  Size of the reserved memory.
+ * @base:  Start address of the reserved memory (optional, 0 for any).
+ * @limit: End address of the reserved memory (optional, 0 for any).
+ *
+ * This function reserves memory for specified device. It should be
+ * called by board specific code when early allocator (memblock or bootmem)
+ * is still activate.
+ */
+
+static inline int dma_declare_contiguous(struct device *dev, phys_addr_t size,
+					 phys_addr_t base, phys_addr_t limit)
+{
+	struct cma *cma;
+	int ret;
+	ret = dma_contiguous_reserve_area(size, base, limit, &cma, true);
+	if (ret == 0)
+		dev_set_cma_area(dev, cma);
+
+	return ret;
+}
+
+struct page *dma_alloc_from_contiguous(struct device *dev, size_t count,
+				       unsigned int order, bool no_warn);
+bool dma_release_from_contiguous(struct device *dev, struct page *pages,
+				 int count);
+struct page *dma_alloc_contiguous(struct device *dev, size_t size, gfp_t gfp);
+void dma_free_contiguous(struct device *dev, struct page *page, size_t size);
+
+#else
+
+static inline struct cma *dev_get_cma_area(struct device *dev)
+{
+	return NULL;
+}
+
+static inline void dev_set_cma_area(struct device *dev, struct cma *cma) { }
+
+static inline void dma_contiguous_set_default(struct cma *cma) { }
+
+static inline void dma_contiguous_reserve(phys_addr_t limit) { }
+
+static inline int dma_contiguous_reserve_area(phys_addr_t size, phys_addr_t base,
+				       phys_addr_t limit, struct cma **res_cma,
+				       bool fixed)
+{
+	return -ENOSYS;
+}
+
+static inline
+int dma_declare_contiguous(struct device *dev, phys_addr_t size,
+			   phys_addr_t base, phys_addr_t limit)
+{
+	return -ENOSYS;
+}
+
+static inline
+struct page *dma_alloc_from_contiguous(struct device *dev, size_t count,
+				       unsigned int order, bool no_warn)
+{
+	return NULL;
+}
+
+static inline
+bool dma_release_from_contiguous(struct device *dev, struct page *pages,
+				 int count)
+{
+	return false;
+}
+
+/* Use fallback alloc() and free() when CONFIG_DMA_CMA=n */
+static inline struct page *dma_alloc_contiguous(struct device *dev, size_t size,
+		gfp_t gfp)
+{
+	return NULL;
+}
+
+static inline void dma_free_contiguous(struct device *dev, struct page *page,
+		size_t size)
+{
+	__free_pages(page, get_order(size));
+}
+
+#endif
+
+#endif
+
+#endif
diff -Nur -X diffexclude a/drivers/staging/android/ion/ion-ioctl.c b/drivers/staging/android/ion/ion-ioctl.c
--- a/drivers/staging/android/ion/ion-ioctl.c	1970-01-01 08:00:00.000000000 +0800
+++ b/drivers/staging/android/ion/ion-ioctl.c	2023-06-09 12:02:57.288923383 +0800
@@ -0,0 +1,190 @@
+/*
+ *
+ * Copyright (C) 2011 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/file.h>
+#include <linux/fs.h>
+#include <linux/uaccess.h>
+
+#include "ion.h"
+#include "ion_priv.h"
+#include "compat_ion.h"
+
+union ion_ioctl_arg {
+	struct ion_fd_data fd;
+	struct ion_allocation_data allocation;
+	struct ion_handle_data handle;
+	struct ion_custom_data custom;
+	struct ion_heap_query query;
+};
+
+static int validate_ioctl_arg(unsigned int cmd, union ion_ioctl_arg *arg)
+{
+	int ret = 0;
+
+	switch (cmd) {
+	case ION_IOC_HEAP_QUERY:
+		ret = arg->query.reserved0 != 0;
+		ret |= arg->query.reserved1 != 0;
+		ret |= arg->query.reserved2 != 0;
+		break;
+	default:
+		break;
+	}
+
+	return ret ? -EINVAL : 0;
+}
+
+/* fix up the cases where the ioctl direction bits are incorrect */
+static unsigned int ion_ioctl_dir(unsigned int cmd)
+{
+	switch (cmd) {
+	case ION_IOC_SYNC:
+	case ION_IOC_FREE:
+	case ION_IOC_CUSTOM:
+		return _IOC_WRITE;
+	default:
+		return _IOC_DIR(cmd);
+	}
+}
+
+long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	struct ion_client *client = filp->private_data;
+	struct ion_device *dev = client->dev;
+	struct ion_handle *cleanup_handle = NULL;
+	int ret = 0;
+	unsigned int dir;
+	union ion_ioctl_arg data;
+
+	dir = ion_ioctl_dir(cmd);
+
+	if (_IOC_SIZE(cmd) > sizeof(data))
+		return -EINVAL;
+
+	/*
+	 * The copy_from_user is unconditional here for both read and write
+	 * to do the validate. If there is no write for the ioctl, the
+	 * buffer is cleared
+	 */
+	if (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))
+		return -EFAULT;
+
+	ret = validate_ioctl_arg(cmd, &data);
+	if (ret) {
+		pr_warn_once("%s: ioctl validate failed\n", __func__);
+		return ret;
+	}
+
+	if (!(dir & _IOC_WRITE))
+		memset(&data, 0, sizeof(data));
+
+	switch (cmd) {
+	case ION_IOC_ALLOC:
+	{
+		struct ion_handle *handle;
+
+		handle = ion_alloc(client, data.allocation.len,
+						data.allocation.align,
+						data.allocation.heap_id_mask,
+						data.allocation.flags);
+		if (IS_ERR(handle))
+			return PTR_ERR(handle);
+
+		data.allocation.handle = handle->id;
+
+		cleanup_handle = handle;
+		break;
+	}
+	case ION_IOC_FREE:
+	{
+		struct ion_handle *handle;
+
+		mutex_lock(&client->lock);
+		handle = ion_handle_get_by_id_nolock(client, data.handle.handle);
+		if (IS_ERR(handle)) {
+			mutex_unlock(&client->lock);
+			return PTR_ERR(handle);
+		}
+		ion_free_nolock(client, handle);
+		ion_handle_put_nolock(handle);
+		mutex_unlock(&client->lock);
+		break;
+	}
+	case ION_IOC_SHARE:
+	case ION_IOC_MAP:
+	{
+		struct ion_handle *handle;
+
+		mutex_lock(&client->lock);
+		handle = ion_handle_get_by_id_nolock(client, data.handle.handle);
+		if (IS_ERR(handle)) {
+			mutex_unlock(&client->lock);
+			return PTR_ERR(handle);
+		}
+		data.fd.fd = ion_share_dma_buf_fd_nolock(client, handle);
+		ion_handle_put_nolock(handle);
+		mutex_unlock(&client->lock);
+		if (data.fd.fd < 0)
+			ret = data.fd.fd;
+		break;
+	}
+	case ION_IOC_IMPORT:
+	{
+		struct ion_handle *handle;
+
+		handle = ion_import_dma_buf_fd(client, data.fd.fd);
+		if (IS_ERR(handle))
+			ret = PTR_ERR(handle);
+		else
+			data.handle.handle = handle->id;
+		break;
+	}
+	case ION_IOC_SYNC:
+	{
+		ret = ion_sync_for_device(client, data.fd.fd);
+		break;
+	}
+	case ION_IOC_CUSTOM:
+	{
+		if (!dev->custom_ioctl)
+			return -ENOTTY;
+		ret = dev->custom_ioctl(client, data.custom.cmd,
+						data.custom.arg);
+		break;
+	}
+	case ION_IOC_HEAP_QUERY:
+		ret = ion_query_heaps(client, &data.query);
+		break;
+
+	case 5: // Stupid Allwinner libcdc
+		if (!dev->custom_ioctl)
+			return -ENOTTY;
+		ret = dev->custom_ioctl(client, cmd, arg);
+		break;
+
+	default:
+		return -ENOTTY;
+	}
+
+	if (dir & _IOC_READ) {
+		if (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {
+			if (cleanup_handle)
+				ion_free(client, cleanup_handle);
+			return -EFAULT;
+		}
+	}
+	return ret;
+}
diff -Nur -X diffexclude a/drivers/staging/android/ion/ion.c b/drivers/staging/android/ion/ion.c
--- a/drivers/staging/android/ion/ion.c	2019-11-24 15:17:01.000000000 +0800
+++ b/drivers/staging/android/ion/ion.c	2023-06-09 12:02:57.288923383 +0800
@@ -1,33 +1,78 @@
-// SPDX-License-Identifier: GPL-2.0
 /*
- * ION Memory Allocator
+ *
+ * drivers/staging/android/ion/ion.c
  *
  * Copyright (C) 2011 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
  */
 
-#include <linux/debugfs.h>
+#include <linux/atomic.h>
 #include <linux/device.h>
-#include <linux/dma-buf.h>
 #include <linux/err.h>
-#include <linux/export.h>
 #include <linux/file.h>
 #include <linux/freezer.h>
 #include <linux/fs.h>
+#include <linux/anon_inodes.h>
 #include <linux/kthread.h>
 #include <linux/list.h>
+#include <linux/memblock.h>
 #include <linux/miscdevice.h>
+#include <linux/export.h>
 #include <linux/mm.h>
 #include <linux/mm_types.h>
 #include <linux/rbtree.h>
-#include <linux/sched/task.h>
 #include <linux/slab.h>
+#include <linux/seq_file.h>
 #include <linux/uaccess.h>
 #include <linux/vmalloc.h>
+#include <linux/debugfs.h>
+#include <linux/dma-buf.h>
+#include <linux/idr.h>
+#include <linux/sched/task.h>
 
 #include "ion.h"
+#include "ion_priv.h"
+#include "compat_ion.h"
 
-static struct ion_device *internal_dev;
-static int heap_id;
+bool ion_buffer_fault_user_mappings(struct ion_buffer *buffer)
+{
+	return (buffer->flags & ION_FLAG_CACHED) &&
+		!(buffer->flags & ION_FLAG_CACHED_NEEDS_SYNC);
+}
+
+bool ion_buffer_cached(struct ion_buffer *buffer)
+{
+	return !!(buffer->flags & ION_FLAG_CACHED);
+}
+
+static inline struct page *ion_buffer_page(struct page *page)
+{
+	return (struct page *)((unsigned long)page & ~(1UL));
+}
+
+static inline bool ion_buffer_page_is_dirty(struct page *page)
+{
+	return !!((unsigned long)page & 1UL);
+}
+
+static inline void ion_buffer_page_dirty(struct page **page)
+{
+	*page = (struct page *)((unsigned long)(*page) | 1UL);
+}
+
+static inline void ion_buffer_page_clean(struct page **page)
+{
+	*page = (struct page *)((unsigned long)(*page) & ~(1UL));
+}
 
 /* this function should only be called while dev->lock is held */
 static void ion_buffer_add(struct ion_device *dev,
@@ -59,10 +104,13 @@
 static struct ion_buffer *ion_buffer_create(struct ion_heap *heap,
 					    struct ion_device *dev,
 					    unsigned long len,
+					    unsigned long align,
 					    unsigned long flags)
 {
 	struct ion_buffer *buffer;
-	int ret;
+	struct sg_table *table;
+	struct scatterlist *sg;
+	int i, ret;
 
 	buffer = kzalloc(sizeof(*buffer), GFP_KERNEL);
 	if (!buffer)
@@ -70,36 +118,68 @@
 
 	buffer->heap = heap;
 	buffer->flags = flags;
-	buffer->dev = dev;
-	buffer->size = len;
+	kref_init(&buffer->ref);
 
-	ret = heap->ops->allocate(heap, buffer, len, flags);
+	ret = heap->ops->allocate(heap, buffer, len, align, flags);
 
 	if (ret) {
 		if (!(heap->flags & ION_HEAP_FLAG_DEFER_FREE))
 			goto err2;
 
 		ion_heap_freelist_drain(heap, 0);
-		ret = heap->ops->allocate(heap, buffer, len, flags);
+		ret = heap->ops->allocate(heap, buffer, len, align,
+					  flags);
 		if (ret)
 			goto err2;
 	}
 
-	if (!buffer->sg_table) {
+	if (buffer->sg_table == NULL) {
 		WARN_ONCE(1, "This heap needs to set the sgtable");
 		ret = -EINVAL;
 		goto err1;
 	}
 
-	spin_lock(&heap->stat_lock);
-	heap->num_of_buffers++;
-	heap->num_of_alloc_bytes += len;
-	if (heap->num_of_alloc_bytes > heap->alloc_bytes_wm)
-		heap->alloc_bytes_wm = heap->num_of_alloc_bytes;
-	spin_unlock(&heap->stat_lock);
+	table = buffer->sg_table;
+	buffer->dev = dev;
+	buffer->size = len;
+
+	if (ion_buffer_fault_user_mappings(buffer)) {
+		int num_pages = PAGE_ALIGN(buffer->size) / PAGE_SIZE;
+		struct scatterlist *sg;
+		int i, j, k = 0;
+
+		buffer->pages = vmalloc(sizeof(struct page *) * num_pages);
+		if (!buffer->pages) {
+			ret = -ENOMEM;
+			goto err1;
+		}
+
+		for_each_sg(table->sgl, sg, table->nents, i) {
+			struct page *page = sg_page(sg);
 
-	INIT_LIST_HEAD(&buffer->attachments);
+			for (j = 0; j < sg->length / PAGE_SIZE; j++)
+				buffer->pages[k++] = page++;
+		}
+	}
+
+	buffer->dev = dev;
+	buffer->size = len;
+	INIT_LIST_HEAD(&buffer->vmas);
 	mutex_init(&buffer->lock);
+	/*
+	 * this will set up dma addresses for the sglist -- it is not
+	 * technically correct as per the dma api -- a specific
+	 * device isn't really taking ownership here.  However, in practice on
+	 * our systems the only dma_address space is physical addresses.
+	 * Additionally, we can't afford the overhead of invalidating every
+	 * allocation via dma_map_sg. The implicit contract here is that
+	 * memory coming from the heaps is ready for dma, ie if it has a
+	 * cached mapping that mapping has been invalidated
+	 */
+	for_each_sg(buffer->sg_table->sgl, sg, buffer->sg_table->nents, i) {
+		sg_dma_address(sg) = sg_phys(sg);
+		sg_dma_len(sg) = sg->length;
+	}
 	mutex_lock(&dev->buffer_lock);
 	ion_buffer_add(dev, buffer);
 	mutex_unlock(&dev->buffer_lock);
@@ -120,16 +200,13 @@
 		buffer->heap->ops->unmap_kernel(buffer->heap, buffer);
 	}
 	buffer->heap->ops->free(buffer);
-	spin_lock(&buffer->heap->stat_lock);
-	buffer->heap->num_of_buffers--;
-	buffer->heap->num_of_alloc_bytes -= buffer->size;
-	spin_unlock(&buffer->heap->stat_lock);
-
+	vfree(buffer->pages);
 	kfree(buffer);
 }
 
-static void _ion_buffer_destroy(struct ion_buffer *buffer)
+static void _ion_buffer_destroy(struct kref *kref)
 {
+	struct ion_buffer *buffer = container_of(kref, struct ion_buffer, ref);
 	struct ion_heap *heap = buffer->heap;
 	struct ion_device *dev = buffer->dev;
 
@@ -143,6 +220,271 @@
 		ion_buffer_destroy(buffer);
 }
 
+static void ion_buffer_get(struct ion_buffer *buffer)
+{
+	kref_get(&buffer->ref);
+}
+
+static int ion_buffer_put(struct ion_buffer *buffer)
+{
+	return kref_put(&buffer->ref, _ion_buffer_destroy);
+}
+
+static void ion_buffer_add_to_handle(struct ion_buffer *buffer)
+{
+	mutex_lock(&buffer->lock);
+	buffer->handle_count++;
+	mutex_unlock(&buffer->lock);
+}
+
+static void ion_buffer_remove_from_handle(struct ion_buffer *buffer)
+{
+	/*
+	 * when a buffer is removed from a handle, if it is not in
+	 * any other handles, copy the taskcomm and the pid of the
+	 * process it's being removed from into the buffer.  At this
+	 * point there will be no way to track what processes this buffer is
+	 * being used by, it only exists as a dma_buf file descriptor.
+	 * The taskcomm and pid can provide a debug hint as to where this fd
+	 * is in the system
+	 */
+	mutex_lock(&buffer->lock);
+	buffer->handle_count--;
+	BUG_ON(buffer->handle_count < 0);
+	if (!buffer->handle_count) {
+		struct task_struct *task;
+
+		task = current->group_leader;
+		get_task_comm(buffer->task_comm, task);
+		buffer->pid = task_pid_nr(task);
+	}
+	mutex_unlock(&buffer->lock);
+}
+
+static struct ion_handle *ion_handle_create(struct ion_client *client,
+					    struct ion_buffer *buffer)
+{
+	struct ion_handle *handle;
+
+	handle = kzalloc(sizeof(*handle), GFP_KERNEL);
+	if (!handle)
+		return ERR_PTR(-ENOMEM);
+	kref_init(&handle->ref);
+	RB_CLEAR_NODE(&handle->node);
+	handle->client = client;
+	ion_buffer_get(buffer);
+	ion_buffer_add_to_handle(buffer);
+	handle->buffer = buffer;
+
+	return handle;
+}
+
+static void ion_handle_kmap_put(struct ion_handle *);
+
+static void ion_handle_destroy(struct kref *kref)
+{
+	struct ion_handle *handle = container_of(kref, struct ion_handle, ref);
+	struct ion_client *client = handle->client;
+	struct ion_buffer *buffer = handle->buffer;
+
+	mutex_lock(&buffer->lock);
+	while (handle->kmap_cnt)
+		ion_handle_kmap_put(handle);
+	mutex_unlock(&buffer->lock);
+
+	idr_remove(&client->idr, handle->id);
+	if (!RB_EMPTY_NODE(&handle->node))
+		rb_erase(&handle->node, &client->handles);
+
+	ion_buffer_remove_from_handle(buffer);
+	ion_buffer_put(buffer);
+
+	kfree(handle);
+}
+
+static void ion_handle_get(struct ion_handle *handle)
+{
+	kref_get(&handle->ref);
+}
+
+/* Must hold the client lock */
+static struct ion_handle *ion_handle_get_check_overflow(
+					struct ion_handle *handle)
+{
+	if (atomic_read(&handle->ref.refcount.refs) + 1 == 0)
+		return ERR_PTR(-EOVERFLOW);
+	ion_handle_get(handle);
+	return handle;
+}
+
+int ion_handle_put_nolock(struct ion_handle *handle)
+{
+	return kref_put(&handle->ref, ion_handle_destroy);
+}
+
+int ion_handle_put(struct ion_handle *handle)
+{
+	struct ion_client *client = handle->client;
+	int ret;
+
+	mutex_lock(&client->lock);
+	ret = ion_handle_put_nolock(handle);
+	mutex_unlock(&client->lock);
+
+	return ret;
+}
+
+static struct ion_handle *ion_handle_lookup(struct ion_client *client,
+					    struct ion_buffer *buffer)
+{
+	struct rb_node *n = client->handles.rb_node;
+
+	while (n) {
+		struct ion_handle *entry = rb_entry(n, struct ion_handle, node);
+
+		if (buffer < entry->buffer)
+			n = n->rb_left;
+		else if (buffer > entry->buffer)
+			n = n->rb_right;
+		else
+			return entry;
+	}
+	return ERR_PTR(-EINVAL);
+}
+
+struct ion_handle *ion_handle_get_by_id_nolock(struct ion_client *client,
+					       int id)
+{
+	struct ion_handle *handle;
+
+	handle = idr_find(&client->idr, id);
+	if (handle)
+		return ion_handle_get_check_overflow(handle);
+
+	return ERR_PTR(-EINVAL);
+}
+
+static bool ion_handle_validate(struct ion_client *client,
+				struct ion_handle *handle)
+{
+	WARN_ON(!mutex_is_locked(&client->lock));
+	return idr_find(&client->idr, handle->id) == handle;
+}
+
+static int ion_handle_add(struct ion_client *client, struct ion_handle *handle)
+{
+	int id;
+	struct rb_node **p = &client->handles.rb_node;
+	struct rb_node *parent = NULL;
+	struct ion_handle *entry;
+
+	id = idr_alloc(&client->idr, handle, 1, 0, GFP_KERNEL);
+	if (id < 0)
+		return id;
+
+	handle->id = id;
+
+	while (*p) {
+		parent = *p;
+		entry = rb_entry(parent, struct ion_handle, node);
+
+		if (handle->buffer < entry->buffer)
+			p = &(*p)->rb_left;
+		else if (handle->buffer > entry->buffer)
+			p = &(*p)->rb_right;
+		else
+			WARN(1, "%s: buffer already found.", __func__);
+	}
+
+	rb_link_node(&handle->node, parent, p);
+	rb_insert_color(&handle->node, &client->handles);
+
+	return 0;
+}
+
+struct ion_handle *ion_alloc(struct ion_client *client, size_t len,
+			     size_t align, unsigned int heap_id_mask,
+			     unsigned int flags)
+{
+	struct ion_handle *handle;
+	struct ion_device *dev = client->dev;
+	struct ion_buffer *buffer = NULL;
+	struct ion_heap *heap;
+	int ret;
+
+	pr_debug("%s: len %zu align %zu heap_id_mask %u flags %x\n", __func__,
+		 len, align, heap_id_mask, flags);
+	/*
+	 * traverse the list of heaps available in this system in priority
+	 * order.  If the heap type is supported by the client, and matches the
+	 * request of the caller allocate from it.  Repeat until allocate has
+	 * succeeded or all heaps have been tried
+	 */
+	len = PAGE_ALIGN(len);
+
+	if (!len)
+		return ERR_PTR(-EINVAL);
+
+	down_read(&dev->lock);
+	plist_for_each_entry(heap, &dev->heaps, node) {
+		/* if the caller didn't specify this heap id */
+		if (!((1 << heap->id) & heap_id_mask))
+			continue;
+		buffer = ion_buffer_create(heap, dev, len, align, flags);
+		if (!IS_ERR(buffer))
+			break;
+	}
+	up_read(&dev->lock);
+
+	if (buffer == NULL)
+		return ERR_PTR(-ENODEV);
+
+	if (IS_ERR(buffer))
+		return ERR_CAST(buffer);
+
+	handle = ion_handle_create(client, buffer);
+
+	/*
+	 * ion_buffer_create will create a buffer with a ref_cnt of 1,
+	 * and ion_handle_create will take a second reference, drop one here
+	 */
+	ion_buffer_put(buffer);
+
+	if (IS_ERR(handle))
+		return handle;
+
+	mutex_lock(&client->lock);
+	ret = ion_handle_add(client, handle);
+	mutex_unlock(&client->lock);
+	if (ret) {
+		ion_handle_put(handle);
+		handle = ERR_PTR(ret);
+	}
+
+	return handle;
+}
+EXPORT_SYMBOL(ion_alloc);
+
+void ion_free_nolock(struct ion_client *client,
+		     struct ion_handle *handle)
+{
+	if (!ion_handle_validate(client, handle)) {
+		WARN(1, "%s: invalid handle passed to free.\n", __func__);
+		return;
+	}
+	ion_handle_put_nolock(handle);
+}
+
+void ion_free(struct ion_client *client, struct ion_handle *handle)
+{
+	BUG_ON(client != handle->client);
+
+	mutex_lock(&client->lock);
+	ion_free_nolock(client, handle);
+	mutex_unlock(&client->lock);
+}
+EXPORT_SYMBOL(ion_free);
+
 static void *ion_buffer_kmap_get(struct ion_buffer *buffer)
 {
 	void *vaddr;
@@ -152,7 +494,7 @@
 		return buffer->vaddr;
 	}
 	vaddr = buffer->heap->ops->map_kernel(buffer->heap, buffer);
-	if (WARN_ONCE(!vaddr,
+	if (WARN_ONCE(vaddr == NULL,
 		      "heap->ops->map_kernel should return ERR_PTR on error"))
 		return ERR_PTR(-EINVAL);
 	if (IS_ERR(vaddr))
@@ -162,6 +504,22 @@
 	return vaddr;
 }
 
+static void *ion_handle_kmap_get(struct ion_handle *handle)
+{
+	struct ion_buffer *buffer = handle->buffer;
+	void *vaddr;
+
+	if (handle->kmap_cnt) {
+		handle->kmap_cnt++;
+		return buffer->vaddr;
+	}
+	vaddr = ion_buffer_kmap_get(buffer);
+	if (IS_ERR(vaddr))
+		return vaddr;
+	handle->kmap_cnt++;
+	return vaddr;
+}
+
 static void ion_buffer_kmap_put(struct ion_buffer *buffer)
 {
 	buffer->kmap_cnt--;
@@ -171,110 +529,409 @@
 	}
 }
 
-static struct sg_table *dup_sg_table(struct sg_table *table)
+static void ion_handle_kmap_put(struct ion_handle *handle)
 {
-	struct sg_table *new_table;
-	int ret, i;
-	struct scatterlist *sg, *new_sg;
+	struct ion_buffer *buffer = handle->buffer;
 
-	new_table = kzalloc(sizeof(*new_table), GFP_KERNEL);
-	if (!new_table)
-		return ERR_PTR(-ENOMEM);
+	if (!handle->kmap_cnt) {
+		WARN(1, "%s: Double unmap detected! bailing...\n", __func__);
+		return;
+	}
+	handle->kmap_cnt--;
+	if (!handle->kmap_cnt)
+		ion_buffer_kmap_put(buffer);
+}
 
-	ret = sg_alloc_table(new_table, table->nents, GFP_KERNEL);
-	if (ret) {
-		kfree(new_table);
-		return ERR_PTR(-ENOMEM);
+void *ion_map_kernel(struct ion_client *client, struct ion_handle *handle)
+{
+	struct ion_buffer *buffer;
+	void *vaddr;
+
+	mutex_lock(&client->lock);
+	if (!ion_handle_validate(client, handle)) {
+		pr_err("%s: invalid handle passed to map_kernel.\n",
+		       __func__);
+		mutex_unlock(&client->lock);
+		return ERR_PTR(-EINVAL);
 	}
 
-	new_sg = new_table->sgl;
-	for_each_sg(table->sgl, sg, table->nents, i) {
-		memcpy(new_sg, sg, sizeof(*sg));
-		new_sg->dma_address = 0;
-		new_sg = sg_next(new_sg);
+	buffer = handle->buffer;
+
+	if (!handle->buffer->heap->ops->map_kernel) {
+		pr_err("%s: map_kernel is not implemented by this heap.\n",
+		       __func__);
+		mutex_unlock(&client->lock);
+		return ERR_PTR(-ENODEV);
 	}
 
-	return new_table;
+	mutex_lock(&buffer->lock);
+	vaddr = ion_handle_kmap_get(handle);
+	mutex_unlock(&buffer->lock);
+	mutex_unlock(&client->lock);
+	return vaddr;
 }
+EXPORT_SYMBOL(ion_map_kernel);
 
-static void free_duped_table(struct sg_table *table)
+void ion_unmap_kernel(struct ion_client *client, struct ion_handle *handle)
 {
-	sg_free_table(table);
-	kfree(table);
+	struct ion_buffer *buffer;
+
+	mutex_lock(&client->lock);
+	buffer = handle->buffer;
+	mutex_lock(&buffer->lock);
+	ion_handle_kmap_put(handle);
+	mutex_unlock(&buffer->lock);
+	mutex_unlock(&client->lock);
 }
+EXPORT_SYMBOL(ion_unmap_kernel);
 
-struct ion_dma_buf_attachment {
-	struct device *dev;
-	struct sg_table *table;
-	struct list_head list;
+static struct mutex debugfs_mutex;
+static struct rb_root *ion_root_client;
+static int is_client_alive(struct ion_client *client)
+{
+	struct rb_node *node;
+	struct ion_client *tmp;
+	struct ion_device *dev;
+
+	node = ion_root_client->rb_node;
+	dev = container_of(ion_root_client, struct ion_device, clients);
+
+	down_read(&dev->lock);
+	while (node) {
+		tmp = rb_entry(node, struct ion_client, node);
+		if (client < tmp) {
+			node = node->rb_left;
+		} else if (client > tmp) {
+			node = node->rb_right;
+		} else {
+			up_read(&dev->lock);
+			return 1;
+		}
+	}
+
+	up_read(&dev->lock);
+	return 0;
+}
+
+static int ion_debug_client_show(struct seq_file *s, void *unused)
+{
+	struct ion_client *client = s->private;
+	struct rb_node *n;
+	size_t sizes[ION_NUM_HEAP_IDS] = {0};
+	const char *names[ION_NUM_HEAP_IDS] = {NULL};
+	int i;
+
+	mutex_lock(&debugfs_mutex);
+	if (!is_client_alive(client)) {
+		seq_printf(s, "ion_client 0x%p dead, can't dump its buffers\n",
+			   client);
+		mutex_unlock(&debugfs_mutex);
+		return 0;
+	}
+
+	mutex_lock(&client->lock);
+	for (n = rb_first(&client->handles); n; n = rb_next(n)) {
+		struct ion_handle *handle = rb_entry(n, struct ion_handle,
+						     node);
+		unsigned int id = handle->buffer->heap->id;
+
+		if (!names[id])
+			names[id] = handle->buffer->heap->name;
+		sizes[id] += handle->buffer->size;
+	}
+	mutex_unlock(&client->lock);
+	mutex_unlock(&debugfs_mutex);
+
+	seq_printf(s, "%16.16s: %16.16s\n", "heap_name", "size_in_bytes");
+	for (i = 0; i < ION_NUM_HEAP_IDS; i++) {
+		if (!names[i])
+			continue;
+		seq_printf(s, "%16.16s: %16zu\n", names[i], sizes[i]);
+	}
+	return 0;
+}
+
+static int ion_debug_client_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, ion_debug_client_show, inode->i_private);
+}
+
+static const struct file_operations debug_client_fops = {
+	.open = ion_debug_client_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
 };
 
-static int ion_dma_buf_attach(struct dma_buf *dmabuf,
-			      struct dma_buf_attachment *attachment)
+static int ion_get_client_serial(const struct rb_root *root,
+				 const unsigned char *name)
 {
-	struct ion_dma_buf_attachment *a;
-	struct sg_table *table;
-	struct ion_buffer *buffer = dmabuf->priv;
+	int serial = -1;
+	struct rb_node *node;
 
-	a = kzalloc(sizeof(*a), GFP_KERNEL);
-	if (!a)
-		return -ENOMEM;
+	for (node = rb_first(root); node; node = rb_next(node)) {
+		struct ion_client *client = rb_entry(node, struct ion_client,
+						     node);
 
-	table = dup_sg_table(buffer->sg_table);
-	if (IS_ERR(table)) {
-		kfree(a);
-		return -ENOMEM;
+		if (strcmp(client->name, name))
+			continue;
+		serial = max(serial, client->display_serial);
 	}
+	return serial + 1;
+}
 
-	a->table = table;
-	a->dev = attachment->dev;
-	INIT_LIST_HEAD(&a->list);
+struct ion_client *ion_client_create(struct ion_device *dev,
+				     const char *name)
+{
+	struct ion_client *client;
+	struct task_struct *task;
+	struct rb_node **p;
+	struct rb_node *parent = NULL;
+	struct ion_client *entry;
+	pid_t pid;
 
-	attachment->priv = a;
+	if (!name) {
+		pr_err("%s: Name cannot be null\n", __func__);
+		return ERR_PTR(-EINVAL);
+	}
 
-	mutex_lock(&buffer->lock);
-	list_add(&a->list, &buffer->attachments);
-	mutex_unlock(&buffer->lock);
+	get_task_struct(current->group_leader);
+	task_lock(current->group_leader);
+	pid = task_pid_nr(current->group_leader);
+	/*
+	 * don't bother to store task struct for kernel threads,
+	 * they can't be killed anyway
+	 */
+	if (current->group_leader->flags & PF_KTHREAD) {
+		put_task_struct(current->group_leader);
+		task = NULL;
+	} else {
+		task = current->group_leader;
+	}
+	task_unlock(current->group_leader);
+
+	client = kzalloc(sizeof(*client), GFP_KERNEL);
+	if (!client)
+		goto err_put_task_struct;
+
+	client->dev = dev;
+	client->handles = RB_ROOT;
+	idr_init(&client->idr);
+	mutex_init(&client->lock);
+	client->task = task;
+	client->pid = pid;
+	client->name = kstrdup(name, GFP_KERNEL);
+	if (!client->name)
+		goto err_free_client;
 
-	return 0;
+	down_write(&dev->lock);
+	client->display_serial = ion_get_client_serial(&dev->clients, name);
+	client->display_name = kasprintf(
+		GFP_KERNEL, "%s-%d", name, client->display_serial);
+	if (!client->display_name) {
+		up_write(&dev->lock);
+		goto err_free_client_name;
+	}
+	p = &dev->clients.rb_node;
+	while (*p) {
+		parent = *p;
+		entry = rb_entry(parent, struct ion_client, node);
+
+		if (client < entry)
+			p = &(*p)->rb_left;
+		else if (client > entry)
+			p = &(*p)->rb_right;
+	}
+	rb_link_node(&client->node, parent, p);
+	rb_insert_color(&client->node, &dev->clients);
+
+	client->debug_root = debugfs_create_file(client->display_name, 0664,
+						 dev->clients_debug_root,
+						 client, &debug_client_fops);
+	if (!client->debug_root) {
+		char buf[256], *path;
+
+		path = dentry_path(dev->clients_debug_root, buf, 256);
+		pr_err("Failed to create client debugfs at %s/%s\n",
+		       path, client->display_name);
+	}
+
+	up_write(&dev->lock);
+
+	return client;
+
+err_free_client_name:
+	kfree(client->name);
+err_free_client:
+	kfree(client);
+err_put_task_struct:
+	if (task)
+		put_task_struct(current->group_leader);
+	return ERR_PTR(-ENOMEM);
 }
+EXPORT_SYMBOL(ion_client_create);
 
-static void ion_dma_buf_detatch(struct dma_buf *dmabuf,
-				struct dma_buf_attachment *attachment)
+void ion_client_destroy(struct ion_client *client)
 {
-	struct ion_dma_buf_attachment *a = attachment->priv;
-	struct ion_buffer *buffer = dmabuf->priv;
+	struct ion_device *dev = client->dev;
+	struct rb_node *n;
 
-	mutex_lock(&buffer->lock);
-	list_del(&a->list);
-	mutex_unlock(&buffer->lock);
-	free_duped_table(a->table);
+	pr_debug("%s: %d\n", __func__, __LINE__);
+	mutex_lock(&debugfs_mutex);
+	while ((n = rb_first(&client->handles))) {
+		struct ion_handle *handle = rb_entry(n, struct ion_handle,
+						     node);
+		ion_handle_destroy(&handle->ref);
+	}
 
-	kfree(a);
-}
+	idr_destroy(&client->idr);
+
+	down_write(&dev->lock);
+	if (client->task)
+		put_task_struct(client->task);
+	rb_erase(&client->node, &dev->clients);
+	debugfs_remove_recursive(client->debug_root);
+	up_write(&dev->lock);
+
+	kfree(client->display_name);
+	kfree(client->name);
+	kfree(client);
+	mutex_unlock(&debugfs_mutex);
+}
+EXPORT_SYMBOL(ion_client_destroy);
+
+static void ion_buffer_sync_for_device(struct ion_buffer *buffer,
+				       struct device *dev,
+				       enum dma_data_direction direction);
 
 static struct sg_table *ion_map_dma_buf(struct dma_buf_attachment *attachment,
 					enum dma_data_direction direction)
 {
-	struct ion_dma_buf_attachment *a = attachment->priv;
-	struct sg_table *table;
-
-	table = a->table;
-
-	if (!dma_map_sg(attachment->dev, table->sgl, table->nents,
-			direction))
-		return ERR_PTR(-ENOMEM);
+	struct dma_buf *dmabuf = attachment->dmabuf;
+	struct ion_buffer *buffer = dmabuf->priv;
 
-	return table;
+	ion_buffer_sync_for_device(buffer, attachment->dev, direction);
+	return buffer->sg_table;
 }
 
 static void ion_unmap_dma_buf(struct dma_buf_attachment *attachment,
 			      struct sg_table *table,
 			      enum dma_data_direction direction)
 {
-	dma_unmap_sg(attachment->dev, table->sgl, table->nents, direction);
 }
 
+void ion_pages_sync_for_device(struct device *dev, struct page *page,
+			       size_t size, enum dma_data_direction dir)
+{
+	struct scatterlist sg;
+
+	sg_init_table(&sg, 1);
+	sg_set_page(&sg, page, size, 0);
+	/*
+	 * This is not correct - sg_dma_address needs a dma_addr_t that is valid
+	 * for the targeted device, but this works on the currently targeted
+	 * hardware.
+	 */
+	sg_dma_address(&sg) = page_to_phys(page);
+	dma_sync_sg_for_device(dev, &sg, 1, dir);
+}
+
+struct ion_vma_list {
+	struct list_head list;
+	struct vm_area_struct *vma;
+};
+
+static void ion_buffer_sync_for_device(struct ion_buffer *buffer,
+				       struct device *dev,
+				       enum dma_data_direction dir)
+{
+	struct ion_vma_list *vma_list;
+	int pages = PAGE_ALIGN(buffer->size) / PAGE_SIZE;
+	int i;
+
+	pr_debug("%s: syncing for device %s\n", __func__,
+		 dev ? dev_name(dev) : "null");
+
+	if (!ion_buffer_fault_user_mappings(buffer))
+		return;
+
+	mutex_lock(&buffer->lock);
+	for (i = 0; i < pages; i++) {
+		struct page *page = buffer->pages[i];
+
+		if (ion_buffer_page_is_dirty(page))
+			ion_pages_sync_for_device(dev, ion_buffer_page(page),
+						  PAGE_SIZE, dir);
+
+		ion_buffer_page_clean(buffer->pages + i);
+	}
+	list_for_each_entry(vma_list, &buffer->vmas, list) {
+		struct vm_area_struct *vma = vma_list->vma;
+
+		zap_page_range(vma, vma->vm_start, vma->vm_end - vma->vm_start);
+	}
+	mutex_unlock(&buffer->lock);
+}
+
+static vm_fault_t ion_vm_fault(struct vm_fault *vmf)
+{
+	struct vm_area_struct *vma = vmf->vma;
+	struct ion_buffer *buffer = vma->vm_private_data;
+	unsigned long pfn;
+	int ret;
+
+	mutex_lock(&buffer->lock);
+	ion_buffer_page_dirty(buffer->pages + vmf->pgoff);
+	BUG_ON(!buffer->pages || !buffer->pages[vmf->pgoff]);
+
+	pfn = page_to_pfn(ion_buffer_page(buffer->pages[vmf->pgoff]));
+	ret = vmf_insert_pfn(vma, (unsigned long)vmf->address, pfn);
+	mutex_unlock(&buffer->lock);
+	if (ret)
+		return VM_FAULT_ERROR;
+
+	return VM_FAULT_NOPAGE;
+}
+
+static void ion_vm_open(struct vm_area_struct *vma)
+{
+	struct ion_buffer *buffer = vma->vm_private_data;
+	struct ion_vma_list *vma_list;
+
+	vma_list = kmalloc(sizeof(*vma_list), GFP_KERNEL);
+	if (!vma_list)
+		return;
+	vma_list->vma = vma;
+	mutex_lock(&buffer->lock);
+	list_add(&vma_list->list, &buffer->vmas);
+	mutex_unlock(&buffer->lock);
+	pr_debug("%s: adding %p\n", __func__, vma);
+}
+
+static void ion_vm_close(struct vm_area_struct *vma)
+{
+	struct ion_buffer *buffer = vma->vm_private_data;
+	struct ion_vma_list *vma_list, *tmp;
+
+	pr_debug("%s\n", __func__);
+	mutex_lock(&buffer->lock);
+	list_for_each_entry_safe(vma_list, tmp, &buffer->vmas, list) {
+		if (vma_list->vma != vma)
+			continue;
+		list_del(&vma_list->list);
+		kfree(vma_list);
+		pr_debug("%s: deleting %p\n", __func__, vma);
+		break;
+	}
+	mutex_unlock(&buffer->lock);
+}
+
+static const struct vm_operations_struct ion_vma_ops = {
+	.open = ion_vm_open,
+	.close = ion_vm_close,
+	.fault = ion_vm_fault,
+};
+
 static int ion_mmap(struct dma_buf *dmabuf, struct vm_area_struct *vma)
 {
 	struct ion_buffer *buffer = dmabuf->priv;
@@ -286,6 +943,15 @@
 		return -EINVAL;
 	}
 
+	if (ion_buffer_fault_user_mappings(buffer)) {
+		vma->vm_flags |= VM_IO | VM_PFNMAP | VM_DONTEXPAND |
+							VM_DONTDUMP;
+		vma->vm_private_data = buffer;
+		vma->vm_ops = &ion_vma_ops;
+		ion_vm_open(vma);
+		return 0;
+	}
+
 	if (!(buffer->flags & ION_FLAG_CACHED))
 		vma->vm_page_prot = pgprot_writecombine(vma->vm_page_prot);
 
@@ -305,7 +971,7 @@
 {
 	struct ion_buffer *buffer = dmabuf->priv;
 
-	_ion_buffer_destroy(buffer);
+	ion_buffer_put(buffer);
 }
 
 static void *ion_dma_buf_kmap(struct dma_buf *dmabuf, unsigned long offset)
@@ -325,106 +991,64 @@
 {
 	struct ion_buffer *buffer = dmabuf->priv;
 	void *vaddr;
-	struct ion_dma_buf_attachment *a;
-	int ret = 0;
 
-	/*
-	 * TODO: Move this elsewhere because we don't always need a vaddr
-	 */
-	if (buffer->heap->ops->map_kernel) {
-		mutex_lock(&buffer->lock);
-		vaddr = ion_buffer_kmap_get(buffer);
-		if (IS_ERR(vaddr)) {
-			ret = PTR_ERR(vaddr);
-			goto unlock;
-		}
-		mutex_unlock(&buffer->lock);
+	if (!buffer->heap->ops->map_kernel) {
+		pr_err("%s: map kernel is not implemented by this heap.\n",
+		       __func__);
+		return -ENODEV;
 	}
 
 	mutex_lock(&buffer->lock);
-	list_for_each_entry(a, &buffer->attachments, list) {
-		dma_sync_sg_for_cpu(a->dev, a->table->sgl, a->table->nents,
-				    direction);
-	}
-
-unlock:
+	vaddr = ion_buffer_kmap_get(buffer);
 	mutex_unlock(&buffer->lock);
-	return ret;
+	return PTR_ERR_OR_ZERO(vaddr);
 }
 
 static int ion_dma_buf_end_cpu_access(struct dma_buf *dmabuf,
 				      enum dma_data_direction direction)
 {
 	struct ion_buffer *buffer = dmabuf->priv;
-	struct ion_dma_buf_attachment *a;
-
-	if (buffer->heap->ops->map_kernel) {
-		mutex_lock(&buffer->lock);
-		ion_buffer_kmap_put(buffer);
-		mutex_unlock(&buffer->lock);
-	}
 
 	mutex_lock(&buffer->lock);
-	list_for_each_entry(a, &buffer->attachments, list) {
-		dma_sync_sg_for_device(a->dev, a->table->sgl, a->table->nents,
-				       direction);
-	}
+	ion_buffer_kmap_put(buffer);
 	mutex_unlock(&buffer->lock);
 
 	return 0;
 }
 
-static const struct dma_buf_ops dma_buf_ops = {
+static struct dma_buf_ops dma_buf_ops = {
 	.map_dma_buf = ion_map_dma_buf,
 	.unmap_dma_buf = ion_unmap_dma_buf,
 	.mmap = ion_mmap,
 	.release = ion_dma_buf_release,
-	.attach = ion_dma_buf_attach,
-	.detach = ion_dma_buf_detatch,
 	.begin_cpu_access = ion_dma_buf_begin_cpu_access,
 	.end_cpu_access = ion_dma_buf_end_cpu_access,
-	.map = ion_dma_buf_kmap,
-	.unmap = ion_dma_buf_kunmap,
+	// .map = ion_dma_buf_kmap,
+	// .unmap = ion_dma_buf_kunmap,
 };
 
-static int ion_alloc(size_t len, unsigned int heap_id_mask, unsigned int flags)
+static struct dma_buf *__ion_share_dma_buf(struct ion_client *client,
+					   struct ion_handle *handle,
+					   bool lock_client)
 {
-	struct ion_device *dev = internal_dev;
-	struct ion_buffer *buffer = NULL;
-	struct ion_heap *heap;
 	DEFINE_DMA_BUF_EXPORT_INFO(exp_info);
-	int fd;
+	struct ion_buffer *buffer;
 	struct dma_buf *dmabuf;
+	bool valid_handle;
 
-	pr_debug("%s: len %zu heap_id_mask %u flags %x\n", __func__,
-		 len, heap_id_mask, flags);
-	/*
-	 * traverse the list of heaps available in this system in priority
-	 * order.  If the heap type is supported by the client, and matches the
-	 * request of the caller allocate from it.  Repeat until allocate has
-	 * succeeded or all heaps have been tried
-	 */
-	len = PAGE_ALIGN(len);
-
-	if (!len)
-		return -EINVAL;
-
-	down_read(&dev->lock);
-	plist_for_each_entry(heap, &dev->heaps, node) {
-		/* if the caller didn't specify this heap id */
-		if (!((1 << heap->id) & heap_id_mask))
-			continue;
-		buffer = ion_buffer_create(heap, dev, len, flags);
-		if (!IS_ERR(buffer))
-			break;
+	if (lock_client)
+		mutex_lock(&client->lock);
+	valid_handle = ion_handle_validate(client, handle);
+	if (!valid_handle) {
+		WARN(1, "%s: invalid handle passed to share.\n", __func__);
+		if (lock_client)
+			mutex_unlock(&client->lock);
+		return ERR_PTR(-EINVAL);
 	}
-	up_read(&dev->lock);
-
-	if (!buffer)
-		return -ENODEV;
-
-	if (IS_ERR(buffer))
-		return PTR_ERR(buffer);
+	buffer = handle->buffer;
+	ion_buffer_get(buffer);
+	if (lock_client)
+		mutex_unlock(&client->lock);
 
 	exp_info.ops = &dma_buf_ops;
 	exp_info.size = buffer->size;
@@ -433,10 +1057,30 @@
 
 	dmabuf = dma_buf_export(&exp_info);
 	if (IS_ERR(dmabuf)) {
-		_ion_buffer_destroy(buffer);
-		return PTR_ERR(dmabuf);
+		ion_buffer_put(buffer);
+		return dmabuf;
 	}
 
+	return dmabuf;
+}
+
+struct dma_buf *ion_share_dma_buf(struct ion_client *client,
+				  struct ion_handle *handle)
+{
+	return __ion_share_dma_buf(client, handle, true);
+}
+EXPORT_SYMBOL(ion_share_dma_buf);
+
+static int __ion_share_dma_buf_fd(struct ion_client *client,
+				  struct ion_handle *handle, bool lock_client)
+{
+	struct dma_buf *dmabuf;
+	int fd;
+
+	dmabuf = __ion_share_dma_buf(client, handle, lock_client);
+	if (IS_ERR(dmabuf))
+		return PTR_ERR(dmabuf);
+
 	fd = dma_buf_fd(dmabuf, O_CLOEXEC);
 	if (fd < 0)
 		dma_buf_put(dmabuf);
@@ -444,9 +1088,103 @@
 	return fd;
 }
 
-static int ion_query_heaps(struct ion_heap_query *query)
+int ion_share_dma_buf_fd(struct ion_client *client, struct ion_handle *handle)
+{
+	return __ion_share_dma_buf_fd(client, handle, true);
+}
+EXPORT_SYMBOL(ion_share_dma_buf_fd);
+
+int ion_share_dma_buf_fd_nolock(struct ion_client *client,
+				struct ion_handle *handle)
+{
+	return __ion_share_dma_buf_fd(client, handle, false);
+}
+
+struct ion_handle *ion_import_dma_buf(struct ion_client *client,
+				      struct dma_buf *dmabuf)
+{
+	struct ion_buffer *buffer;
+	struct ion_handle *handle;
+	int ret;
+
+	/* if this memory came from ion */
+
+	if (dmabuf->ops != &dma_buf_ops) {
+		pr_err("%s: can not import dmabuf from another exporter\n",
+		       __func__);
+		return ERR_PTR(-EINVAL);
+	}
+	buffer = dmabuf->priv;
+
+	mutex_lock(&client->lock);
+	/* if a handle exists for this buffer just take a reference to it */
+	handle = ion_handle_lookup(client, buffer);
+	if (!IS_ERR(handle)) {
+		handle = ion_handle_get_check_overflow(handle);
+		mutex_unlock(&client->lock);
+		goto end;
+	}
+
+	handle = ion_handle_create(client, buffer);
+	if (IS_ERR(handle)) {
+		mutex_unlock(&client->lock);
+		goto end;
+	}
+
+	ret = ion_handle_add(client, handle);
+	mutex_unlock(&client->lock);
+	if (ret) {
+		ion_handle_put(handle);
+		handle = ERR_PTR(ret);
+	}
+
+end:
+	return handle;
+}
+EXPORT_SYMBOL(ion_import_dma_buf);
+
+struct ion_handle *ion_import_dma_buf_fd(struct ion_client *client, int fd)
+{
+	struct dma_buf *dmabuf;
+	struct ion_handle *handle;
+
+	dmabuf = dma_buf_get(fd);
+	if (IS_ERR(dmabuf))
+		return ERR_CAST(dmabuf);
+
+	handle = ion_import_dma_buf(client, dmabuf);
+	dma_buf_put(dmabuf);
+	return handle;
+}
+EXPORT_SYMBOL(ion_import_dma_buf_fd);
+
+int ion_sync_for_device(struct ion_client *client, int fd)
+{
+	struct dma_buf *dmabuf;
+	struct ion_buffer *buffer;
+
+	dmabuf = dma_buf_get(fd);
+	if (IS_ERR(dmabuf))
+		return PTR_ERR(dmabuf);
+
+	/* if this memory came from ion */
+	if (dmabuf->ops != &dma_buf_ops) {
+		pr_err("%s: can not sync dmabuf from another exporter\n",
+		       __func__);
+		dma_buf_put(dmabuf);
+		return -EINVAL;
+	}
+	buffer = dmabuf->priv;
+
+	dma_sync_sg_for_device(NULL, buffer->sg_table->sgl,
+			       buffer->sg_table->nents, DMA_BIDIRECTIONAL);
+	dma_buf_put(dmabuf);
+	return 0;
+}
+
+int ion_query_heaps(struct ion_client *client, struct ion_heap_query *query)
 {
-	struct ion_device *dev = internal_dev;
+	struct ion_device *dev = client->dev;
 	struct ion_heap_data __user *buffer = u64_to_user_ptr(query->heaps);
 	int ret = -EINVAL, cnt = 0, max_cnt;
 	struct ion_heap *heap;
@@ -483,93 +1221,138 @@
 	}
 
 	query->cnt = cnt;
-	ret = 0;
 out:
 	up_read(&dev->lock);
 	return ret;
 }
 
-union ion_ioctl_arg {
-	struct ion_allocation_data allocation;
-	struct ion_heap_query query;
-};
-
-static int validate_ioctl_arg(unsigned int cmd, union ion_ioctl_arg *arg)
+static int ion_release(struct inode *inode, struct file *file)
 {
-	switch (cmd) {
-	case ION_IOC_HEAP_QUERY:
-		if (arg->query.reserved0 ||
-		    arg->query.reserved1 ||
-		    arg->query.reserved2)
-			return -EINVAL;
-		break;
-	default:
-		break;
-	}
+	struct ion_client *client = file->private_data;
 
+	pr_debug("%s: %d\n", __func__, __LINE__);
+	ion_client_destroy(client);
 	return 0;
 }
 
-static long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+static int ion_open(struct inode *inode, struct file *file)
 {
-	int ret = 0;
-	union ion_ioctl_arg data;
-
-	if (_IOC_SIZE(cmd) > sizeof(data))
-		return -EINVAL;
+	struct miscdevice *miscdev = file->private_data;
+	struct ion_device *dev = container_of(miscdev, struct ion_device, dev);
+	struct ion_client *client;
+	char debug_name[64];
 
-	/*
-	 * The copy_from_user is unconditional here for both read and write
-	 * to do the validate. If there is no write for the ioctl, the
-	 * buffer is cleared
-	 */
-	if (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))
-		return -EFAULT;
+	pr_debug("%s: %d\n", __func__, __LINE__);
+	snprintf(debug_name, 64, "%u", task_pid_nr(current->group_leader));
+	client = ion_client_create(dev, debug_name);
+	if (IS_ERR(client))
+		return PTR_ERR(client);
+	file->private_data = client;
 
-	ret = validate_ioctl_arg(cmd, &data);
-	if (ret) {
-		pr_warn_once("%s: ioctl validate failed\n", __func__);
-		return ret;
-	}
+	return 0;
+}
 
-	if (!(_IOC_DIR(cmd) & _IOC_WRITE))
-		memset(&data, 0, sizeof(data));
+static const struct file_operations ion_fops = {
+	.owner          = THIS_MODULE,
+	.open           = ion_open,
+	.release        = ion_release,
+	.unlocked_ioctl = ion_ioctl,
+	.compat_ioctl   = compat_ion_ioctl,
+};
 
-	switch (cmd) {
-	case ION_IOC_ALLOC:
-	{
-		int fd;
+static size_t ion_debug_heap_total(struct ion_client *client,
+				   unsigned int id)
+{
+	size_t size = 0;
+	struct rb_node *n;
 
-		fd = ion_alloc(data.allocation.len,
-			       data.allocation.heap_id_mask,
-			       data.allocation.flags);
-		if (fd < 0)
-			return fd;
+	mutex_lock(&client->lock);
+	for (n = rb_first(&client->handles); n; n = rb_next(n)) {
+		struct ion_handle *handle = rb_entry(n,
+						     struct ion_handle,
+						     node);
+		if (handle->buffer->heap->id == id)
+			size += handle->buffer->size;
+	}
+	mutex_unlock(&client->lock);
+	return size;
+}
+
+static int ion_debug_heap_show(struct seq_file *s, void *unused)
+{
+	struct ion_heap *heap = s->private;
+	struct ion_device *dev = heap->dev;
+	struct rb_node *n;
+	size_t total_size = 0;
+	size_t total_orphaned_size = 0;
+
+	seq_printf(s, "%16s %16s %16s\n", "client", "pid", "size");
+	seq_puts(s, "----------------------------------------------------\n");
+
+	mutex_lock(&debugfs_mutex);
+	for (n = rb_first(&dev->clients); n; n = rb_next(n)) {
+		struct ion_client *client = rb_entry(n, struct ion_client,
+						     node);
+		size_t size = ion_debug_heap_total(client, heap->id);
 
-		data.allocation.fd = fd;
+		if (!size)
+			continue;
+		if (client->task) {
+			char task_comm[TASK_COMM_LEN];
 
-		break;
-	}
-	case ION_IOC_HEAP_QUERY:
-		ret = ion_query_heaps(&data.query);
-		break;
-	default:
-		return -ENOTTY;
+			get_task_comm(task_comm, client->task);
+			seq_printf(s, "%16s %16u %16zu\n", task_comm,
+				   client->pid, size);
+		} else {
+			seq_printf(s, "%16s %16u %16zu\n", client->name,
+				   client->pid, size);
+		}
 	}
+	mutex_unlock(&debugfs_mutex);
 
-	if (_IOC_DIR(cmd) & _IOC_READ) {
-		if (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd)))
-			return -EFAULT;
+	seq_puts(s, "----------------------------------------------------\n");
+	seq_puts(s, "orphaned allocations (info is from last known client):\n");
+	mutex_lock(&dev->buffer_lock);
+	for (n = rb_first(&dev->buffers); n; n = rb_next(n)) {
+		struct ion_buffer *buffer = rb_entry(n, struct ion_buffer,
+						     node);
+		if (buffer->heap->id != heap->id)
+			continue;
+		total_size += buffer->size;
+		if (!buffer->handle_count) {
+			seq_printf(s, "%16s %16u %16zu %d %d\n",
+				   buffer->task_comm, buffer->pid,
+				   buffer->size, buffer->kmap_cnt,
+				   atomic_read(&buffer->ref.refcount.refs));
+			total_orphaned_size += buffer->size;
+		}
 	}
-	return ret;
+	mutex_unlock(&dev->buffer_lock);
+	seq_puts(s, "----------------------------------------------------\n");
+	seq_printf(s, "%16s %16zu\n", "total orphaned",
+		   total_orphaned_size);
+	seq_printf(s, "%16s %16zu\n", "total ", total_size);
+	if (heap->flags & ION_HEAP_FLAG_DEFER_FREE)
+		seq_printf(s, "%16s %16zu\n", "deferred free",
+			   heap->free_list_size);
+	seq_puts(s, "----------------------------------------------------\n");
+
+	if (heap->debug_show)
+		heap->debug_show(heap, s, unused);
+
+	return 0;
 }
 
-static const struct file_operations ion_fops = {
-	.owner          = THIS_MODULE,
-	.unlocked_ioctl = ion_ioctl,
-#ifdef CONFIG_COMPAT
-	.compat_ioctl	= ion_ioctl,
-#endif
+static int ion_debug_heap_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, ion_debug_heap_show, inode->i_private);
+}
+
+static const struct file_operations debug_heap_fops = {
+	.open = ion_debug_heap_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
 };
 
 static int debug_shrink_set(void *data, u64 val)
@@ -607,80 +1390,75 @@
 DEFINE_SIMPLE_ATTRIBUTE(debug_shrink_fops, debug_shrink_get,
 			debug_shrink_set, "%llu\n");
 
-void ion_device_add_heap(struct ion_heap *heap)
+void ion_device_add_heap(struct ion_device *dev, struct ion_heap *heap)
 {
-	struct ion_device *dev = internal_dev;
-	int ret;
-	struct dentry *heap_root;
-	char debug_name[64];
+	struct dentry *debug_file;
 
 	if (!heap->ops->allocate || !heap->ops->free)
 		pr_err("%s: can not add heap with invalid ops struct.\n",
 		       __func__);
 
 	spin_lock_init(&heap->free_lock);
-	spin_lock_init(&heap->stat_lock);
 	heap->free_list_size = 0;
 
 	if (heap->flags & ION_HEAP_FLAG_DEFER_FREE)
 		ion_heap_init_deferred_free(heap);
 
-	if ((heap->flags & ION_HEAP_FLAG_DEFER_FREE) || heap->ops->shrink) {
-		ret = ion_heap_init_shrinker(heap);
-		if (ret)
-			pr_err("%s: Failed to register shrinker\n", __func__);
-	}
+	if ((heap->flags & ION_HEAP_FLAG_DEFER_FREE) || heap->ops->shrink)
+		ion_heap_init_shrinker(heap);
 
 	heap->dev = dev;
-	heap->num_of_buffers = 0;
-	heap->num_of_alloc_bytes = 0;
-	heap->alloc_bytes_wm = 0;
-
-	heap_root = debugfs_create_dir(heap->name, dev->debug_root);
-	debugfs_create_u64("num_of_buffers",
-			   0444, heap_root,
-			   &heap->num_of_buffers);
-	debugfs_create_u64("num_of_alloc_bytes",
-			   0444,
-			   heap_root,
-			   &heap->num_of_alloc_bytes);
-	debugfs_create_u64("alloc_bytes_wm",
-			   0444,
-			   heap_root,
-			   &heap->alloc_bytes_wm);
-
-	if (heap->shrinker.count_objects &&
-	    heap->shrinker.scan_objects) {
-		snprintf(debug_name, 64, "%s_shrink", heap->name);
-		debugfs_create_file(debug_name,
-				    0644,
-				    heap_root,
-				    heap,
-				    &debug_shrink_fops);
-	}
-
 	down_write(&dev->lock);
-	heap->id = heap_id++;
 	/*
 	 * use negative heap->id to reverse the priority -- when traversing
 	 * the list later attempt higher id numbers first
 	 */
 	plist_node_init(&heap->node, -heap->id);
 	plist_add(&heap->node, &dev->heaps);
+	debug_file = debugfs_create_file(heap->name, 0664,
+					 dev->heaps_debug_root, heap,
+					 &debug_heap_fops);
+
+	if (!debug_file) {
+		char buf[256], *path;
+
+		path = dentry_path(dev->heaps_debug_root, buf, 256);
+		pr_err("Failed to create heap debugfs at %s/%s\n",
+		       path, heap->name);
+	}
+
+	if (heap->shrinker.count_objects && heap->shrinker.scan_objects) {
+		char debug_name[64];
+
+		snprintf(debug_name, 64, "%s_shrink", heap->name);
+		debug_file = debugfs_create_file(
+			debug_name, 0644, dev->heaps_debug_root, heap,
+			&debug_shrink_fops);
+		if (!debug_file) {
+			char buf[256], *path;
+
+			path = dentry_path(dev->heaps_debug_root, buf, 256);
+			pr_err("Failed to create heap shrinker debugfs at %s/%s\n",
+			       path, debug_name);
+		}
+	}
 
 	dev->heap_cnt++;
 	up_write(&dev->lock);
 }
 EXPORT_SYMBOL(ion_device_add_heap);
 
-static int ion_device_create(void)
+struct ion_device *ion_device_create(long (*custom_ioctl)
+				     (struct ion_client *client,
+				      unsigned int cmd,
+				      unsigned long arg))
 {
 	struct ion_device *idev;
 	int ret;
 
 	idev = kzalloc(sizeof(*idev), GFP_KERNEL);
 	if (!idev)
-		return -ENOMEM;
+		return ERR_PTR(-ENOMEM);
 
 	idev->dev.minor = MISC_DYNAMIC_MINOR;
 	idev->dev.name = "ion";
@@ -690,15 +1468,43 @@
 	if (ret) {
 		pr_err("ion: failed to register misc device.\n");
 		kfree(idev);
-		return ret;
+		return ERR_PTR(ret);
 	}
 
 	idev->debug_root = debugfs_create_dir("ion", NULL);
+	if (!idev->debug_root) {
+		pr_err("ion: failed to create debugfs root directory.\n");
+		goto debugfs_done;
+	}
+	idev->heaps_debug_root = debugfs_create_dir("heaps", idev->debug_root);
+	if (!idev->heaps_debug_root) {
+		pr_err("ion: failed to create debugfs heaps directory.\n");
+		goto debugfs_done;
+	}
+	idev->clients_debug_root = debugfs_create_dir("clients",
+						idev->debug_root);
+	if (!idev->clients_debug_root)
+		pr_err("ion: failed to create debugfs clients directory.\n");
+
+debugfs_done:
+
+	idev->custom_ioctl = custom_ioctl;
 	idev->buffers = RB_ROOT;
 	mutex_init(&idev->buffer_lock);
 	init_rwsem(&idev->lock);
 	plist_head_init(&idev->heaps);
-	internal_dev = idev;
-	return 0;
+	idev->clients = RB_ROOT;
+	ion_root_client = &idev->clients;
+	mutex_init(&debugfs_mutex);
+	return idev;
+}
+EXPORT_SYMBOL(ion_device_create);
+
+void ion_device_destroy(struct ion_device *dev)
+{
+	misc_deregister(&dev->dev);
+	debugfs_remove_recursive(dev->debug_root);
+	/* XXX need to free the heaps and clients ? */
+	kfree(dev);
 }
-subsys_initcall(ion_device_create);
+EXPORT_SYMBOL(ion_device_destroy);
diff -Nur -X diffexclude a/drivers/staging/android/ion/ion.h b/drivers/staging/android/ion/ion.h
--- a/drivers/staging/android/ion/ion.h	2019-11-24 15:17:01.000000000 +0800
+++ b/drivers/staging/android/ion/ion.h	2023-06-09 12:02:57.288923383 +0800
@@ -1,311 +1,176 @@
-/* SPDX-License-Identifier: GPL-2.0 */
 /*
- * ION Memory Allocator kernel interface header
+ * drivers/staging/android/ion/ion.h
  *
  * Copyright (C) 2011 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
  */
 
-#ifndef _ION_H
-#define _ION_H
+#ifndef _LINUX_ION_H
+#define _LINUX_ION_H
 
-#include <linux/device.h>
-#include <linux/dma-direction.h>
-#include <linux/kref.h>
-#include <linux/mm_types.h>
-#include <linux/mutex.h>
-#include <linux/rbtree.h>
-#include <linux/sched.h>
-#include <linux/shrinker.h>
 #include <linux/types.h>
-#include <linux/miscdevice.h>
-
-#include "../uapi/ion.h"
-
-/**
- * struct ion_buffer - metadata for a particular buffer
- * @node:		node in the ion_device buffers tree
- * @list:		element in list of deferred freeable buffers
- * @dev:		back pointer to the ion_device
- * @heap:		back pointer to the heap the buffer came from
- * @flags:		buffer specific flags
- * @private_flags:	internal buffer specific flags
- * @size:		size of the buffer
- * @priv_virt:		private data to the buffer representable as
- *			a void *
- * @lock:		protects the buffers cnt fields
- * @kmap_cnt:		number of times the buffer is mapped to the kernel
- * @vaddr:		the kernel mapping if kmap_cnt is not zero
- * @sg_table:		the sg table for the buffer
- * @attachments:	list of devices attached to this buffer
- */
-struct ion_buffer {
-	union {
-		struct rb_node node;
-		struct list_head list;
-	};
-	struct ion_device *dev;
-	struct ion_heap *heap;
-	unsigned long flags;
-	unsigned long private_flags;
-	size_t size;
-	void *priv_virt;
-	struct mutex lock;
-	int kmap_cnt;
-	void *vaddr;
-	struct sg_table *sg_table;
-	struct list_head attachments;
-};
 
-void ion_buffer_destroy(struct ion_buffer *buffer);
+#include "uapi/ion.h"
 
-/**
- * struct ion_device - the metadata of the ion device node
- * @dev:		the actual misc device
- * @buffers:		an rb tree of all the existing buffers
- * @buffer_lock:	lock protecting the tree of buffers
- * @lock:		rwsem protecting the tree of heaps and clients
- */
-struct ion_device {
-	struct miscdevice dev;
-	struct rb_root buffers;
-	struct mutex buffer_lock;
-	struct rw_semaphore lock;
-	struct plist_head heaps;
-	struct dentry *debug_root;
-	int heap_cnt;
-};
-
-/**
- * struct ion_heap_ops - ops to operate on a given heap
- * @allocate:		allocate memory
- * @free:		free memory
- * @map_kernel		map memory to the kernel
- * @unmap_kernel	unmap memory to the kernel
- * @map_user		map memory to userspace
- *
- * allocate, phys, and map_user return 0 on success, -errno on error.
- * map_dma and map_kernel return pointer on success, ERR_PTR on
- * error. @free will be called with ION_PRIV_FLAG_SHRINKER_FREE set in
- * the buffer's private_flags when called from a shrinker. In that
- * case, the pages being free'd must be truly free'd back to the
- * system, not put in a page pool or otherwise cached.
- */
-struct ion_heap_ops {
-	int (*allocate)(struct ion_heap *heap,
-			struct ion_buffer *buffer, unsigned long len,
-			unsigned long flags);
-	void (*free)(struct ion_buffer *buffer);
-	void * (*map_kernel)(struct ion_heap *heap, struct ion_buffer *buffer);
-	void (*unmap_kernel)(struct ion_heap *heap, struct ion_buffer *buffer);
-	int (*map_user)(struct ion_heap *mapper, struct ion_buffer *buffer,
-			struct vm_area_struct *vma);
-	int (*shrink)(struct ion_heap *heap, gfp_t gfp_mask, int nr_to_scan);
-};
-
-/**
- * heap flags - flags between the heaps and core ion code
- */
-#define ION_HEAP_FLAG_DEFER_FREE BIT(0)
+struct ion_handle;
+struct ion_device;
+struct ion_heap;
+struct ion_mapper;
+struct ion_client;
+struct ion_buffer;
 
-/**
- * private flags - flags internal to ion
- */
 /*
- * Buffer is being freed from a shrinker function. Skip any possible
- * heap-specific caching mechanism (e.g. page pools). Guarantees that
- * any buffer storage that came from the system allocator will be
- * returned to the system allocator.
- */
-#define ION_PRIV_FLAG_SHRINKER_FREE BIT(0)
-
-/**
- * struct ion_heap - represents a heap in the system
- * @node:		rb node to put the heap on the device's tree of heaps
- * @dev:		back pointer to the ion_device
- * @type:		type of heap
- * @ops:		ops struct as above
- * @flags:		flags
- * @id:			id of heap, also indicates priority of this heap when
- *			allocating.  These are specified by platform data and
- *			MUST be unique
- * @name:		used for debugging
- * @shrinker:		a shrinker for the heap
- * @free_list:		free list head if deferred free is used
- * @free_list_size	size of the deferred free list in bytes
- * @lock:		protects the free list
- * @waitqueue:		queue to wait on from deferred free thread
- * @task:		task struct of deferred free thread
- * @num_of_buffers	the number of currently allocated buffers
- * @num_of_alloc_bytes	the number of allocated bytes
- * @alloc_bytes_wm	the number of allocated bytes watermark
- *
- * Represents a pool of memory from which buffers can be made.  In some
- * systems the only heap is regular system memory allocated via vmalloc.
- * On others, some blocks might require large physically contiguous buffers
- * that are allocated from a specially reserved heap.
- */
-struct ion_heap {
-	struct plist_node node;
-	struct ion_device *dev;
+ * This should be removed some day when phys_addr_t's are fully
+ * plumbed in the kernel, and all instances of ion_phys_addr_t should
+ * be converted to phys_addr_t.  For the time being many kernel interfaces
+ * do not accept phys_addr_t's that would have to
+ */
+#define ion_phys_addr_t unsigned long
+
+/**
+ * struct ion_platform_heap - defines a heap in the given platform
+ * @type:	type of the heap from ion_heap_type enum
+ * @id:		unique identifier for heap.  When allocating higher numbers
+ *		will be allocated from first.  At allocation these are passed
+ *		as a bit mask and therefore can not exceed ION_NUM_HEAP_IDS.
+ * @name:	used for debug purposes
+ * @base:	base address of heap in physical memory if applicable
+ * @size:	size of the heap in bytes if applicable
+ * @align:	required alignment in physical memory if applicable
+ * @priv:	private info passed from the board file
+ *
+ * Provided by the board file.
+ */
+struct ion_platform_heap {
 	enum ion_heap_type type;
-	struct ion_heap_ops *ops;
-	unsigned long flags;
 	unsigned int id;
 	const char *name;
-
-	/* deferred free support */
-	struct shrinker shrinker;
-	struct list_head free_list;
-	size_t free_list_size;
-	spinlock_t free_lock;
-	wait_queue_head_t waitqueue;
-	struct task_struct *task;
-
-	/* heap statistics */
-	u64 num_of_buffers;
-	u64 num_of_alloc_bytes;
-	u64 alloc_bytes_wm;
-
-	/* protect heap statistics */
-	spinlock_t stat_lock;
+	ion_phys_addr_t base;
+	size_t size;
+	ion_phys_addr_t align;
+	void *priv;
 };
 
 /**
- * ion_device_add_heap - adds a heap to the ion device
- * @heap:		the heap to add
- */
-void ion_device_add_heap(struct ion_heap *heap);
+ * struct ion_platform_data - array of platform heaps passed from board file
+ * @nr:		number of structures in the array
+ * @heaps:	array of platform_heap structions
+ *
+ * Provided by the board file in the form of platform data to a platform device.
+ */
+struct ion_platform_data {
+	int nr;
+	struct ion_platform_heap *heaps;
+};
 
 /**
- * some helpers for common operations on buffers using the sg_table
- * and vaddr fields
+ * ion_client_create() -  allocate a client and returns it
+ * @dev:		the global ion device
+ * @name:		used for debugging
  */
-void *ion_heap_map_kernel(struct ion_heap *heap, struct ion_buffer *buffer);
-void ion_heap_unmap_kernel(struct ion_heap *heap, struct ion_buffer *buffer);
-int ion_heap_map_user(struct ion_heap *heap, struct ion_buffer *buffer,
-		      struct vm_area_struct *vma);
-int ion_heap_buffer_zero(struct ion_buffer *buffer);
-int ion_heap_pages_zero(struct page *page, size_t size, pgprot_t pgprot);
+struct ion_client *ion_client_create(struct ion_device *dev,
+				     const char *name);
 
 /**
- * ion_heap_init_shrinker
- * @heap:		the heap
+ * ion_client_destroy() -  free's a client and all it's handles
+ * @client:	the client
  *
- * If a heap sets the ION_HEAP_FLAG_DEFER_FREE flag or defines the shrink op
- * this function will be called to setup a shrinker to shrink the freelists
- * and call the heap's shrink op.
+ * Free the provided client and all it's resources including
+ * any handles it is holding.
  */
-int ion_heap_init_shrinker(struct ion_heap *heap);
+void ion_client_destroy(struct ion_client *client);
 
 /**
- * ion_heap_init_deferred_free -- initialize deferred free functionality
- * @heap:		the heap
+ * ion_alloc - allocate ion memory
+ * @client:		the client
+ * @len:		size of the allocation
+ * @align:		requested allocation alignment, lots of hardware blocks
+ *			have alignment requirements of some kind
+ * @heap_id_mask:	mask of heaps to allocate from, if multiple bits are set
+ *			heaps will be tried in order from highest to lowest
+ *			id
+ * @flags:		heap flags, the low 16 bits are consumed by ion, the
+ *			high 16 bits are passed on to the respective heap and
+ *			can be heap custom
  *
- * If a heap sets the ION_HEAP_FLAG_DEFER_FREE flag this function will
- * be called to setup deferred frees. Calls to free the buffer will
- * return immediately and the actual free will occur some time later
+ * Allocate memory in one of the heaps provided in heap mask and return
+ * an opaque handle to it.
  */
-int ion_heap_init_deferred_free(struct ion_heap *heap);
+struct ion_handle *ion_alloc(struct ion_client *client, size_t len,
+			     size_t align, unsigned int heap_id_mask,
+			     unsigned int flags);
 
 /**
- * ion_heap_freelist_add - add a buffer to the deferred free list
- * @heap:		the heap
- * @buffer:		the buffer
+ * ion_free - free a handle
+ * @client:	the client
+ * @handle:	the handle to free
  *
- * Adds an item to the deferred freelist.
+ * Free the provided handle.
  */
-void ion_heap_freelist_add(struct ion_heap *heap, struct ion_buffer *buffer);
+void ion_free(struct ion_client *client, struct ion_handle *handle);
 
 /**
- * ion_heap_freelist_drain - drain the deferred free list
- * @heap:		the heap
- * @size:		amount of memory to drain in bytes
+ * ion_map_kernel - create mapping for the given handle
+ * @client:	the client
+ * @handle:	handle to map
  *
- * Drains the indicated amount of memory from the deferred freelist immediately.
- * Returns the total amount freed.  The total freed may be higher depending
- * on the size of the items in the list, or lower if there is insufficient
- * total memory on the freelist.
+ * Map the given handle into the kernel and return a kernel address that
+ * can be used to access this address.
  */
-size_t ion_heap_freelist_drain(struct ion_heap *heap, size_t size);
+void *ion_map_kernel(struct ion_client *client, struct ion_handle *handle);
 
 /**
- * ion_heap_freelist_shrink - drain the deferred free
- *				list, skipping any heap-specific
- *				pooling or caching mechanisms
- *
- * @heap:		the heap
- * @size:		amount of memory to drain in bytes
- *
- * Drains the indicated amount of memory from the deferred freelist immediately.
- * Returns the total amount freed.  The total freed may be higher depending
- * on the size of the items in the list, or lower if there is insufficient
- * total memory on the freelist.
- *
- * Unlike with @ion_heap_freelist_drain, don't put any pages back into
- * page pools or otherwise cache the pages. Everything must be
- * genuinely free'd back to the system. If you're free'ing from a
- * shrinker you probably want to use this. Note that this relies on
- * the heap.ops.free callback honoring the ION_PRIV_FLAG_SHRINKER_FREE
- * flag.
+ * ion_unmap_kernel() - destroy a kernel mapping for a handle
+ * @client:	the client
+ * @handle:	handle to unmap
  */
-size_t ion_heap_freelist_shrink(struct ion_heap *heap,
-				size_t size);
+void ion_unmap_kernel(struct ion_client *client, struct ion_handle *handle);
 
 /**
- * ion_heap_freelist_size - returns the size of the freelist in bytes
- * @heap:		the heap
+ * ion_share_dma_buf() - share buffer as dma-buf
+ * @client:	the client
+ * @handle:	the handle
  */
-size_t ion_heap_freelist_size(struct ion_heap *heap);
+struct dma_buf *ion_share_dma_buf(struct ion_client *client,
+						struct ion_handle *handle);
 
 /**
- * functions for creating and destroying a heap pool -- allows you
- * to keep a pool of pre allocated memory to use from your heap.  Keeping
- * a pool of memory that is ready for dma, ie any cached mapping have been
- * invalidated from the cache, provides a significant performance benefit on
- * many systems
+ * ion_share_dma_buf_fd() - given an ion client, create a dma-buf fd
+ * @client:	the client
+ * @handle:	the handle
  */
+int ion_share_dma_buf_fd(struct ion_client *client, struct ion_handle *handle);
 
 /**
- * struct ion_page_pool - pagepool struct
- * @high_count:		number of highmem items in the pool
- * @low_count:		number of lowmem items in the pool
- * @high_items:		list of highmem items
- * @low_items:		list of lowmem items
- * @mutex:		lock protecting this struct and especially the count
- *			item list
- * @gfp_mask:		gfp_mask to use from alloc
- * @order:		order of pages in the pool
- * @list:		plist node for list of pools
+ * ion_import_dma_buf() - get ion_handle from dma-buf
+ * @client:	the client
+ * @dmabuf:	the dma-buf
  *
- * Allows you to keep a pool of pre allocated pages to use from your heap.
- * Keeping a pool of pages that is ready for dma, ie any cached mapping have
- * been invalidated from the cache, provides a significant performance benefit
- * on many systems
+ * Get the ion_buffer associated with the dma-buf and return the ion_handle.
+ * If no ion_handle exists for this buffer, return newly created ion_handle.
+ * If dma-buf from another exporter is passed, return ERR_PTR(-EINVAL)
  */
-struct ion_page_pool {
-	int high_count;
-	int low_count;
-	struct list_head high_items;
-	struct list_head low_items;
-	struct mutex mutex;
-	gfp_t gfp_mask;
-	unsigned int order;
-	struct plist_node list;
-};
+struct ion_handle *ion_import_dma_buf(struct ion_client *client,
+				      struct dma_buf *dmabuf);
 
-struct ion_page_pool *ion_page_pool_create(gfp_t gfp_mask, unsigned int order);
-void ion_page_pool_destroy(struct ion_page_pool *pool);
-struct page *ion_page_pool_alloc(struct ion_page_pool *pool);
-void ion_page_pool_free(struct ion_page_pool *pool, struct page *page);
-
-/** ion_page_pool_shrink - shrinks the size of the memory cached in the pool
- * @pool:		the pool
- * @gfp_mask:		the memory type to reclaim
- * @nr_to_scan:		number of items to shrink in pages
+/**
+ * ion_import_dma_buf_fd() - given a dma-buf fd from the ion exporter get handle
+ * @client:	the client
+ * @fd:		the dma-buf fd
  *
- * returns the number of items freed in pages
+ * Given an dma-buf fd that was allocated through ion via ion_share_dma_buf_fd,
+ * import that fd and return a handle representing it. If a dma-buf from
+ * another exporter is passed in this function will return ERR_PTR(-EINVAL)
  */
-int ion_page_pool_shrink(struct ion_page_pool *pool, gfp_t gfp_mask,
-			 int nr_to_scan);
+struct ion_handle *ion_import_dma_buf_fd(struct ion_client *client, int fd);
 
-#endif /* _ION_H */
+#endif /* _LINUX_ION_H */
diff -Nur -X diffexclude a/drivers/staging/android/ion/ion_carveout_heap.c b/drivers/staging/android/ion/ion_carveout_heap.c
--- a/drivers/staging/android/ion/ion_carveout_heap.c	1970-01-01 08:00:00.000000000 +0800
+++ b/drivers/staging/android/ion/ion_carveout_heap.c	2023-06-09 12:02:57.288923383 +0800
@@ -0,0 +1,186 @@
+/*
+ * drivers/staging/android/ion/ion_carveout_heap.c
+ *
+ * Copyright (C) 2011 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+#include <linux/spinlock.h>
+#include <linux/dma-mapping.h>
+#include <linux/err.h>
+#include <linux/genalloc.h>
+#include <linux/io.h>
+#include <linux/mm.h>
+#include <linux/scatterlist.h>
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+#include "ion.h"
+#include "ion_priv.h"
+
+#define ION_CARVEOUT_ALLOCATE_FAIL	-1
+
+struct ion_carveout_heap {
+	struct ion_heap heap;
+	struct gen_pool *pool;
+	ion_phys_addr_t base;
+};
+
+struct ion_carveout_buffer_info {
+	ion_phys_addr_t handle;
+	struct sg_table *table;
+};
+
+static ion_phys_addr_t ion_carveout_allocate(struct ion_heap *heap,
+					     unsigned long size,
+					     unsigned long align)
+{
+	struct ion_carveout_heap *carveout_heap =
+		container_of(heap, struct ion_carveout_heap, heap);
+	unsigned long offset = gen_pool_alloc(carveout_heap->pool, size);
+
+	if (!offset)
+		return ION_CARVEOUT_ALLOCATE_FAIL;
+
+	return offset;
+}
+
+static void ion_carveout_free(struct ion_heap *heap, ion_phys_addr_t addr,
+			      unsigned long size)
+{
+	struct ion_carveout_heap *carveout_heap =
+		container_of(heap, struct ion_carveout_heap, heap);
+
+	if (addr == ION_CARVEOUT_ALLOCATE_FAIL)
+		return;
+	gen_pool_free(carveout_heap->pool, addr, size);
+}
+
+static int ion_carveout_heap_allocate(struct ion_heap *heap,
+				      struct ion_buffer *buffer,
+				      unsigned long size, unsigned long align,
+				      unsigned long flags)
+{
+	struct sg_table *table;
+	ion_phys_addr_t paddr;
+	int ret;
+	struct ion_carveout_buffer_info *info;
+
+	if (align > PAGE_SIZE)
+		return -EINVAL;
+
+	info = kzalloc(sizeof(struct ion_carveout_buffer_info), GFP_KERNEL);
+	if (!info)
+		return -ENOMEM;
+
+	table = kmalloc(sizeof(*table), GFP_KERNEL);
+	if (!table)
+		return -ENOMEM;
+	info->table = table;
+
+	ret = sg_alloc_table(table, 1, GFP_KERNEL);
+	if (ret)
+		goto err_free;
+
+	paddr = ion_carveout_allocate(heap, size, align);
+	if (paddr == ION_CARVEOUT_ALLOCATE_FAIL) {
+		ret = -ENOMEM;
+		goto err_free_table;
+	}
+	info->handle = paddr;
+
+	sg_set_page(table->sgl, pfn_to_page(PFN_DOWN(paddr)), size, 0);
+	buffer->sg_table = table;
+	buffer->priv_virt = info;
+	buffer->size = size;
+
+	return 0;
+
+err_free_table:
+	sg_free_table(table);
+err_free:
+	kfree(table);
+	kfree(info);
+	return ret;
+}
+
+static void ion_carveout_heap_free(struct ion_buffer *buffer)
+{
+	struct ion_heap *heap = buffer->heap;
+	struct sg_table *table = buffer->sg_table;
+	struct page *page = sg_page(table->sgl);
+	ion_phys_addr_t paddr = PFN_PHYS(page_to_pfn(page));
+	struct ion_carveout_buffer_info *info = buffer->priv_virt;
+
+	ion_heap_buffer_zero(buffer);
+
+	// if (ion_buffer_cached(buffer))
+	// 	dma_sync_sg_for_device(NULL, table->sgl, table->nents,
+	// 			       DMA_BIDIRECTIONAL);
+
+	ion_carveout_free(heap, paddr, buffer->size);
+	sg_free_table(table);
+	kfree(table);
+	kfree(info);
+}
+
+static struct ion_heap_ops carveout_heap_ops = {
+	.allocate = ion_carveout_heap_allocate,
+	.free = ion_carveout_heap_free,
+	.map_user = ion_heap_map_user,
+	.map_kernel = ion_heap_map_kernel,
+	.unmap_kernel = ion_heap_unmap_kernel,
+};
+
+struct ion_heap *ion_carveout_heap_create(struct ion_platform_heap *heap_data)
+{
+	struct ion_carveout_heap *carveout_heap;
+	int ret;
+
+	struct page *page;
+	size_t size;
+
+	page = pfn_to_page(PFN_DOWN(heap_data->base));
+	size = heap_data->size;
+
+	//ion_pages_sync_for_device(NULL, page, size, DMA_BIDIRECTIONAL);
+
+	ret = ion_heap_pages_zero(page, size, pgprot_writecombine(PAGE_KERNEL));
+	if (ret)
+		return ERR_PTR(ret);
+
+	carveout_heap = kzalloc(sizeof(*carveout_heap), GFP_KERNEL);
+	if (!carveout_heap)
+		return ERR_PTR(-ENOMEM);
+
+	carveout_heap->pool = gen_pool_create(PAGE_SHIFT, -1);
+	if (!carveout_heap->pool) {
+		kfree(carveout_heap);
+		return ERR_PTR(-ENOMEM);
+	}
+	carveout_heap->base = heap_data->base;
+	gen_pool_add(carveout_heap->pool, carveout_heap->base, heap_data->size,
+		     -1);
+	carveout_heap->heap.ops = &carveout_heap_ops;
+	carveout_heap->heap.type = ION_HEAP_TYPE_CARVEOUT;
+	carveout_heap->heap.flags = ION_HEAP_FLAG_DEFER_FREE;
+
+	return &carveout_heap->heap;
+}
+
+void ion_carveout_heap_destroy(struct ion_heap *heap)
+{
+	struct ion_carveout_heap *carveout_heap =
+	     container_of(heap, struct  ion_carveout_heap, heap);
+
+	gen_pool_destroy(carveout_heap->pool);
+	kfree(carveout_heap);
+	carveout_heap = NULL;
+}
diff -Nur -X diffexclude a/drivers/staging/android/ion/ion_chunk_heap.c b/drivers/staging/android/ion/ion_chunk_heap.c
--- a/drivers/staging/android/ion/ion_chunk_heap.c	1970-01-01 08:00:00.000000000 +0800
+++ b/drivers/staging/android/ion/ion_chunk_heap.c	2023-06-09 12:02:57.288923383 +0800
@@ -0,0 +1,181 @@
+/*
+ * drivers/staging/android/ion/ion_chunk_heap.c
+ *
+ * Copyright (C) 2012 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+#include <linux/dma-mapping.h>
+#include <linux/err.h>
+#include <linux/genalloc.h>
+#include <linux/io.h>
+#include <linux/mm.h>
+#include <linux/scatterlist.h>
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+#include "ion.h"
+#include "ion_priv.h"
+
+struct ion_chunk_heap {
+	struct ion_heap heap;
+	struct gen_pool *pool;
+	ion_phys_addr_t base;
+	unsigned long chunk_size;
+	unsigned long size;
+	unsigned long allocated;
+};
+
+static int ion_chunk_heap_allocate(struct ion_heap *heap,
+				   struct ion_buffer *buffer,
+				   unsigned long size, unsigned long align,
+				   unsigned long flags)
+{
+	struct ion_chunk_heap *chunk_heap =
+		container_of(heap, struct ion_chunk_heap, heap);
+	struct sg_table *table;
+	struct scatterlist *sg;
+	int ret, i;
+	unsigned long num_chunks;
+	unsigned long allocated_size;
+
+	if (align > chunk_heap->chunk_size)
+		return -EINVAL;
+
+	allocated_size = ALIGN(size, chunk_heap->chunk_size);
+	num_chunks = allocated_size / chunk_heap->chunk_size;
+
+	if (allocated_size > chunk_heap->size - chunk_heap->allocated)
+		return -ENOMEM;
+
+	table = kmalloc(sizeof(*table), GFP_KERNEL);
+	if (!table)
+		return -ENOMEM;
+	ret = sg_alloc_table(table, num_chunks, GFP_KERNEL);
+	if (ret) {
+		kfree(table);
+		return ret;
+	}
+
+	sg = table->sgl;
+	for (i = 0; i < num_chunks; i++) {
+		unsigned long paddr = gen_pool_alloc(chunk_heap->pool,
+						     chunk_heap->chunk_size);
+		if (!paddr)
+			goto err;
+		sg_set_page(sg, pfn_to_page(PFN_DOWN(paddr)),
+			    chunk_heap->chunk_size, 0);
+		sg = sg_next(sg);
+	}
+
+	buffer->sg_table = table;
+	chunk_heap->allocated += allocated_size;
+	return 0;
+err:
+	sg = table->sgl;
+	for (i -= 1; i >= 0; i--) {
+		gen_pool_free(chunk_heap->pool, page_to_phys(sg_page(sg)),
+			      sg->length);
+		sg = sg_next(sg);
+	}
+	sg_free_table(table);
+	kfree(table);
+	return -ENOMEM;
+}
+
+static void ion_chunk_heap_free(struct ion_buffer *buffer)
+{
+	struct ion_heap *heap = buffer->heap;
+	struct ion_chunk_heap *chunk_heap =
+		container_of(heap, struct ion_chunk_heap, heap);
+	struct sg_table *table = buffer->sg_table;
+	struct scatterlist *sg;
+	int i;
+	unsigned long allocated_size;
+
+	allocated_size = ALIGN(buffer->size, chunk_heap->chunk_size);
+
+	ion_heap_buffer_zero(buffer);
+
+	if (ion_buffer_cached(buffer))
+		dma_sync_sg_for_device(NULL, table->sgl, table->nents,
+				       DMA_BIDIRECTIONAL);
+
+	for_each_sg(table->sgl, sg, table->nents, i) {
+		gen_pool_free(chunk_heap->pool, page_to_phys(sg_page(sg)),
+			      sg->length);
+	}
+	chunk_heap->allocated -= allocated_size;
+	sg_free_table(table);
+	kfree(table);
+}
+
+static struct ion_heap_ops chunk_heap_ops = {
+	.allocate = ion_chunk_heap_allocate,
+	.free = ion_chunk_heap_free,
+	.map_user = ion_heap_map_user,
+	.map_kernel = ion_heap_map_kernel,
+	.unmap_kernel = ion_heap_unmap_kernel,
+};
+
+struct ion_heap *ion_chunk_heap_create(struct ion_platform_heap *heap_data)
+{
+	struct ion_chunk_heap *chunk_heap;
+	int ret;
+	struct page *page;
+	size_t size;
+
+	page = pfn_to_page(PFN_DOWN(heap_data->base));
+	size = heap_data->size;
+
+	ion_pages_sync_for_device(NULL, page, size, DMA_BIDIRECTIONAL);
+
+	ret = ion_heap_pages_zero(page, size, pgprot_writecombine(PAGE_KERNEL));
+	if (ret)
+		return ERR_PTR(ret);
+
+	chunk_heap = kzalloc(sizeof(*chunk_heap), GFP_KERNEL);
+	if (!chunk_heap)
+		return ERR_PTR(-ENOMEM);
+
+	chunk_heap->chunk_size = (unsigned long)heap_data->priv;
+	chunk_heap->pool = gen_pool_create(get_order(chunk_heap->chunk_size) +
+					   PAGE_SHIFT, -1);
+	if (!chunk_heap->pool) {
+		ret = -ENOMEM;
+		goto error_gen_pool_create;
+	}
+	chunk_heap->base = heap_data->base;
+	chunk_heap->size = heap_data->size;
+	chunk_heap->allocated = 0;
+
+	gen_pool_add(chunk_heap->pool, chunk_heap->base, heap_data->size, -1);
+	chunk_heap->heap.ops = &chunk_heap_ops;
+	chunk_heap->heap.type = ION_HEAP_TYPE_CHUNK;
+	chunk_heap->heap.flags = ION_HEAP_FLAG_DEFER_FREE;
+	pr_debug("%s: base %lu size %zu align %ld\n", __func__,
+		 chunk_heap->base, heap_data->size, heap_data->align);
+
+	return &chunk_heap->heap;
+
+error_gen_pool_create:
+	kfree(chunk_heap);
+	return ERR_PTR(ret);
+}
+
+void ion_chunk_heap_destroy(struct ion_heap *heap)
+{
+	struct ion_chunk_heap *chunk_heap =
+	     container_of(heap, struct  ion_chunk_heap, heap);
+
+	gen_pool_destroy(chunk_heap->pool);
+	kfree(chunk_heap);
+	chunk_heap = NULL;
+}
diff -Nur -X diffexclude a/drivers/staging/android/ion/ion_cma_heap.c b/drivers/staging/android/ion/ion_cma_heap.c
--- a/drivers/staging/android/ion/ion_cma_heap.c	2019-11-24 15:17:01.000000000 +0800
+++ b/drivers/staging/android/ion/ion_cma_heap.c	2023-06-09 12:02:57.288923383 +0800
@@ -1,138 +1,173 @@
-// SPDX-License-Identifier: GPL-2.0
 /*
- * ION Memory Allocator CMA heap exporter
+ * drivers/staging/android/ion/ion_cma_heap.c
  *
  * Copyright (C) Linaro 2012
  * Author: <benjamin.gaignard@linaro.org> for ST-Ericsson.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
  */
 
 #include <linux/device.h>
 #include <linux/slab.h>
 #include <linux/errno.h>
 #include <linux/err.h>
-#include <linux/cma.h>
-#include <linux/scatterlist.h>
-#include <linux/highmem.h>
+#include <linux/dma-mapping.h>
 
 #include "ion.h"
+#include "ion_priv.h"
+
+#define ION_CMA_ALLOCATE_FAILED -1
 
 struct ion_cma_heap {
 	struct ion_heap heap;
-	struct cma *cma;
+	struct device *dev;
 };
 
 #define to_cma_heap(x) container_of(x, struct ion_cma_heap, heap)
 
+struct ion_cma_buffer_info {
+	void *cpu_addr;
+	dma_addr_t handle;
+	struct sg_table *table;
+};
+
+
 /* ION CMA heap operations functions */
 static int ion_cma_allocate(struct ion_heap *heap, struct ion_buffer *buffer,
-			    unsigned long len,
+			    unsigned long len, unsigned long align,
 			    unsigned long flags)
 {
 	struct ion_cma_heap *cma_heap = to_cma_heap(heap);
-	struct sg_table *table;
-	struct page *pages;
-	unsigned long size = PAGE_ALIGN(len);
-	unsigned long nr_pages = size >> PAGE_SHIFT;
-	unsigned long align = get_order(size);
-	int ret;
-
-	if (align > CONFIG_CMA_ALIGNMENT)
-		align = CONFIG_CMA_ALIGNMENT;
-
-	pages = cma_alloc(cma_heap->cma, nr_pages, align, false);
-	if (!pages)
-		return -ENOMEM;
-
-	if (PageHighMem(pages)) {
-		unsigned long nr_clear_pages = nr_pages;
-		struct page *page = pages;
-
-		while (nr_clear_pages > 0) {
-			void *vaddr = kmap_atomic(page);
-
-			memset(vaddr, 0, PAGE_SIZE);
-			kunmap_atomic(vaddr);
-			page++;
-			nr_clear_pages--;
-		}
-	} else {
-		memset(page_address(pages), 0, size);
-	}
+	struct device *dev = cma_heap->dev;
+	struct ion_cma_buffer_info *info;
 
-	table = kmalloc(sizeof(*table), GFP_KERNEL);
-	if (!table)
+	dev_dbg(dev, "Request buffer allocation len %ld\n", len);
+
+	// if (buffer->flags & ION_FLAG_CACHED){
+	// 	return -EINVAL;
+	// }
+
+	if (align > PAGE_SIZE)
+		return -EINVAL;
+
+	info = kzalloc(sizeof(struct ion_cma_buffer_info), GFP_KERNEL);
+	if (!info)
+		return ION_CMA_ALLOCATE_FAILED;
+
+	info->cpu_addr = dma_alloc_coherent(dev, len, &(info->handle),
+						GFP_HIGHUSER | __GFP_ZERO);
+
+	if (!info->cpu_addr) {
+		dev_err(dev, "Fail to allocate buffer\n");
 		goto err;
+	}
 
-	ret = sg_alloc_table(table, 1, GFP_KERNEL);
-	if (ret)
+	info->table = kmalloc(sizeof(struct sg_table), GFP_KERNEL);
+	if (!info->table)
 		goto free_mem;
 
-	sg_set_page(table->sgl, pages, size, 0);
-
-	buffer->priv_virt = pages;
-	buffer->sg_table = table;
+	if (dma_get_sgtable(dev, info->table, info->cpu_addr, info->handle,
+			    len))
+		goto free_table;
+	/* keep this for memory release */
+	buffer->priv_virt = info;
+	buffer->sg_table = info->table;
+	dev_dbg(dev, "Allocate buffer %p\n", buffer);
 	return 0;
 
+free_table:
+	kfree(info->table);
 free_mem:
-	kfree(table);
+	dma_free_coherent(dev, len, info->cpu_addr, info->handle);
 err:
-	cma_release(cma_heap->cma, pages, nr_pages);
-	return -ENOMEM;
+	kfree(info);
+	return ION_CMA_ALLOCATE_FAILED;
 }
 
 static void ion_cma_free(struct ion_buffer *buffer)
 {
 	struct ion_cma_heap *cma_heap = to_cma_heap(buffer->heap);
-	struct page *pages = buffer->priv_virt;
-	unsigned long nr_pages = PAGE_ALIGN(buffer->size) >> PAGE_SHIFT;
+	struct device *dev = cma_heap->dev;
+	struct ion_cma_buffer_info *info = buffer->priv_virt;
 
+	dev_dbg(dev, "Release buffer %p\n", buffer);
 	/* release memory */
-	cma_release(cma_heap->cma, pages, nr_pages);
+	dma_free_coherent(dev, buffer->size, info->cpu_addr, info->handle);
 	/* release sg table */
-	sg_free_table(buffer->sg_table);
-	kfree(buffer->sg_table);
+	sg_free_table(info->table);
+	kfree(info->table);
+	kfree(info);
+}
+
+static int ion_cma_mmap(struct ion_heap *mapper, struct ion_buffer *buffer,
+			struct vm_area_struct *vma)
+{
+	struct ion_cma_heap *cma_heap = to_cma_heap(buffer->heap);
+	struct device *dev = cma_heap->dev;
+	struct ion_cma_buffer_info *info = buffer->priv_virt;
+
+	if (buffer->flags & ION_FLAG_CACHED){
+		return remap_pfn_range(vma, vma->vm_start,
+		      __phys_to_pfn((u32)info->handle) + vma->vm_pgoff,
+		      vma->vm_end - vma->vm_start,
+		      vma->vm_page_prot);
+	}
+	
+	return dma_mmap_coherent(dev, vma, info->cpu_addr, info->handle,
+				 buffer->size);
+}
+
+static void *ion_cma_map_kernel(struct ion_heap *heap,
+				struct ion_buffer *buffer)
+{
+	struct ion_cma_buffer_info *info = buffer->priv_virt;
+	/* kernel memory mapping has been done at allocation time */
+	return info->cpu_addr;
+}
+
+static void ion_cma_unmap_kernel(struct ion_heap *heap,
+				 struct ion_buffer *buffer)
+{
 }
 
 static struct ion_heap_ops ion_cma_ops = {
 	.allocate = ion_cma_allocate,
 	.free = ion_cma_free,
-	.map_user = ion_heap_map_user,
-	.map_kernel = ion_heap_map_kernel,
-	.unmap_kernel = ion_heap_unmap_kernel,
+	.map_user = ion_cma_mmap,
+	.map_kernel = ion_cma_map_kernel,
+	.unmap_kernel = ion_cma_unmap_kernel,
 };
 
-static struct ion_heap *__ion_cma_heap_create(struct cma *cma)
+struct ion_heap *ion_cma_heap_create(struct ion_platform_heap *data)
 {
 	struct ion_cma_heap *cma_heap;
 
-	cma_heap = kzalloc(sizeof(*cma_heap), GFP_KERNEL);
+	cma_heap = kzalloc(sizeof(struct ion_cma_heap), GFP_KERNEL);
 
 	if (!cma_heap)
 		return ERR_PTR(-ENOMEM);
 
 	cma_heap->heap.ops = &ion_cma_ops;
-	cma_heap->cma = cma;
+	/*
+	 * get device from private heaps data, later it will be
+	 * used to make the link with reserved CMA memory
+	 */
+	cma_heap->dev = data->priv;
 	cma_heap->heap.type = ION_HEAP_TYPE_DMA;
 	return &cma_heap->heap;
 }
 
-static int __ion_add_cma_heaps(struct cma *cma, void *data)
+void ion_cma_heap_destroy(struct ion_heap *heap)
 {
-	struct ion_heap *heap;
-
-	heap = __ion_cma_heap_create(cma);
-	if (IS_ERR(heap))
-		return PTR_ERR(heap);
-
-	heap->name = cma_get_name(cma);
-
-	ion_device_add_heap(heap);
-	return 0;
-}
+	struct ion_cma_heap *cma_heap = to_cma_heap(heap);
 
-static int ion_add_cma_heaps(void)
-{
-	cma_for_each_area(__ion_add_cma_heaps, NULL);
-	return 0;
+	kfree(cma_heap);
 }
-device_initcall(ion_add_cma_heaps);
diff -Nur -X diffexclude a/drivers/staging/android/ion/ion_heap.c b/drivers/staging/android/ion/ion_heap.c
--- a/drivers/staging/android/ion/ion_heap.c	2019-11-24 15:17:01.000000000 +0800
+++ b/drivers/staging/android/ion/ion_heap.c	2023-06-09 12:15:44.234729108 +0800
@@ -1,8 +1,17 @@
-// SPDX-License-Identifier: GPL-2.0
 /*
- * ION Memory Allocator generic heap helpers
+ * drivers/staging/android/ion/ion_heap.c
  *
  * Copyright (C) 2011 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
  */
 
 #include <linux/err.h>
@@ -11,11 +20,11 @@
 #include <linux/mm.h>
 #include <linux/rtmutex.h>
 #include <linux/sched.h>
-#include <uapi/linux/sched/types.h>
 #include <linux/scatterlist.h>
 #include <linux/vmalloc.h>
-
+#include <uapi/linux/sched/types.h>
 #include "ion.h"
+#include "ion_priv.h"
 
 void *ion_heap_map_kernel(struct ion_heap *heap,
 			  struct ion_buffer *buffer)
@@ -26,8 +35,7 @@
 	pgprot_t pgprot;
 	struct sg_table *table = buffer->sg_table;
 	int npages = PAGE_ALIGN(buffer->size) / PAGE_SIZE;
-	struct page **pages = vmalloc(array_size(npages,
-						 sizeof(struct page *)));
+	struct page **pages = vmalloc(sizeof(struct page *) * npages);
 	struct page **tmp = pages;
 
 	if (!pages)
@@ -93,7 +101,6 @@
 		if (addr >= vma->vm_end)
 			return 0;
 	}
-
 	return 0;
 }
 
@@ -256,7 +263,6 @@
 		return PTR_ERR_OR_ZERO(heap->task);
 	}
 	sched_setscheduler(heap->task, SCHED_IDLE, &param);
-
 	return 0;
 }
 
@@ -268,10 +274,8 @@
 	int total = 0;
 
 	total = ion_heap_freelist_size(heap) / PAGE_SIZE;
-
 	if (heap->ops->shrink)
 		total += heap->ops->shrink(heap, sc->gfp_mask, 0);
-
 	return total;
 }
 
@@ -300,16 +304,81 @@
 
 	if (heap->ops->shrink)
 		freed += heap->ops->shrink(heap, sc->gfp_mask, to_scan);
-
 	return freed;
 }
 
-int ion_heap_init_shrinker(struct ion_heap *heap)
+void ion_heap_init_shrinker(struct ion_heap *heap)
 {
 	heap->shrinker.count_objects = ion_heap_shrink_count;
 	heap->shrinker.scan_objects = ion_heap_shrink_scan;
 	heap->shrinker.seeks = DEFAULT_SEEKS;
 	heap->shrinker.batch = 0;
+	register_shrinker(&heap->shrinker);
+}
 
-	return register_shrinker(&heap->shrinker);
+struct ion_heap *ion_heap_create(struct ion_platform_heap *heap_data)
+{
+	struct ion_heap *heap = NULL;
+
+	switch (heap_data->type) {
+	case ION_HEAP_TYPE_SYSTEM_CONTIG:
+		heap = ion_system_contig_heap_create(heap_data);
+		break;
+	case ION_HEAP_TYPE_SYSTEM:
+		heap = ion_system_heap_create(heap_data);
+		break;
+	case ION_HEAP_TYPE_CARVEOUT:
+		heap = ion_carveout_heap_create(heap_data);
+		break;
+	case ION_HEAP_TYPE_CHUNK:
+		heap = ion_chunk_heap_create(heap_data);
+		break;
+	case ION_HEAP_TYPE_DMA:
+		heap = ion_cma_heap_create(heap_data);
+		break;
+	default:
+		pr_err("%s: Invalid heap type %d\n", __func__,
+		       heap_data->type);
+		return ERR_PTR(-EINVAL);
+	}
+
+	if (IS_ERR_OR_NULL(heap)) {
+		pr_err("%s: error creating heap %s type %d base %lu size %zu\n",
+		       __func__, heap_data->name, heap_data->type,
+		       heap_data->base, heap_data->size);
+		return ERR_PTR(-EINVAL);
+	}
+
+	heap->name = heap_data->name;
+	heap->id = heap_data->id;
+	return heap;
+}
+EXPORT_SYMBOL(ion_heap_create);
+
+void ion_heap_destroy(struct ion_heap *heap)
+{
+	if (!heap)
+		return;
+
+	switch (heap->type) {
+	case ION_HEAP_TYPE_SYSTEM_CONTIG:
+		ion_system_contig_heap_destroy(heap);
+		break;
+	case ION_HEAP_TYPE_SYSTEM:
+		ion_system_heap_destroy(heap);
+		break;
+	case ION_HEAP_TYPE_CARVEOUT:
+		ion_carveout_heap_destroy(heap);
+		break;
+	case ION_HEAP_TYPE_CHUNK:
+		ion_chunk_heap_destroy(heap);
+		break;
+	case ION_HEAP_TYPE_DMA:
+		ion_cma_heap_destroy(heap);
+		break;
+	default:
+		pr_err("%s: Invalid heap type %d\n", __func__,
+		       heap->type);
+	}
 }
+EXPORT_SYMBOL(ion_heap_destroy);
diff -Nur -X diffexclude a/drivers/staging/android/ion/ion_of.c b/drivers/staging/android/ion/ion_of.c
--- a/drivers/staging/android/ion/ion_of.c	1970-01-01 08:00:00.000000000 +0800
+++ b/drivers/staging/android/ion/ion_of.c	2023-06-09 12:02:57.288923383 +0800
@@ -0,0 +1,185 @@
+/*
+ * Based on work from:
+ *   Andrew Andrianov <andrew@ncrmnt.org>
+ *   Google
+ *   The Linux Foundation
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/init.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/of_address.h>
+#include <linux/clk.h>
+#include <linux/dma-mapping.h>
+#include <linux/cma.h>
+#include "dma-contiguous.h"
+#include <linux/io.h>
+#include <linux/of_reserved_mem.h>
+#include "ion.h"
+#include "ion_priv.h"
+#include "ion_of.h"
+
+static int ion_parse_dt_heap_common(struct device_node *heap_node,
+				    struct ion_platform_heap *heap,
+				    struct ion_of_heap *compatible)
+{
+	int i;
+
+	for (i = 0; compatible[i].name; i++) {
+		if (of_device_is_compatible(heap_node, compatible[i].compat))
+			break;
+	}
+
+	if (!compatible[i].name)
+		return -ENODEV;
+
+	heap->id = compatible[i].heap_id;
+	heap->type = compatible[i].type;
+	heap->name = compatible[i].name;
+	heap->align = compatible[i].align;
+
+	/* Some kind of callback function pointer? */
+
+	pr_info("%s: id %d type %d name %s align %lx\n", __func__,
+		heap->id, heap->type, heap->name, heap->align);
+	return 0;
+}
+
+static int ion_setup_heap_common(struct platform_device *parent,
+				 struct device_node *heap_node,
+				 struct ion_platform_heap *heap)
+{
+	int ret = 0;
+
+	switch (heap->type) {
+	case ION_HEAP_TYPE_CARVEOUT:
+	case ION_HEAP_TYPE_CHUNK:
+		if (heap->base && heap->size)
+			return 0;
+
+		ret = of_reserved_mem_device_init(heap->priv);
+		break;
+	default:
+		break;
+	}
+
+	return ret;
+}
+
+struct ion_platform_data *ion_parse_dt(struct platform_device *pdev,
+				       struct ion_of_heap *compatible)
+{
+	int num_heaps, ret;
+	const struct device_node *dt_node = pdev->dev.of_node;
+	struct device_node *node;
+	struct ion_platform_heap *heaps;
+	struct ion_platform_data *data;
+	int i = 0;
+
+	num_heaps = of_get_available_child_count(dt_node);
+
+	if (!num_heaps)
+		return ERR_PTR(-EINVAL);
+
+	heaps = devm_kzalloc(&pdev->dev,
+			     sizeof(struct ion_platform_heap) * num_heaps,
+			     GFP_KERNEL);
+	if (!heaps)
+		return ERR_PTR(-ENOMEM);
+
+	data = devm_kzalloc(&pdev->dev, sizeof(struct ion_platform_data),
+			    GFP_KERNEL);
+	if (!data)
+		return ERR_PTR(-ENOMEM);
+
+	for_each_available_child_of_node(dt_node, node) {
+		struct platform_device *heap_pdev;
+
+		ret = ion_parse_dt_heap_common(node, &heaps[i], compatible);
+		if (ret)
+			return ERR_PTR(ret);
+
+		heap_pdev = of_platform_device_create(node, heaps[i].name,
+						      &pdev->dev);
+		if (!heap_pdev)
+			return ERR_PTR(-ENOMEM);
+		heap_pdev->dev.platform_data = &heaps[i];
+
+		heaps[i].priv = &heap_pdev->dev;
+
+		ret = ion_setup_heap_common(pdev, node, &heaps[i]);
+		if (ret)
+			goto out_err;
+		i++;
+	}
+
+	data->heaps = heaps;
+	data->nr = num_heaps;
+	return data;
+
+out_err:
+	for ( ; i >= 0; i--)
+		if (heaps[i].priv)
+			of_device_unregister(to_platform_device(heaps[i].priv));
+
+	return ERR_PTR(ret);
+}
+
+void ion_destroy_platform_data(struct ion_platform_data *data)
+{
+	int i;
+
+	for (i = 0; i < data->nr; i++)
+		if (data->heaps[i].priv)
+			of_device_unregister(to_platform_device(
+				data->heaps[i].priv));
+}
+
+#ifdef CONFIG_OF_RESERVED_MEM
+#include <linux/of.h>
+#include <linux/of_fdt.h>
+#include <linux/of_reserved_mem.h>
+
+static int rmem_ion_device_init(struct reserved_mem *rmem, struct device *dev)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+	struct ion_platform_heap *heap = pdev->dev.platform_data;
+
+	heap->base = rmem->base;
+	heap->size = rmem->size;
+	pr_debug("%s: heap %s base %pa size %pa dev %p\n", __func__,
+		 heap->name, &rmem->base, &rmem->size, dev);
+	return 0;
+}
+
+static void rmem_ion_device_release(struct reserved_mem *rmem,
+				    struct device *dev)
+{
+	return;
+}
+
+static const struct reserved_mem_ops rmem_dma_ops = {
+	.device_init	= rmem_ion_device_init,
+	.device_release	= rmem_ion_device_release,
+};
+
+static int __init rmem_ion_setup(struct reserved_mem *rmem)
+{
+	phys_addr_t size = rmem->size;
+
+	size = size / 1024;
+
+	pr_info("Ion memory setup at %pa size %pa MiB\n",
+		&rmem->base, &size);
+	rmem->ops = &rmem_dma_ops;
+	return 0;
+}
+
+RESERVEDMEM_OF_DECLARE(ion, "ion-region", rmem_ion_setup);
+#endif
diff -Nur -X diffexclude a/drivers/staging/android/ion/ion_of.h b/drivers/staging/android/ion/ion_of.h
--- a/drivers/staging/android/ion/ion_of.h	1970-01-01 08:00:00.000000000 +0800
+++ b/drivers/staging/android/ion/ion_of.h	2023-06-09 12:02:57.292923413 +0800
@@ -0,0 +1,37 @@
+/*
+ * Based on work from:
+ *   Andrew Andrianov <andrew@ncrmnt.org>
+ *   Google
+ *   The Linux Foundation
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef _ION_OF_H
+#define _ION_OF_H
+
+struct ion_of_heap {
+	const char *compat;
+	int heap_id;
+	int type;
+	const char *name;
+	int align;
+};
+
+#define PLATFORM_HEAP(_compat, _id, _type, _name) \
+{ \
+	.compat = _compat, \
+	.heap_id = _id, \
+	.type = _type, \
+	.name = _name, \
+	.align = PAGE_SIZE, \
+}
+
+struct ion_platform_data *ion_parse_dt(struct platform_device *pdev,
+					struct ion_of_heap *compatible);
+
+void ion_destroy_platform_data(struct ion_platform_data *data);
+
+#endif
diff -Nur -X diffexclude a/drivers/staging/android/ion/ion_page_pool.c b/drivers/staging/android/ion/ion_page_pool.c
--- a/drivers/staging/android/ion/ion_page_pool.c	2019-11-24 15:17:01.000000000 +0800
+++ b/drivers/staging/android/ion/ion_page_pool.c	2023-06-09 12:02:57.292923413 +0800
@@ -1,22 +1,39 @@
-// SPDX-License-Identifier: GPL-2.0
 /*
- * ION Memory Allocator page pool helpers
+ * drivers/staging/android/ion/ion_mem_pool.c
  *
  * Copyright (C) 2011 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
  */
 
+#include <linux/debugfs.h>
+#include <linux/dma-mapping.h>
+#include <linux/err.h>
+#include <linux/fs.h>
 #include <linux/list.h>
+#include <linux/init.h>
 #include <linux/slab.h>
 #include <linux/swap.h>
-#include <linux/sched/signal.h>
-
-#include "ion.h"
+#include "ion_priv.h"
 
-static inline struct page *ion_page_pool_alloc_pages(struct ion_page_pool *pool)
+static void *ion_page_pool_alloc_pages(struct ion_page_pool *pool)
 {
-	if (fatal_signal_pending(current))
+	struct page *page = alloc_pages(pool->gfp_mask, pool->order);
+
+	if (!page)
 		return NULL;
-	return alloc_pages(pool->gfp_mask, pool->order);
+	if (!pool->cached)
+		ion_pages_sync_for_device(NULL, page, PAGE_SIZE << pool->order,
+					  DMA_BIDIRECTIONAL);
+	return page;
 }
 
 static void ion_page_pool_free_pages(struct ion_page_pool *pool,
@@ -25,7 +42,7 @@
 	__free_pages(page, pool->order);
 }
 
-static void ion_page_pool_add(struct ion_page_pool *pool, struct page *page)
+static int ion_page_pool_add(struct ion_page_pool *pool, struct page *page)
 {
 	mutex_lock(&pool->mutex);
 	if (PageHighMem(page)) {
@@ -35,10 +52,8 @@
 		list_add_tail(&page->lru, &pool->low_items);
 		pool->low_count++;
 	}
-
-	mod_node_page_state(page_pgdat(page), NR_KERNEL_MISC_RECLAIMABLE,
-							1 << pool->order);
 	mutex_unlock(&pool->mutex);
+	return 0;
 }
 
 static struct page *ion_page_pool_remove(struct ion_page_pool *pool, bool high)
@@ -56,8 +71,6 @@
 	}
 
 	list_del(&page->lru);
-	mod_node_page_state(page_pgdat(page), NR_KERNEL_MISC_RECLAIMABLE,
-							-(1 << pool->order));
 	return page;
 }
 
@@ -82,9 +95,13 @@
 
 void ion_page_pool_free(struct ion_page_pool *pool, struct page *page)
 {
+	int ret;
+
 	BUG_ON(pool->order != compound_order(page));
 
-	ion_page_pool_add(pool, page);
+	ret = ion_page_pool_add(pool, page);
+	if (ret)
+		ion_page_pool_free_pages(pool, page);
 }
 
 static int ion_page_pool_total(struct ion_page_pool *pool, bool high)
@@ -131,7 +148,8 @@
 	return freed;
 }
 
-struct ion_page_pool *ion_page_pool_create(gfp_t gfp_mask, unsigned int order)
+struct ion_page_pool *ion_page_pool_create(gfp_t gfp_mask, unsigned int order,
+					   bool cached)
 {
 	struct ion_page_pool *pool = kmalloc(sizeof(*pool), GFP_KERNEL);
 
@@ -145,6 +163,8 @@
 	pool->order = order;
 	mutex_init(&pool->mutex);
 	plist_node_init(&pool->list, order);
+	if (cached)
+		pool->cached = true;
 
 	return pool;
 }
@@ -153,3 +173,9 @@
 {
 	kfree(pool);
 }
+
+static int __init ion_page_pool_init(void)
+{
+	return 0;
+}
+device_initcall(ion_page_pool_init);
diff -Nur -X diffexclude a/drivers/staging/android/ion/ion_priv.h b/drivers/staging/android/ion/ion_priv.h
--- a/drivers/staging/android/ion/ion_priv.h	1970-01-01 08:00:00.000000000 +0800
+++ b/drivers/staging/android/ion/ion_priv.h	2023-06-09 12:02:57.292923413 +0800
@@ -0,0 +1,473 @@
+/*
+ * drivers/staging/android/ion/ion_priv.h
+ *
+ * Copyright (C) 2011 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#ifndef _ION_PRIV_H
+#define _ION_PRIV_H
+
+#include <linux/device.h>
+#include <linux/dma-direction.h>
+#include <linux/kref.h>
+#include <linux/mm_types.h>
+#include <linux/mutex.h>
+#include <linux/rbtree.h>
+#include <linux/sched.h>
+#include <linux/shrinker.h>
+#include <linux/types.h>
+#include <linux/miscdevice.h>
+
+#include "ion.h"
+
+/**
+ * struct ion_buffer - metadata for a particular buffer
+ * @ref:		reference count
+ * @node:		node in the ion_device buffers tree
+ * @dev:		back pointer to the ion_device
+ * @heap:		back pointer to the heap the buffer came from
+ * @flags:		buffer specific flags
+ * @private_flags:	internal buffer specific flags
+ * @size:		size of the buffer
+ * @priv_virt:		private data to the buffer representable as
+ *			a void *
+ * @lock:		protects the buffers cnt fields
+ * @kmap_cnt:		number of times the buffer is mapped to the kernel
+ * @vaddr:		the kernel mapping if kmap_cnt is not zero
+ * @dmap_cnt:		number of times the buffer is mapped for dma
+ * @sg_table:		the sg table for the buffer if dmap_cnt is not zero
+ * @pages:		flat array of pages in the buffer -- used by fault
+ *			handler and only valid for buffers that are faulted in
+ * @vmas:		list of vma's mapping this buffer
+ * @handle_count:	count of handles referencing this buffer
+ * @task_comm:		taskcomm of last client to reference this buffer in a
+ *			handle, used for debugging
+ * @pid:		pid of last client to reference this buffer in a
+ *			handle, used for debugging
+*/
+struct ion_buffer {
+	struct kref ref;
+	union {
+		struct rb_node node;
+		struct list_head list;
+	};
+	struct ion_device *dev;
+	struct ion_heap *heap;
+	unsigned long flags;
+	unsigned long private_flags;
+	size_t size;
+	void *priv_virt;
+	struct mutex lock;
+	int kmap_cnt;
+	void *vaddr;
+	int dmap_cnt;
+	struct sg_table *sg_table;
+	struct page **pages;
+	struct list_head vmas;
+	/* used to track orphaned buffers */
+	int handle_count;
+	char task_comm[TASK_COMM_LEN];
+	pid_t pid;
+};
+void ion_buffer_destroy(struct ion_buffer *buffer);
+
+/**
+ * struct ion_device - the metadata of the ion device node
+ * @dev:		the actual misc device
+ * @buffers:		an rb tree of all the existing buffers
+ * @buffer_lock:	lock protecting the tree of buffers
+ * @lock:		rwsem protecting the tree of heaps and clients
+ * @heaps:		list of all the heaps in the system
+ * @user_clients:	list of all the clients created from userspace
+ */
+struct ion_device {
+	struct miscdevice dev;
+	struct rb_root buffers;
+	struct mutex buffer_lock;
+	struct rw_semaphore lock;
+	struct plist_head heaps;
+	long (*custom_ioctl)(struct ion_client *client, unsigned int cmd,
+			     unsigned long arg);
+	struct rb_root clients;
+	struct dentry *debug_root;
+	struct dentry *heaps_debug_root;
+	struct dentry *clients_debug_root;
+	int heap_cnt;
+};
+
+/**
+ * struct ion_client - a process/hw block local address space
+ * @node:		node in the tree of all clients
+ * @dev:		backpointer to ion device
+ * @handles:		an rb tree of all the handles in this client
+ * @idr:		an idr space for allocating handle ids
+ * @lock:		lock protecting the tree of handles
+ * @name:		used for debugging
+ * @display_name:	used for debugging (unique version of @name)
+ * @display_serial:	used for debugging (to make display_name unique)
+ * @task:		used for debugging
+ *
+ * A client represents a list of buffers this client may access.
+ * The mutex stored here is used to protect both handles tree
+ * as well as the handles themselves, and should be held while modifying either.
+ */
+struct ion_client {
+	struct rb_node node;
+	struct ion_device *dev;
+	struct rb_root handles;
+	struct idr idr;
+	struct mutex lock;
+	const char *name;
+	char *display_name;
+	int display_serial;
+	struct task_struct *task;
+	pid_t pid;
+	struct dentry *debug_root;
+};
+
+/**
+ * ion_handle - a client local reference to a buffer
+ * @ref:		reference count
+ * @client:		back pointer to the client the buffer resides in
+ * @buffer:		pointer to the buffer
+ * @node:		node in the client's handle rbtree
+ * @kmap_cnt:		count of times this client has mapped to kernel
+ * @id:			client-unique id allocated by client->idr
+ *
+ * Modifications to node, map_cnt or mapping should be protected by the
+ * lock in the client.  Other fields are never changed after initialization.
+ */
+struct ion_handle {
+	struct kref ref;
+	struct ion_client *client;
+	struct ion_buffer *buffer;
+	struct rb_node node;
+	unsigned int kmap_cnt;
+	int id;
+};
+
+/**
+ * struct ion_heap_ops - ops to operate on a given heap
+ * @allocate:		allocate memory
+ * @free:		free memory
+ * @map_kernel		map memory to the kernel
+ * @unmap_kernel	unmap memory to the kernel
+ * @map_user		map memory to userspace
+ *
+ * allocate, phys, and map_user return 0 on success, -errno on error.
+ * map_dma and map_kernel return pointer on success, ERR_PTR on
+ * error. @free will be called with ION_PRIV_FLAG_SHRINKER_FREE set in
+ * the buffer's private_flags when called from a shrinker. In that
+ * case, the pages being free'd must be truly free'd back to the
+ * system, not put in a page pool or otherwise cached.
+ */
+struct ion_heap_ops {
+	int (*allocate)(struct ion_heap *heap,
+			struct ion_buffer *buffer, unsigned long len,
+			unsigned long align, unsigned long flags);
+	void (*free)(struct ion_buffer *buffer);
+	void * (*map_kernel)(struct ion_heap *heap, struct ion_buffer *buffer);
+	void (*unmap_kernel)(struct ion_heap *heap, struct ion_buffer *buffer);
+	int (*map_user)(struct ion_heap *mapper, struct ion_buffer *buffer,
+			struct vm_area_struct *vma);
+	int (*shrink)(struct ion_heap *heap, gfp_t gfp_mask, int nr_to_scan);
+};
+
+/**
+ * heap flags - flags between the heaps and core ion code
+ */
+#define ION_HEAP_FLAG_DEFER_FREE (1 << 0)
+
+/**
+ * private flags - flags internal to ion
+ */
+/*
+ * Buffer is being freed from a shrinker function. Skip any possible
+ * heap-specific caching mechanism (e.g. page pools). Guarantees that
+ * any buffer storage that came from the system allocator will be
+ * returned to the system allocator.
+ */
+#define ION_PRIV_FLAG_SHRINKER_FREE (1 << 0)
+
+/**
+ * struct ion_heap - represents a heap in the system
+ * @node:		rb node to put the heap on the device's tree of heaps
+ * @dev:		back pointer to the ion_device
+ * @type:		type of heap
+ * @ops:		ops struct as above
+ * @flags:		flags
+ * @id:			id of heap, also indicates priority of this heap when
+ *			allocating.  These are specified by platform data and
+ *			MUST be unique
+ * @name:		used for debugging
+ * @shrinker:		a shrinker for the heap
+ * @free_list:		free list head if deferred free is used
+ * @free_list_size	size of the deferred free list in bytes
+ * @lock:		protects the free list
+ * @waitqueue:		queue to wait on from deferred free thread
+ * @task:		task struct of deferred free thread
+ * @debug_show:		called when heap debug file is read to add any
+ *			heap specific debug info to output
+ *
+ * Represents a pool of memory from which buffers can be made.  In some
+ * systems the only heap is regular system memory allocated via vmalloc.
+ * On others, some blocks might require large physically contiguous buffers
+ * that are allocated from a specially reserved heap.
+ */
+struct ion_heap {
+	struct plist_node node;
+	struct ion_device *dev;
+	enum ion_heap_type type;
+	struct ion_heap_ops *ops;
+	unsigned long flags;
+	unsigned int id;
+	const char *name;
+	struct shrinker shrinker;
+	struct list_head free_list;
+	size_t free_list_size;
+	spinlock_t free_lock;
+	wait_queue_head_t waitqueue;
+	struct task_struct *task;
+
+	int (*debug_show)(struct ion_heap *heap, struct seq_file *, void *);
+};
+
+/**
+ * ion_buffer_cached - this ion buffer is cached
+ * @buffer:		buffer
+ *
+ * indicates whether this ion buffer is cached
+ */
+bool ion_buffer_cached(struct ion_buffer *buffer);
+
+/**
+ * ion_buffer_fault_user_mappings - fault in user mappings of this buffer
+ * @buffer:		buffer
+ *
+ * indicates whether userspace mappings of this buffer will be faulted
+ * in, this can affect how buffers are allocated from the heap.
+ */
+bool ion_buffer_fault_user_mappings(struct ion_buffer *buffer);
+
+/**
+ * ion_device_create - allocates and returns an ion device
+ * @custom_ioctl:	arch specific ioctl function if applicable
+ *
+ * returns a valid device or -PTR_ERR
+ */
+struct ion_device *ion_device_create(long (*custom_ioctl)
+				     (struct ion_client *client,
+				      unsigned int cmd,
+				      unsigned long arg));
+
+/**
+ * ion_device_destroy - free and device and it's resource
+ * @dev:		the device
+ */
+void ion_device_destroy(struct ion_device *dev);
+
+/**
+ * ion_device_add_heap - adds a heap to the ion device
+ * @dev:		the device
+ * @heap:		the heap to add
+ */
+void ion_device_add_heap(struct ion_device *dev, struct ion_heap *heap);
+
+/**
+ * some helpers for common operations on buffers using the sg_table
+ * and vaddr fields
+ */
+void *ion_heap_map_kernel(struct ion_heap *, struct ion_buffer *);
+void ion_heap_unmap_kernel(struct ion_heap *, struct ion_buffer *);
+int ion_heap_map_user(struct ion_heap *, struct ion_buffer *,
+			struct vm_area_struct *);
+int ion_heap_buffer_zero(struct ion_buffer *buffer);
+int ion_heap_pages_zero(struct page *page, size_t size, pgprot_t pgprot);
+
+/**
+ * ion_heap_init_shrinker
+ * @heap:		the heap
+ *
+ * If a heap sets the ION_HEAP_FLAG_DEFER_FREE flag or defines the shrink op
+ * this function will be called to setup a shrinker to shrink the freelists
+ * and call the heap's shrink op.
+ */
+void ion_heap_init_shrinker(struct ion_heap *heap);
+
+/**
+ * ion_heap_init_deferred_free -- initialize deferred free functionality
+ * @heap:		the heap
+ *
+ * If a heap sets the ION_HEAP_FLAG_DEFER_FREE flag this function will
+ * be called to setup deferred frees. Calls to free the buffer will
+ * return immediately and the actual free will occur some time later
+ */
+int ion_heap_init_deferred_free(struct ion_heap *heap);
+
+/**
+ * ion_heap_freelist_add - add a buffer to the deferred free list
+ * @heap:		the heap
+ * @buffer:		the buffer
+ *
+ * Adds an item to the deferred freelist.
+ */
+void ion_heap_freelist_add(struct ion_heap *heap, struct ion_buffer *buffer);
+
+/**
+ * ion_heap_freelist_drain - drain the deferred free list
+ * @heap:		the heap
+ * @size:		amount of memory to drain in bytes
+ *
+ * Drains the indicated amount of memory from the deferred freelist immediately.
+ * Returns the total amount freed.  The total freed may be higher depending
+ * on the size of the items in the list, or lower if there is insufficient
+ * total memory on the freelist.
+ */
+size_t ion_heap_freelist_drain(struct ion_heap *heap, size_t size);
+
+/**
+ * ion_heap_freelist_shrink - drain the deferred free
+ *				list, skipping any heap-specific
+ *				pooling or caching mechanisms
+ *
+ * @heap:		the heap
+ * @size:		amount of memory to drain in bytes
+ *
+ * Drains the indicated amount of memory from the deferred freelist immediately.
+ * Returns the total amount freed.  The total freed may be higher depending
+ * on the size of the items in the list, or lower if there is insufficient
+ * total memory on the freelist.
+ *
+ * Unlike with @ion_heap_freelist_drain, don't put any pages back into
+ * page pools or otherwise cache the pages. Everything must be
+ * genuinely free'd back to the system. If you're free'ing from a
+ * shrinker you probably want to use this. Note that this relies on
+ * the heap.ops.free callback honoring the ION_PRIV_FLAG_SHRINKER_FREE
+ * flag.
+ */
+size_t ion_heap_freelist_shrink(struct ion_heap *heap,
+					size_t size);
+
+/**
+ * ion_heap_freelist_size - returns the size of the freelist in bytes
+ * @heap:		the heap
+ */
+size_t ion_heap_freelist_size(struct ion_heap *heap);
+
+
+/**
+ * functions for creating and destroying the built in ion heaps.
+ * architectures can add their own custom architecture specific
+ * heaps as appropriate.
+ */
+
+struct ion_heap *ion_heap_create(struct ion_platform_heap *);
+void ion_heap_destroy(struct ion_heap *);
+struct ion_heap *ion_system_heap_create(struct ion_platform_heap *);
+void ion_system_heap_destroy(struct ion_heap *);
+
+struct ion_heap *ion_system_contig_heap_create(struct ion_platform_heap *);
+void ion_system_contig_heap_destroy(struct ion_heap *);
+
+struct ion_heap *ion_carveout_heap_create(struct ion_platform_heap *);
+void ion_carveout_heap_destroy(struct ion_heap *);
+
+struct ion_heap *ion_chunk_heap_create(struct ion_platform_heap *);
+void ion_chunk_heap_destroy(struct ion_heap *);
+struct ion_heap *ion_cma_heap_create(struct ion_platform_heap *);
+void ion_cma_heap_destroy(struct ion_heap *);
+
+/**
+ * functions for creating and destroying a heap pool -- allows you
+ * to keep a pool of pre allocated memory to use from your heap.  Keeping
+ * a pool of memory that is ready for dma, ie any cached mapping have been
+ * invalidated from the cache, provides a significant performance benefit on
+ * many systems
+ */
+
+/**
+ * struct ion_page_pool - pagepool struct
+ * @high_count:		number of highmem items in the pool
+ * @low_count:		number of lowmem items in the pool
+ * @high_items:		list of highmem items
+ * @low_items:		list of lowmem items
+ * @mutex:		lock protecting this struct and especially the count
+ *			item list
+ * @gfp_mask:		gfp_mask to use from alloc
+ * @order:		order of pages in the pool
+ * @list:		plist node for list of pools
+ * @cached:		it's cached pool or not
+ *
+ * Allows you to keep a pool of pre allocated pages to use from your heap.
+ * Keeping a pool of pages that is ready for dma, ie any cached mapping have
+ * been invalidated from the cache, provides a significant performance benefit
+ * on many systems
+ */
+struct ion_page_pool {
+	int high_count;
+	int low_count;
+	bool cached;
+	struct list_head high_items;
+	struct list_head low_items;
+	struct mutex mutex;
+	gfp_t gfp_mask;
+	unsigned int order;
+	struct plist_node list;
+};
+
+struct ion_page_pool *ion_page_pool_create(gfp_t gfp_mask, unsigned int order,
+					   bool cached);
+void ion_page_pool_destroy(struct ion_page_pool *);
+struct page *ion_page_pool_alloc(struct ion_page_pool *);
+void ion_page_pool_free(struct ion_page_pool *, struct page *);
+
+/** ion_page_pool_shrink - shrinks the size of the memory cached in the pool
+ * @pool:		the pool
+ * @gfp_mask:		the memory type to reclaim
+ * @nr_to_scan:		number of items to shrink in pages
+ *
+ * returns the number of items freed in pages
+ */
+int ion_page_pool_shrink(struct ion_page_pool *pool, gfp_t gfp_mask,
+			  int nr_to_scan);
+
+/**
+ * ion_pages_sync_for_device - cache flush pages for use with the specified
+ *                             device
+ * @dev:		the device the pages will be used with
+ * @page:		the first page to be flushed
+ * @size:		size in bytes of region to be flushed
+ * @dir:		direction of dma transfer
+ */
+void ion_pages_sync_for_device(struct device *dev, struct page *page,
+		size_t size, enum dma_data_direction dir);
+
+long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);
+
+int ion_sync_for_device(struct ion_client *client, int fd);
+
+struct ion_handle *ion_handle_get_by_id_nolock(struct ion_client *client,
+						int id);
+
+void ion_free_nolock(struct ion_client *client, struct ion_handle *handle);
+
+int ion_handle_put_nolock(struct ion_handle *handle);
+
+int ion_handle_put(struct ion_handle *handle);
+
+int ion_query_heaps(struct ion_client *client, struct ion_heap_query *query);
+
+int ion_share_dma_buf_fd_nolock(struct ion_client *client,
+				struct ion_handle *handle);
+
+#endif /* _ION_PRIV_H */
diff -Nur -X diffexclude a/drivers/staging/android/ion/ion_system_heap.c b/drivers/staging/android/ion/ion_system_heap.c
--- a/drivers/staging/android/ion/ion_system_heap.c	2019-11-24 15:17:01.000000000 +0800
+++ b/drivers/staging/android/ion/ion_system_heap.c	2023-06-09 12:02:57.292923413 +0800
@@ -1,8 +1,17 @@
-// SPDX-License-Identifier: GPL-2.0
 /*
- * ION Memory Allocator system heap exporter
+ * drivers/staging/android/ion/ion_system_heap.c
  *
  * Copyright (C) 2011 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
  */
 
 #include <asm/page.h>
@@ -11,16 +20,17 @@
 #include <linux/highmem.h>
 #include <linux/mm.h>
 #include <linux/scatterlist.h>
+#include <linux/seq_file.h>
 #include <linux/slab.h>
 #include <linux/vmalloc.h>
-
 #include "ion.h"
+#include "ion_priv.h"
 
 #define NUM_ORDERS ARRAY_SIZE(orders)
 
 static gfp_t high_order_gfp_flags = (GFP_HIGHUSER | __GFP_ZERO | __GFP_NOWARN |
 				     __GFP_NORETRY) & ~__GFP_RECLAIM;
-static gfp_t low_order_gfp_flags  = GFP_HIGHUSER | __GFP_ZERO;
+static gfp_t low_order_gfp_flags  = (GFP_HIGHUSER | __GFP_ZERO);
 static const unsigned int orders[] = {8, 4, 0};
 
 static int order_to_index(unsigned int order)
@@ -41,16 +51,34 @@
 
 struct ion_system_heap {
 	struct ion_heap heap;
-	struct ion_page_pool *pools[NUM_ORDERS];
+	struct ion_page_pool *uncached_pools[NUM_ORDERS];
+	struct ion_page_pool *cached_pools[NUM_ORDERS];
 };
 
+/**
+ * The page from page-pool are all zeroed before. We need do cache
+ * clean for cached buffer. The uncached buffer are always non-cached
+ * since it's allocated. So no need for non-cached pages.
+ */
 static struct page *alloc_buffer_page(struct ion_system_heap *heap,
 				      struct ion_buffer *buffer,
 				      unsigned long order)
 {
-	struct ion_page_pool *pool = heap->pools[order_to_index(order)];
+	bool cached = ion_buffer_cached(buffer);
+	struct ion_page_pool *pool;
+	struct page *page;
 
-	return ion_page_pool_alloc(pool);
+	if (!cached)
+		pool = heap->uncached_pools[order_to_index(order)];
+	else
+		pool = heap->cached_pools[order_to_index(order)];
+
+	page = ion_page_pool_alloc(pool);
+
+	if (cached)
+		ion_pages_sync_for_device(NULL, page, PAGE_SIZE << order,
+					  DMA_BIDIRECTIONAL);
+	return page;
 }
 
 static void free_buffer_page(struct ion_system_heap *heap,
@@ -58,6 +86,7 @@
 {
 	struct ion_page_pool *pool;
 	unsigned int order = compound_order(page);
+	bool cached = ion_buffer_cached(buffer);
 
 	/* go to system */
 	if (buffer->private_flags & ION_PRIV_FLAG_SHRINKER_FREE) {
@@ -65,11 +94,15 @@
 		return;
 	}
 
-	pool = heap->pools[order_to_index(order)];
+	if (!cached)
+		pool = heap->uncached_pools[order_to_index(order)];
+	else
+		pool = heap->cached_pools[order_to_index(order)];
 
 	ion_page_pool_free(pool, page);
 }
 
+
 static struct page *alloc_largest_available(struct ion_system_heap *heap,
 					    struct ion_buffer *buffer,
 					    unsigned long size,
@@ -96,7 +129,7 @@
 
 static int ion_system_heap_allocate(struct ion_heap *heap,
 				    struct ion_buffer *buffer,
-				    unsigned long size,
+				    unsigned long size, unsigned long align,
 				    unsigned long flags)
 {
 	struct ion_system_heap *sys_heap = container_of(heap,
@@ -110,7 +143,10 @@
 	unsigned long size_remaining = PAGE_ALIGN(size);
 	unsigned int max_order = orders[0];
 
-	if (size / PAGE_SIZE > totalram_pages() / 2)
+	if (align > PAGE_SIZE)
+		return -EINVAL;
+
+	if ((size / PAGE_SIZE) > (totalram_pages() / 2))
 		return -ENOMEM;
 
 	INIT_LIST_HEAD(&pages);
@@ -124,7 +160,7 @@
 		max_order = compound_order(page);
 		i++;
 	}
-	table = kmalloc(sizeof(*table), GFP_KERNEL);
+	table = kmalloc(sizeof(struct sg_table), GFP_KERNEL);
 	if (!table)
 		goto free_pages;
 
@@ -171,7 +207,8 @@
 static int ion_system_heap_shrink(struct ion_heap *heap, gfp_t gfp_mask,
 				  int nr_to_scan)
 {
-	struct ion_page_pool *pool;
+	struct ion_page_pool *uncached_pool;
+	struct ion_page_pool *cached_pool;
 	struct ion_system_heap *sys_heap;
 	int nr_total = 0;
 	int i, nr_freed;
@@ -183,15 +220,26 @@
 		only_scan = 1;
 
 	for (i = 0; i < NUM_ORDERS; i++) {
-		pool = sys_heap->pools[i];
+		uncached_pool = sys_heap->uncached_pools[i];
+		cached_pool = sys_heap->cached_pools[i];
 
 		if (only_scan) {
-			nr_total += ion_page_pool_shrink(pool,
+			nr_total += ion_page_pool_shrink(uncached_pool,
 							 gfp_mask,
 							 nr_to_scan);
 
+			nr_total += ion_page_pool_shrink(cached_pool,
+							 gfp_mask,
+							 nr_to_scan);
 		} else {
-			nr_freed = ion_page_pool_shrink(pool,
+			nr_freed = ion_page_pool_shrink(uncached_pool,
+							gfp_mask,
+							nr_to_scan);
+			nr_to_scan -= nr_freed;
+			nr_total += nr_freed;
+			if (nr_to_scan <= 0)
+				break;
+			nr_freed = ion_page_pool_shrink(cached_pool,
 							gfp_mask,
 							nr_to_scan);
 			nr_to_scan -= nr_freed;
@@ -212,6 +260,40 @@
 	.shrink = ion_system_heap_shrink,
 };
 
+static int ion_system_heap_debug_show(struct ion_heap *heap, struct seq_file *s,
+				      void *unused)
+{
+
+	struct ion_system_heap *sys_heap = container_of(heap,
+							struct ion_system_heap,
+							heap);
+	int i;
+	struct ion_page_pool *pool;
+
+	for (i = 0; i < NUM_ORDERS; i++) {
+		pool = sys_heap->uncached_pools[i];
+
+		seq_printf(s, "%d order %u highmem pages uncached %lu total\n",
+			   pool->high_count, pool->order,
+			   (PAGE_SIZE << pool->order) * pool->high_count);
+		seq_printf(s, "%d order %u lowmem pages uncached %lu total\n",
+			   pool->low_count, pool->order,
+			   (PAGE_SIZE << pool->order) * pool->low_count);
+	}
+
+	for (i = 0; i < NUM_ORDERS; i++) {
+		pool = sys_heap->cached_pools[i];
+
+		seq_printf(s, "%d order %u highmem pages cached %lu total\n",
+			   pool->high_count, pool->order,
+			   (PAGE_SIZE << pool->order) * pool->high_count);
+		seq_printf(s, "%d order %u lowmem pages cached %lu total\n",
+			   pool->low_count, pool->order,
+			   (PAGE_SIZE << pool->order) * pool->low_count);
+	}
+	return 0;
+}
+
 static void ion_system_heap_destroy_pools(struct ion_page_pool **pools)
 {
 	int i;
@@ -221,7 +303,8 @@
 			ion_page_pool_destroy(pools[i]);
 }
 
-static int ion_system_heap_create_pools(struct ion_page_pool **pools)
+static int ion_system_heap_create_pools(struct ion_page_pool **pools,
+					bool cached)
 {
 	int i;
 
@@ -232,12 +315,11 @@
 		if (orders[i] > 4)
 			gfp_flags = high_order_gfp_flags;
 
-		pool = ion_page_pool_create(gfp_flags, orders[i]);
+		pool = ion_page_pool_create(gfp_flags, orders[i], cached);
 		if (!pool)
 			goto err_create_pool;
 		pools[i] = pool;
 	}
-
 	return 0;
 
 err_create_pool:
@@ -245,7 +327,7 @@
 	return -ENOMEM;
 }
 
-static struct ion_heap *__ion_system_heap_create(void)
+struct ion_heap *ion_system_heap_create(struct ion_platform_heap *unused)
 {
 	struct ion_system_heap *heap;
 
@@ -256,34 +338,41 @@
 	heap->heap.type = ION_HEAP_TYPE_SYSTEM;
 	heap->heap.flags = ION_HEAP_FLAG_DEFER_FREE;
 
-	if (ion_system_heap_create_pools(heap->pools))
+	if (ion_system_heap_create_pools(heap->uncached_pools, false))
 		goto free_heap;
 
+	if (ion_system_heap_create_pools(heap->cached_pools, true))
+		goto destroy_uncached_pools;
+
+	heap->heap.debug_show = ion_system_heap_debug_show;
 	return &heap->heap;
 
+destroy_uncached_pools:
+	ion_system_heap_destroy_pools(heap->uncached_pools);
+
 free_heap:
 	kfree(heap);
 	return ERR_PTR(-ENOMEM);
 }
 
-static int ion_system_heap_create(void)
+void ion_system_heap_destroy(struct ion_heap *heap)
 {
-	struct ion_heap *heap;
-
-	heap = __ion_system_heap_create();
-	if (IS_ERR(heap))
-		return PTR_ERR(heap);
-	heap->name = "ion_system_heap";
-
-	ion_device_add_heap(heap);
+	struct ion_system_heap *sys_heap = container_of(heap,
+							struct ion_system_heap,
+							heap);
+	int i;
 
-	return 0;
+	for (i = 0; i < NUM_ORDERS; i++) {
+		ion_page_pool_destroy(sys_heap->uncached_pools[i]);
+		ion_page_pool_destroy(sys_heap->cached_pools[i]);
+	}
+	kfree(sys_heap);
 }
-device_initcall(ion_system_heap_create);
 
 static int ion_system_contig_heap_allocate(struct ion_heap *heap,
 					   struct ion_buffer *buffer,
 					   unsigned long len,
+					   unsigned long align,
 					   unsigned long flags)
 {
 	int order = get_order(len);
@@ -292,6 +381,9 @@
 	unsigned long i;
 	int ret;
 
+	if (align > (PAGE_SIZE << order))
+		return -EINVAL;
+
 	page = alloc_pages(low_order_gfp_flags | __GFP_NOWARN, order);
 	if (!page)
 		return -ENOMEM;
@@ -302,7 +394,7 @@
 	for (i = len >> PAGE_SHIFT; i < (1 << order); i++)
 		__free_page(page + i);
 
-	table = kmalloc(sizeof(*table), GFP_KERNEL);
+	table = kmalloc(sizeof(struct sg_table), GFP_KERNEL);
 	if (!table) {
 		ret = -ENOMEM;
 		goto free_pages;
@@ -316,6 +408,8 @@
 
 	buffer->sg_table = table;
 
+	ion_pages_sync_for_device(NULL, page, len, DMA_BIDIRECTIONAL);
+
 	return 0;
 
 free_table:
@@ -348,30 +442,19 @@
 	.map_user = ion_heap_map_user,
 };
 
-static struct ion_heap *__ion_system_contig_heap_create(void)
+struct ion_heap *ion_system_contig_heap_create(struct ion_platform_heap *unused)
 {
 	struct ion_heap *heap;
 
-	heap = kzalloc(sizeof(*heap), GFP_KERNEL);
+	heap = kzalloc(sizeof(struct ion_heap), GFP_KERNEL);
 	if (!heap)
 		return ERR_PTR(-ENOMEM);
 	heap->ops = &kmalloc_ops;
 	heap->type = ION_HEAP_TYPE_SYSTEM_CONTIG;
-	heap->name = "ion_system_contig_heap";
-
 	return heap;
 }
 
-static int ion_system_contig_heap_create(void)
+void ion_system_contig_heap_destroy(struct ion_heap *heap)
 {
-	struct ion_heap *heap;
-
-	heap = __ion_system_contig_heap_create();
-	if (IS_ERR(heap))
-		return PTR_ERR(heap);
-
-	ion_device_add_heap(heap);
-
-	return 0;
+	kfree(heap);
 }
-device_initcall(ion_system_contig_heap_create);
diff -Nur -X diffexclude a/drivers/staging/android/ion/sunxi/Makefile b/drivers/staging/android/ion/sunxi/Makefile
--- a/drivers/staging/android/ion/sunxi/Makefile	1970-01-01 08:00:00.000000000 +0800
+++ b/drivers/staging/android/ion/sunxi/Makefile	2023-06-09 12:02:57.292923413 +0800
@@ -0,0 +1 @@
+obj-$(CONFIG_VIDEO_SUNXI_CEDAR_ION) += sunxi_ion.o cache.o
diff -Nur -X diffexclude a/drivers/staging/android/ion/sunxi/cache.h b/drivers/staging/android/ion/sunxi/cache.h
--- a/drivers/staging/android/ion/sunxi/cache.h	1970-01-01 08:00:00.000000000 +0800
+++ b/drivers/staging/android/ion/sunxi/cache.h	2023-06-09 12:02:57.292923413 +0800
@@ -0,0 +1,6 @@
+#ifndef _CACHE_H
+#define _CACHE_H
+int flush_clean_user_range(long start, long end);
+int flush_user_range(long start, long end);
+void flush_dcache_all(void);
+#endif
\ No newline at end of file
diff -Nur -X diffexclude a/drivers/staging/android/ion/sunxi/sunxi_ion.c b/drivers/staging/android/ion/sunxi/sunxi_ion.c
--- a/drivers/staging/android/ion/sunxi/sunxi_ion.c	1970-01-01 08:00:00.000000000 +0800
+++ b/drivers/staging/android/ion/sunxi/sunxi_ion.c	2023-06-09 12:02:57.292923413 +0800
@@ -0,0 +1,191 @@
+/*
+ * Allwinner SUNXI ION Driver
+ *
+ * Copyright (c) 2017 Allwinnertech.
+ *
+ * Author: fanqinghua <fanqinghua@allwinnertech.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#define pr_fmt(fmt) "Ion: " fmt
+
+#include <linux/err.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+#include <linux/of.h>
+#include <linux/mm.h>
+#include <linux/uaccess.h>
+#include <asm/cacheflush.h>
+#include "../ion_priv.h"
+#include "../ion.h"
+#include "../ion_of.h"
+#include "sunxi_ion_priv.h"
+
+struct sunxi_ion_dev {
+	struct ion_heap **heaps;
+	struct ion_device *idev;
+	struct ion_platform_data *data;
+};
+struct device *g_ion_dev;
+struct ion_device *idev;
+/* export for IMG GPU(sgx544) */
+EXPORT_SYMBOL(idev);
+
+static struct ion_of_heap sunxi_heaps[] = {
+	PLATFORM_HEAP("allwinner,sys_user", 0, ION_HEAP_TYPE_SYSTEM,
+		      "sys_user"),
+	PLATFORM_HEAP("allwinner,sys_contig", 1, ION_HEAP_TYPE_SYSTEM_CONTIG,
+		      "sys_contig"),
+	PLATFORM_HEAP("allwinner,carveout", ION_HEAP_TYPE_CARVEOUT, ION_HEAP_TYPE_CARVEOUT, 
+		      "carveout"),
+	PLATFORM_HEAP("allwinner,cma", ION_HEAP_TYPE_DMA, ION_HEAP_TYPE_DMA,
+		      "cma"),
+	PLATFORM_HEAP("allwinner,secure", ION_HEAP_TYPE_SECURE,
+		      ION_HEAP_TYPE_SECURE, "secure"),
+	{}
+};
+
+struct device *get_ion_dev(void)
+{
+	return g_ion_dev;
+}
+
+long sunxi_ion_ioctl(struct ion_client *client, unsigned int cmd,
+		     unsigned long arg)
+{
+	long ret = 0;
+	switch (cmd) {
+	case ION_IOC_SUNXI_FLUSH_RANGE: {
+		sunxi_cache_range range;
+		if (copy_from_user(&range, (void __user *)arg,
+				   sizeof(sunxi_cache_range))) {
+			ret = -EINVAL;
+			goto end;
+		}
+		
+#if __LINUX_ARM_ARCH__ == 7
+		if (flush_clean_user_range(range.start, range.end)) {
+			ret = -EINVAL;
+			goto end;
+		}
+#else
+		dmac_flush_range((void*)range.start, (void*)range.end);
+#endif
+		break;
+	}
+#if __LINUX_ARM_ARCH__ == 7
+	case ION_IOC_SUNXI_FLUSH_ALL: {
+		flush_dcache_all();
+		break;
+	}
+#endif
+	case ION_IOC_SUNXI_PHYS_ADDR: {
+		sunxi_phys_data data;
+		struct ion_handle *handle;
+		if (copy_from_user(&data, (void __user *)arg,
+				   sizeof(sunxi_phys_data)))
+			return -EFAULT;
+		handle =
+			ion_handle_get_by_id_nolock(client, data.handle.handle);
+		/* FIXME Hardcoded CMA struct pointer */
+		data.phys_addr =
+			((struct ion_carveout_buffer_info *)(handle->buffer
+								->priv_virt))
+				->handle;
+		data.size = handle->buffer->size;
+		if (copy_to_user((void __user *)arg, &data,
+				 sizeof(sunxi_phys_data)))
+			return -EFAULT;
+		break;
+	}
+	
+	default:
+		return -ENOTTY;
+	}
+end:
+	return ret;
+}
+
+static int sunxi_ion_probe(struct platform_device *pdev)
+{
+	struct sunxi_ion_dev *ipdev;
+	int i;
+
+	ipdev = devm_kzalloc(&pdev->dev, sizeof(*ipdev), GFP_KERNEL);
+	if (!ipdev)
+		return -ENOMEM;
+
+	g_ion_dev = &pdev->dev;
+	platform_set_drvdata(pdev, ipdev);
+
+	ipdev->idev = ion_device_create(sunxi_ion_ioctl);
+	if (IS_ERR(ipdev->idev))
+		return PTR_ERR(ipdev->idev);
+
+	idev = ipdev->idev;
+
+	ipdev->data = ion_parse_dt(pdev, sunxi_heaps);
+	if (IS_ERR(ipdev->data)) {
+		pr_err("%s: ion_parse_dt error! errorid: %d\n", __func__, ipdev->data);
+		return PTR_ERR(ipdev->data);
+	}
+
+	ipdev->heaps = devm_kzalloc(&pdev->dev,
+				    sizeof(struct ion_heap) * ipdev->data->nr,
+				    GFP_KERNEL);
+	if (!ipdev->heaps) {
+		ion_destroy_platform_data(ipdev->data);
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < ipdev->data->nr; i++) {
+		ipdev->heaps[i] = ion_heap_create(&ipdev->data->heaps[i]);
+		if (!ipdev->heaps) {
+			ion_destroy_platform_data(ipdev->data);
+			return -ENOMEM;
+		} else if (ipdev->heaps[i] == ERR_PTR(-EINVAL)) {
+			return 0;
+		}
+		ion_device_add_heap(ipdev->idev, ipdev->heaps[i]);
+	}
+	return 0;
+}
+
+static int sunxi_ion_remove(struct platform_device *pdev)
+{
+	struct sunxi_ion_dev *ipdev;
+	int i;
+
+	ipdev = platform_get_drvdata(pdev);
+
+	for (i = 0; i < ipdev->data->nr; i++)
+		ion_heap_destroy(ipdev->heaps[i]);
+
+	ion_destroy_platform_data(ipdev->data);
+	ion_device_destroy(ipdev->idev);
+
+	return 0;
+}
+
+static const struct of_device_id sunxi_ion_match_table[] = {
+	{ .compatible = "allwinner,sunxi-ion" },
+	{},
+};
+
+static struct platform_driver sunxi_ion_driver = {
+	.probe = sunxi_ion_probe,
+	.remove = sunxi_ion_remove,
+	.driver = {
+		.name = "ion-sunxi",
+		.of_match_table = sunxi_ion_match_table,
+	},
+};
+
+static int __init sunxi_ion_init(void)
+{
+	return platform_driver_register(&sunxi_ion_driver);
+}
+subsys_initcall(sunxi_ion_init);
diff -Nur -X diffexclude a/drivers/staging/android/ion/sunxi/sunxi_ion.h b/drivers/staging/android/ion/sunxi/sunxi_ion.h
--- a/drivers/staging/android/ion/sunxi/sunxi_ion.h	1970-01-01 08:00:00.000000000 +0800
+++ b/drivers/staging/android/ion/sunxi/sunxi_ion.h	2023-06-09 12:02:57.292923413 +0800
@@ -0,0 +1,33 @@
+/*
+ * drivers/staging/android/ion/sunxi/ion_sunxi.h
+ *
+ * Copyright(c) 2015-2020 Allwinnertech Co., Ltd.
+ *      http://www.allwinnertech.com
+ *
+ * Author: Wim Hwang <huangwei@allwinnertech.com>
+ *
+ * sunxi ion header file
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+
+#ifndef _LINUX_ION_SUNXI_H
+#define _LINUX_ION_SUNXI_H
+
+/**
+ * ion_client_create() -  allocate a client and returns it
+ * @name:	used for debugging
+ */
+struct ion_client *sunxi_ion_client_create(const char *name);
+
+void sunxi_ion_probe_drm_info(u32 *drm_phy_addr, u32 *drm_tee_addr);
+
+int  optee_probe_drm_configure(
+		unsigned long *drm_base,
+		size_t *drm_size,
+		unsigned long  *tee_base);
+
+#endif
diff -Nur -X diffexclude a/drivers/staging/android/ion/sunxi/sunxi_ion_priv.h b/drivers/staging/android/ion/sunxi/sunxi_ion_priv.h
--- a/drivers/staging/android/ion/sunxi/sunxi_ion_priv.h	1970-01-01 08:00:00.000000000 +0800
+++ b/drivers/staging/android/ion/sunxi/sunxi_ion_priv.h	2023-06-09 12:02:57.292923413 +0800
@@ -0,0 +1,35 @@
+#ifndef _SUNXI_ION_PRIV_H
+#define _SUNXI_ION_PRIV_H
+
+#include "cache.h"
+
+#define ION_IOC_SUNXI_FLUSH_RANGE           5
+#define ION_IOC_SUNXI_FLUSH_ALL             6
+#define ION_IOC_SUNXI_PHYS_ADDR             7
+#define ION_IOC_SUNXI_DMA_COPY              8
+#define ION_IOC_SUNXI_DUMP                  9
+#define ION_IOC_SUNXI_POOL_FREE             10
+
+typedef struct {
+	long 	start;
+	long 	end;
+}sunxi_cache_range;
+
+typedef struct {
+	struct ion_handle_data handle;
+	unsigned int phys_addr;
+	unsigned int size;
+}sunxi_phys_data;
+
+struct ion_cma_buffer_info {
+	void *cpu_addr;
+	dma_addr_t handle;
+	struct sg_table *table;
+};
+
+struct ion_carveout_buffer_info {
+	ion_phys_addr_t handle;
+	struct sg_table *table;
+};
+
+#endif
diff -Nur -X diffexclude a/drivers/staging/android/ion/uapi/ion.h b/drivers/staging/android/ion/uapi/ion.h
--- a/drivers/staging/android/ion/uapi/ion.h	1970-01-01 08:00:00.000000000 +0800
+++ b/drivers/staging/android/ion/uapi/ion.h	2023-06-09 12:02:57.292923413 +0800
@@ -0,0 +1,237 @@
+/*
+ * drivers/staging/android/uapi/ion.h
+ *
+ * Copyright (C) 2011 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#ifndef _UAPI_LINUX_ION_H
+#define _UAPI_LINUX_ION_H
+
+#include <linux/ioctl.h>
+#include <linux/types.h>
+
+typedef int ion_user_handle_t;
+
+/**
+ * enum ion_heap_types - list of all possible types of heaps
+ * @ION_HEAP_TYPE_SYSTEM:	 memory allocated via vmalloc
+ * @ION_HEAP_TYPE_SYSTEM_CONTIG: memory allocated via kmalloc
+ * @ION_HEAP_TYPE_CARVEOUT:	 memory allocated from a prereserved
+ *				 carveout heap, allocations are physically
+ *				 contiguous
+ * @ION_HEAP_TYPE_DMA:		 memory allocated via DMA API
+ * @ION_NUM_HEAPS:		 helper for iterating over heaps, a bit mask
+ *				 is used to identify the heaps, so only 32
+ *				 total heap types are supported
+ */
+enum ion_heap_type {
+	ION_HEAP_TYPE_SYSTEM,
+	ION_HEAP_TYPE_SYSTEM_CONTIG,
+	ION_HEAP_TYPE_CARVEOUT,
+	ION_HEAP_TYPE_CHUNK,
+	ION_HEAP_TYPE_DMA,
+	ION_HEAP_TYPE_SECURE, /* allwinner add */
+	ION_HEAP_TYPE_CUSTOM, /*
+			       * must be last so device specific heaps always
+			       * are at the end of this enum
+			       */
+};
+
+#define ION_NUM_HEAP_IDS		(sizeof(unsigned int) * 8)
+
+/**
+ * allocation flags - the lower 16 bits are used by core ion, the upper 16
+ * bits are reserved for use by the heaps themselves.
+ */
+
+/*
+ * mappings of this buffer should be cached, ion will do cache maintenance
+ * when the buffer is mapped for dma
+ */
+#define ION_FLAG_CACHED 1
+
+/*
+ * mappings of this buffer will created at mmap time, if this is set
+ * caches must be managed manually
+ */
+#define ION_FLAG_CACHED_NEEDS_SYNC 2
+
+/**
+ * DOC: Ion Userspace API
+ *
+ * create a client by opening /dev/ion
+ * most operations handled via following ioctls
+ *
+ */
+
+/**
+ * struct ion_allocation_data - metadata passed from userspace for allocations
+ * @len:		size of the allocation
+ * @align:		required alignment of the allocation
+ * @heap_id_mask:	mask of heap ids to allocate from
+ * @flags:		flags passed to heap
+ * @handle:		pointer that will be populated with a cookie to use to
+ *			refer to this allocation
+ *
+ * Provided by userspace as an argument to the ioctl
+ */
+struct ion_allocation_data {
+	size_t len;
+	size_t align;
+	unsigned int heap_id_mask;
+	unsigned int flags;
+	ion_user_handle_t handle;
+};
+
+/**
+ * struct ion_fd_data - metadata passed to/from userspace for a handle/fd pair
+ * @handle:	a handle
+ * @fd:		a file descriptor representing that handle
+ *
+ * For ION_IOC_SHARE or ION_IOC_MAP userspace populates the handle field with
+ * the handle returned from ion alloc, and the kernel returns the file
+ * descriptor to share or map in the fd field.  For ION_IOC_IMPORT, userspace
+ * provides the file descriptor and the kernel returns the handle.
+ */
+struct ion_fd_data {
+	ion_user_handle_t handle;
+	int fd;
+};
+
+/**
+ * struct ion_handle_data - a handle passed to/from the kernel
+ * @handle:	a handle
+ */
+struct ion_handle_data {
+	ion_user_handle_t handle;
+};
+
+/**
+ * struct ion_custom_data - metadata passed to/from userspace for a custom ioctl
+ * @cmd:	the custom ioctl function to call
+ * @arg:	additional data to pass to the custom ioctl, typically a user
+ *		pointer to a predefined structure
+ *
+ * This works just like the regular cmd and arg fields of an ioctl.
+ */
+struct ion_custom_data {
+	unsigned int cmd;
+	unsigned long arg;
+};
+
+#define MAX_HEAP_NAME			32
+
+/**
+ * struct ion_heap_data - data about a heap
+ * @name - first 32 characters of the heap name
+ * @type - heap type
+ * @heap_id - heap id for the heap
+ */
+struct ion_heap_data {
+	char name[MAX_HEAP_NAME];
+	__u32 type;
+	__u32 heap_id;
+	__u32 reserved0;
+	__u32 reserved1;
+	__u32 reserved2;
+};
+
+/**
+ * struct ion_heap_query - collection of data about all heaps
+ * @cnt - total number of heaps to be copied
+ * @heaps - buffer to copy heap data
+ */
+struct ion_heap_query {
+	__u32 cnt; /* Total number of heaps to be copied */
+	__u32 reserved0; /* align to 64bits */
+	__u64 heaps; /* buffer to be populated */
+	__u32 reserved1;
+	__u32 reserved2;
+};
+
+#define ION_IOC_MAGIC		'I'
+
+/**
+ * DOC: ION_IOC_ALLOC - allocate memory
+ *
+ * Takes an ion_allocation_data struct and returns it with the handle field
+ * populated with the opaque handle for the allocation.
+ */
+#define ION_IOC_ALLOC		_IOWR(ION_IOC_MAGIC, 0, \
+				      struct ion_allocation_data)
+
+/**
+ * DOC: ION_IOC_FREE - free memory
+ *
+ * Takes an ion_handle_data struct and frees the handle.
+ */
+#define ION_IOC_FREE		_IOWR(ION_IOC_MAGIC, 1, struct ion_handle_data)
+
+/**
+ * DOC: ION_IOC_MAP - get a file descriptor to mmap
+ *
+ * Takes an ion_fd_data struct with the handle field populated with a valid
+ * opaque handle.  Returns the struct with the fd field set to a file
+ * descriptor open in the current address space.  This file descriptor
+ * can then be used as an argument to mmap.
+ */
+#define ION_IOC_MAP		_IOWR(ION_IOC_MAGIC, 2, struct ion_fd_data)
+
+/**
+ * DOC: ION_IOC_SHARE - creates a file descriptor to use to share an allocation
+ *
+ * Takes an ion_fd_data struct with the handle field populated with a valid
+ * opaque handle.  Returns the struct with the fd field set to a file
+ * descriptor open in the current address space.  This file descriptor
+ * can then be passed to another process.  The corresponding opaque handle can
+ * be retrieved via ION_IOC_IMPORT.
+ */
+#define ION_IOC_SHARE		_IOWR(ION_IOC_MAGIC, 4, struct ion_fd_data)
+
+/**
+ * DOC: ION_IOC_IMPORT - imports a shared file descriptor
+ *
+ * Takes an ion_fd_data struct with the fd field populated with a valid file
+ * descriptor obtained from ION_IOC_SHARE and returns the struct with the handle
+ * filed set to the corresponding opaque handle.
+ */
+#define ION_IOC_IMPORT		_IOWR(ION_IOC_MAGIC, 5, struct ion_fd_data)
+
+/**
+ * DOC: ION_IOC_SYNC - syncs a shared file descriptors to memory
+ *
+ * Deprecated in favor of using the dma_buf api's correctly (syncing
+ * will happen automatically when the buffer is mapped to a device).
+ * If necessary should be used after touching a cached buffer from the cpu,
+ * this will make the buffer in memory coherent.
+ */
+#define ION_IOC_SYNC		_IOWR(ION_IOC_MAGIC, 7, struct ion_fd_data)
+
+/**
+ * DOC: ION_IOC_CUSTOM - call architecture specific ion ioctl
+ *
+ * Takes the argument of the architecture specific ioctl to call and
+ * passes appropriate userdata for that ioctl
+ */
+#define ION_IOC_CUSTOM		_IOWR(ION_IOC_MAGIC, 6, struct ion_custom_data)
+
+/**
+ * DOC: ION_IOC_HEAP_QUERY - information about available heaps
+ *
+ * Takes an ion_heap_query structure and populates information about
+ * available Ion heaps.
+ */
+#define ION_IOC_HEAP_QUERY     _IOWR(ION_IOC_MAGIC, 8, \
+					struct ion_heap_query)
+
+#endif /* _UAPI_LINUX_ION_H */
diff -Nur -X diffexclude a/drivers/staging/media/sunxi/Kconfig b/drivers/staging/media/sunxi/Kconfig
--- a/drivers/staging/media/sunxi/Kconfig	2019-11-24 15:17:01.000000000 +0800
+++ b/drivers/staging/media/sunxi/Kconfig	2023-06-09 12:04:53.997822191 +0800
@@ -13,4 +13,6 @@
 
 source "drivers/staging/media/sunxi/cedrus/Kconfig"
 
+source "drivers/staging/media/sunxi/cedar_ve/Kconfig"
+
 endif
diff -Nur -X diffexclude a/drivers/staging/media/sunxi/Makefile b/drivers/staging/media/sunxi/Makefile
--- a/drivers/staging/media/sunxi/Makefile	2019-11-24 15:17:01.000000000 +0800
+++ b/drivers/staging/media/sunxi/Makefile	2023-06-09 12:05:06.013914739 +0800
@@ -1,2 +1,3 @@
 # SPDX-License-Identifier: GPL-2.0
 obj-$(CONFIG_VIDEO_SUNXI_CEDRUS)	+= cedrus/
+obj-$(CONFIG_VIDEO_SUNXI_CEDAR_VE)    += cedar_ve/
diff -Nur -X diffexclude a/drivers/staging/media/sunxi/cedar_ve/Kconfig b/drivers/staging/media/sunxi/cedar_ve/Kconfig
--- a/drivers/staging/media/sunxi/cedar_ve/Kconfig	1970-01-01 08:00:00.000000000 +0800
+++ b/drivers/staging/media/sunxi/cedar_ve/Kconfig	2023-06-09 12:02:57.292923413 +0800
@@ -0,0 +1,8 @@
+config VIDEO_SUNXI_CEDAR_VE
+	tristate "Allwinner CedarX VideoEngine Driver"
+	select DMA_SHARED_BUFFER
+	help
+		This is the driver for sunxi video decoder, including h264/
+		mpeg4/mpeg2/vc1/rmvb.
+		To compile this driver as a module, choose M here: the
+		module will be called cedar_dev.
diff -Nur -X diffexclude a/drivers/staging/media/sunxi/cedar_ve/Makefile b/drivers/staging/media/sunxi/cedar_ve/Makefile
--- a/drivers/staging/media/sunxi/cedar_ve/Makefile	1970-01-01 08:00:00.000000000 +0800
+++ b/drivers/staging/media/sunxi/cedar_ve/Makefile	2023-06-09 14:20:28.294920432 +0800
@@ -0,0 +1 @@
+obj-$(CONFIG_VIDEO_SUNXI_CEDAR_VE) += cedar_ve.o cache-v7.o
diff -Nur -X diffexclude a/drivers/staging/media/sunxi/cedar_ve/cedar_ve.c b/drivers/staging/media/sunxi/cedar_ve/cedar_ve.c
--- a/drivers/staging/media/sunxi/cedar_ve/cedar_ve.c	1970-01-01 08:00:00.000000000 +0800
+++ b/drivers/staging/media/sunxi/cedar_ve/cedar_ve.c	2023-06-09 13:34:36.442731344 +0800
@@ -0,0 +1,1997 @@
+/*
+ * drivers\media\cedar_ve
+ * (C) Copyright 2010-2016
+ * Reuuimlla Technology Co., Ltd. <www.allwinnertech.com>
+ * fangning<fangning@allwinnertech.com>
+ *
+ * some simple description for this code
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
+ *
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/ioctl.h>
+#include <linux/fs.h>
+#include <linux/device.h>
+#include <linux/err.h>
+#include <linux/list.h>
+#include <linux/errno.h>
+#include <linux/mutex.h>
+#include <linux/slab.h>
+#include <linux/preempt.h>
+#include <linux/cdev.h>
+#include <linux/platform_device.h>
+#include <linux/interrupt.h>
+#include <linux/clk.h>
+#include <linux/rmap.h>
+#include <linux/wait.h>
+#include <linux/semaphore.h>
+#include <linux/poll.h>
+#include <linux/spinlock.h>
+#include <linux/sched.h>
+#include <linux/kthread.h>
+#include <linux/delay.h>
+#include <linux/version.h>
+#include <asm/uaccess.h>
+#include <asm/io.h>
+#include <asm/dma.h>
+#include <linux/dma-mapping.h>
+#include <linux/mm.h>
+#include <asm/siginfo.h>
+#include <asm/signal.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)
+#include <linux/reset.h>
+#include <linux/sched/signal.h>
+#include <linux/mfd/syscon.h>
+#include <linux/regmap.h>
+#else /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+#include <linux/clk/sunxi.h>
+#endif /*LINUX_VERSION_CODE < KERNEL_VERSION(4,11,0)*/
+
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+
+#include "cedar_ve.h"
+#include <linux/regulator/consumer.h>
+#include <linux/of_reserved_mem.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5,1,0)
+#include <linux/soc/sunxi/sunxi_sram.h>
+#endif
+
+#define MMAP_UNCACHABLE
+
+#ifndef MMAP_UNCACHABLE
+extern int flush_clean_user_range(long start, long end);
+#endif
+
+struct regulator *regu;
+
+//#define USE_ION
+//#define CEDAR_DEBUG
+#define HAVE_TIMER_SETUP
+
+#define DRV_VERSION "0.01beta"
+
+#undef USE_CEDAR_ENGINE
+
+#ifndef CEDARDEV_MAJOR
+#define CEDARDEV_MAJOR (150)
+#endif
+#ifndef CEDARDEV_MINOR
+#define CEDARDEV_MINOR (0)
+#endif
+
+#define MACC_REGS_BASE      (0x01C0E000)           // Media ACCelerate
+
+#ifndef CONFIG_OF
+//H3: arch/arm/mach-sunxi/include/mach/sun8i/irqs-sun8iw7p1.h
+//#define SUNXI_IRQ_VE		(90)
+#endif
+
+#define cedar_ve_printk(level, msg...) printk(level "cedar_ve: " msg)
+
+#define VE_CLK_HIGH_WATER  (700)//400MHz
+#define VE_CLK_LOW_WATER   (100) //160MHz
+
+static int g_dev_major = CEDARDEV_MAJOR;
+static int g_dev_minor = CEDARDEV_MINOR;
+module_param(g_dev_major, int, S_IRUGO);//S_IRUGO represent that g_dev_major can be read,but canot be write
+module_param(g_dev_minor, int, S_IRUGO);
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)
+#define SYSCON_SRAM_CTRL_REG0	0x0
+#define SYSCON_SRAM_C1_MAP_VE	0x7fffffff
+#else /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+struct clk *ve_moduleclk = NULL;
+struct clk *ve_parent_pll_clk = NULL;
+struct clk *ve_power_gating = NULL;
+static u32 ve_parent_clk_rate = 300000000;
+#endif /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+
+struct iomap_para{
+	volatile char* regs_macc;
+	volatile char* regs_avs;
+};
+
+static DECLARE_WAIT_QUEUE_HEAD(wait_ve);
+struct cedar_dev {
+	struct cdev cdev;	             /* char device struct                 */
+        struct platform_device  *pdev;
+        struct device *dev;
+        struct device *odev;              /* ptr to class device struct         */
+	struct class  *class;            /* class for auto create device node  */
+
+	struct semaphore sem;            /* mutual exclusion semaphore         */
+
+	wait_queue_head_t wq;            /* wait queue for poll ops            */
+
+	struct iomap_para iomap_addrs;   /* io remap addrs                     */
+
+	struct timer_list cedar_engine_timer;
+	struct timer_list cedar_engine_timer_rel;
+
+	u32 irq;                         /* cedar video engine irq number      */
+	u32 de_irq_flag;                    /* flag of video decoder engine irq generated */
+	u32 de_irq_value;                   /* value of video decoder engine irq          */
+	u32 en_irq_flag;                    /* flag of video encoder engine irq generated */
+	u32 en_irq_value;                   /* value of video encoder engine irq          */
+	u32 irq_has_enable;
+	u32 ref_count;
+
+	u32 jpeg_irq_flag;                    /* flag of video jpeg dec irq generated */
+	u32 jpeg_irq_value;                   /* value of video jpeg dec  irq */
+
+	volatile u32* sram_bass_vir ;
+	volatile u32* clk_bass_vir;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)
+  struct clk *mod_clk;
+  struct clk *ahb_clk;
+  struct clk *ram_clk;
+  struct reset_control *rstc;
+  struct regmap *syscon;
+#endif /*LINUX_VERSION_CODE < KERNEL_VERSION(4,11,0)*/
+  	unsigned long ve_start;
+	unsigned long ve_size;
+	void *ve_start_virt;
+	resource_size_t ve_start_pa;
+};
+
+struct ve_info {
+	u32 set_vol_flag;
+};
+
+struct cedar_dev *cedar_devp;
+struct file *ve_file;
+
+u32 int_sta=0,int_value;
+
+/*
+ * Video engine interrupt service routine
+ * To wake up ve wait queue
+ */
+
+#if defined(CONFIG_OF)
+static struct of_device_id sunxi_cedar_ve_match[] = {
+	{ .compatible = "allwinner,sunxi-cedar-ve", },
+	//{ .compatible = "allwinner,sun8i-h3-video-engine", },
+	{}
+};
+MODULE_DEVICE_TABLE(of, sunxi_cedar_ve_match);
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0) || ((defined CONFIG_ARCH_SUN8IW7P1) || (defined CONFIG_ARCH_SUN8IW8P1) || (defined CONFIG_ARCH_SUN8IW9P1))
+#if  defined(CONFIG_SUNXI_TRUSTZONE)
+static inline u32 sunxi_smc_readl(void __iomem *addr)
+{
+if (sunxi_soc_is_secure()) {
+return call_firmware_op(read_reg, addr);
+} else {
+return readl(addr);
+}
+
+}
+static inline void sunxi_smc_writel(u32 val, void __iomem *addr)
+{
+  if (sunxi_soc_is_secure()) {
+    call_firmware_op(write_reg, val, addr);
+  } else {
+    writel(val, addr);
+  }
+}
+#else
+static inline u32 sunxi_smc_readl(void __iomem *addr)
+{
+return readl(addr);
+}
+static inline void sunxi_smc_writel(u32 val, void __iomem *addr)
+{
+  writel(val, addr);
+}
+#endif
+#endif /*((defined CONFIG_ARCH_SUN8IW7P1) || (defined CONFIG_ARCH_SUN8IW8P1) || (defined CONFIG_ARCH_SUN8IW9P1))*/
+
+static irqreturn_t VideoEngineInterupt(int irq, void *dev)
+{
+	ulong ve_int_status_reg;
+	ulong ve_int_ctrl_reg;
+	u32 status = 0;
+	volatile int val;
+	int modual_sel = 0;
+	u32 interrupt_enable = 0;
+	struct iomap_para addrs = cedar_devp->iomap_addrs;
+
+	modual_sel = readl(addrs.regs_macc + 0);
+
+	//printk("!!!!!!!!! [cedar ISR]: modual_sel %08x\n", modual_sel);
+
+	//return IRQ_HANDLED;
+//001300cb
+//001300c1
+	/* ENCODER (EN) case for VE 1633 and newer */
+	if ((modual_sel&(3<<6)) || (modual_sel == 0xB)) {
+
+		if (modual_sel == 0xB) {
+			/*avc enc*/
+			ve_int_status_reg = (ulong)(addrs.regs_macc + 0xb00 + 0x1c);
+    		ve_int_ctrl_reg = (ulong)(addrs.regs_macc + 0xb00 + 0x14);
+			interrupt_enable = readl((void*)ve_int_ctrl_reg) &(0x7);
+			status = readl((void*)ve_int_status_reg);
+			status &= 0xf;
+		} else {
+			modual_sel &= ~(0xF);
+
+			if (modual_sel&(1<<7)) {
+				/*avc enc*/
+				//printk("[cedrus]: AVC interrupt!!!\n");
+				ve_int_status_reg = (ulong)(addrs.regs_macc + 0xb00 + 0x1c);
+				ve_int_ctrl_reg = (ulong)(addrs.regs_macc + 0xb00 + 0x14);
+				interrupt_enable = readl((void*)ve_int_ctrl_reg) &(0x7);
+				status = readl((void*)ve_int_status_reg);
+				status &= 0xf;
+			} else {
+				/*isp*/
+				ve_int_status_reg = (ulong)(addrs.regs_macc + 0xa00 + 0x10);
+				ve_int_ctrl_reg = (ulong)(addrs.regs_macc + 0xa00 + 0x08);
+				interrupt_enable = readl((void*)ve_int_ctrl_reg) &(0x1);
+				status = readl((void*)ve_int_status_reg);
+				status &= 0x1;
+			}
+		}
+
+		/*modify by fangning 2013-05-22*/
+		if (status && interrupt_enable) {
+			/*disable interrupt*/
+			if (modual_sel == 0xB) {
+				ve_int_ctrl_reg = (ulong)(addrs.regs_macc + 0xb00 + 0x14);
+				val = readl((void*)ve_int_ctrl_reg);
+				writel(val & (~0xf), (void*)ve_int_ctrl_reg);
+			} else {
+				/*avc enc*/
+				if (modual_sel&(1<<7)) {
+					//printk("[cedrus]: Read AVC interrupt!!!\n");
+					ve_int_ctrl_reg = (ulong)(addrs.regs_macc + 0xb00 + 0x14);
+					val = readl((void*)ve_int_ctrl_reg);
+					writel(val & (~0x7), (void*)ve_int_ctrl_reg);
+				} else {
+					/*isp*/
+					ve_int_ctrl_reg = (ulong)(addrs.regs_macc + 0xa00 + 0x08);
+					val = readl((void*)ve_int_ctrl_reg);
+					writel(val & (~0x1), (void*)ve_int_ctrl_reg);
+				}
+			}
+			/*hx modify 2011-8-1 16:08:47*/
+			cedar_devp->en_irq_value = 1;
+			cedar_devp->en_irq_flag = 1;
+			/*any interrupt will wake up wait queue*/
+			//printk("[cedrus]: Video interrupt occurs EN!!!\n");
+			wake_up_interruptible(&wait_ve);
+		}
+		return IRQ_HANDLED;
+	}
+
+#if ((defined CONFIG_ARCH_SUN8IW8P1) || (defined CONFIG_ARCH_SUN50I))
+	if (modual_sel&(0x20)) {
+		ve_int_status_reg = (ulong)(addrs.regs_macc + 0xe00 + 0x1c);
+		ve_int_ctrl_reg = (ulong)(addrs.regs_macc + 0xe00 + 0x14);
+		interrupt_enable = readl((void*)ve_int_ctrl_reg) & (0x38);
+
+		status = readl((void*)ve_int_status_reg);
+
+		if ((status&0x7) && interrupt_enable) {
+			/*disable interrupt*/
+			val = readl((void*)ve_int_ctrl_reg);
+			writel(val & (~0x38), (void*)ve_int_ctrl_reg);
+
+			cedar_devp->jpeg_irq_value = 1;
+			cedar_devp->jpeg_irq_flag = 1;
+
+			/*any interrupt will wake up wait queue*/
+			wake_up_interruptible(&wait_ve);
+		}
+	}
+#endif
+
+	/* DECODER (DE) case for all VE versions */
+	modual_sel &= 0xf;
+	if((modual_sel<=4) /*|| (modual_sel == 0xB)*/) {
+		/*estimate Which video format*/
+		switch (modual_sel)
+		{
+			case 0: /*mpeg124*/
+				ve_int_status_reg = (ulong)
+					(addrs.regs_macc + 0x100 + 0x1c);
+				ve_int_ctrl_reg = (ulong)(addrs.regs_macc + 0x100 + 0x14);
+				interrupt_enable = readl((void*)ve_int_ctrl_reg) & (0x7c);
+				break;
+			case 1: /*h264*/
+				ve_int_status_reg = (ulong)
+					(addrs.regs_macc + 0x200 + 0x28);
+				ve_int_ctrl_reg = (ulong)(addrs.regs_macc + 0x200 + 0x20);
+				interrupt_enable = readl((void*)ve_int_ctrl_reg) & (0xf);
+				break;
+			case 2: /*vc1*/
+				ve_int_status_reg = (ulong)(addrs.regs_macc +
+					0x300 + 0x2c);
+				ve_int_ctrl_reg = (ulong)(addrs.regs_macc + 0x300 + 0x24);
+				interrupt_enable = readl((void*)ve_int_ctrl_reg) & (0xf);
+				break;
+			case 3: /*rmvb*/
+				ve_int_status_reg = (ulong)
+					(addrs.regs_macc + 0x400 + 0x1c);
+				ve_int_ctrl_reg = (ulong)(addrs.regs_macc + 0x400 + 0x14);
+				interrupt_enable = readl((void*)ve_int_ctrl_reg) & (0xf);
+				break;
+
+			case 4: /*hevc*/
+				ve_int_status_reg = (ulong)
+					(addrs.regs_macc + 0x500 + 0x38);
+				ve_int_ctrl_reg = (ulong)(addrs.regs_macc + 0x500 + 0x30);
+				interrupt_enable = readl((void*)ve_int_ctrl_reg) & (0xf);
+				break;
+#if 0
+			case 0xB: /*AVC (h264 encoder)*/
+				ve_int_status_reg = (unsigned int)(addrs.regs_macc + 0xb00 + 0x1c);
+    			ve_int_ctrl_reg = (unsigned int)(addrs.regs_macc + 0xb00 + 0x14);
+				interrupt_enable = readl((void*)ve_int_ctrl_reg) &(0x7);
+				break;	
+#endif
+			default:   
+				ve_int_status_reg = (ulong)(addrs.regs_macc + 0x100 + 0x1c);
+				ve_int_ctrl_reg = (ulong)(addrs.regs_macc + 0x100 + 0x14);
+				interrupt_enable = readl((void*)ve_int_ctrl_reg) & (0xf);
+				cedar_ve_printk(KERN_WARNING, "ve mode :%x "
+					"not defined!\n", modual_sel);
+				break;
+		}
+
+		status = readl((void*)ve_int_status_reg);
+
+		/*modify by fangning 2013-05-22*/
+		if ((status&0xf) && interrupt_enable) {
+			/*disable interrupt*/
+			if (modual_sel == 0) {
+				val = readl((void*)ve_int_ctrl_reg);
+				writel(val & (~0x7c), (void*)ve_int_ctrl_reg);
+			} else {
+				val = readl((void*)ve_int_ctrl_reg);
+				writel(val & (~0xf), (void*)ve_int_ctrl_reg);
+			}
+
+			cedar_devp->de_irq_value = 1;
+			cedar_devp->de_irq_flag = 1;
+			/*any interrupt will wake up wait queue*/
+			wake_up_interruptible(&wait_ve);
+		}
+	}
+	//printk("%08X\n", modual_sel);
+	return IRQ_HANDLED;
+}
+
+static int clk_status = 0;
+static LIST_HEAD(run_task_list);
+static LIST_HEAD(del_task_list);
+static spinlock_t cedar_spin_lock;
+#define CEDAR_RUN_LIST_NONULL	-1
+#define CEDAR_NONBLOCK_TASK  0
+#define CEDAR_BLOCK_TASK 1
+#define CLK_REL_TIME 10000
+#define TIMER_CIRCLE 50
+#define TASK_INIT      0x00
+#define TASK_TIMEOUT   0x55
+#define TASK_RELEASE   0xaa
+#define SIG_CEDAR		35
+
+static int enable_cedar_hw_clk(void)
+{
+	ulong flags;
+	int res = -EFAULT;
+
+	spin_lock_irqsave(&cedar_spin_lock, flags);		
+
+	if (clk_status == 1)
+		goto out;
+
+	clk_status = 1;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)
+	reset_control_deassert(cedar_devp->rstc);
+	if (clk_enable(cedar_devp->mod_clk)) { cedar_ve_printk(KERN_WARNING, "try to enable ve_moduleclk failed! (%d=\n", __LINE__); goto out; }
+	res = 0;
+#else /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+	sunxi_periph_reset_deassert(ve_moduleclk);
+	if (clk_enable(ve_moduleclk)) {
+		cedar_ve_printk(KERN_WARNING, "enable ve_moduleclk failed;\n");
+		goto out;
+	}else {
+		res = 0;
+	}
+#endif /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+
+#ifdef CEDAR_DEBUG
+	printk("%s,%d\n",__func__,__LINE__);
+#endif
+
+out:
+	spin_unlock_irqrestore(&cedar_spin_lock, flags);
+	return res;
+}
+
+int disable_cedar_hw_clk(void)
+{
+	ulong flags;
+	int res = -EFAULT;
+
+	spin_lock_irqsave(&cedar_spin_lock, flags);		
+
+	if (clk_status == 0) {
+		res = 0;
+		goto out;
+	}
+	clk_status = 0;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)
+	if ((NULL == cedar_devp->mod_clk)||(IS_ERR(cedar_devp->mod_clk))) {
+		cedar_ve_printk(KERN_WARNING, "ve_moduleclk is invalid\n");
+	} else {
+		clk_disable(cedar_devp->mod_clk);
+		reset_control_assert(cedar_devp->rstc);
+		res = 0;
+	}
+#else /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+	if ((NULL == ve_moduleclk)||(IS_ERR(ve_moduleclk))) {
+		cedar_ve_printk(KERN_WARNING, "ve_moduleclk is invalid\n");
+	} else {
+		clk_disable(ve_moduleclk);
+		sunxi_periph_reset_assert(ve_moduleclk);
+		res = 0;
+	}
+#endif /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+
+#ifdef CEDAR_DEBUG
+	printk("%s,%d\n",__func__,__LINE__);
+#endif
+
+out:
+	spin_unlock_irqrestore(&cedar_spin_lock, flags);
+	return res;
+}
+
+void cedardev_insert_task(struct cedarv_engine_task* new_task)
+{	
+	struct cedarv_engine_task *task_entry;
+	ulong flags;
+
+	spin_lock_irqsave(&cedar_spin_lock, flags);		
+
+	if (list_empty(&run_task_list))
+		new_task->is_first_task = 1;
+
+
+	list_for_each_entry(task_entry, &run_task_list, list) {
+		if ((task_entry->is_first_task == 0) && (task_entry->running == 0) && (task_entry->t.task_prio < new_task->t.task_prio)) {
+			break;
+		}
+	}
+
+	list_add(&new_task->list, task_entry->list.prev);	
+
+#ifdef CEDAR_DEBUG
+	printk("%s,%d, TASK_ID:",__func__,__LINE__);
+	list_for_each_entry(task_entry, &run_task_list, list) {
+		printk("%d!", task_entry->t.ID);
+	}
+	printk("\n");
+#endif
+
+	mod_timer(&cedar_devp->cedar_engine_timer, jiffies + 0);
+
+	spin_unlock_irqrestore(&cedar_spin_lock, flags);
+}
+
+int cedardev_del_task(int task_id)
+{
+	struct cedarv_engine_task *task_entry;
+	ulong flags;
+
+	spin_lock_irqsave(&cedar_spin_lock, flags);		
+
+	list_for_each_entry(task_entry, &run_task_list, list) {
+		if (task_entry->t.ID == task_id && task_entry->status != TASK_RELEASE) {
+			task_entry->status = TASK_RELEASE;
+
+			spin_unlock_irqrestore(&cedar_spin_lock, flags);
+			mod_timer(&cedar_devp->cedar_engine_timer, jiffies + 0);
+			return 0;
+		}
+	}
+	spin_unlock_irqrestore(&cedar_spin_lock, flags);
+
+	return -1;
+}
+
+int cedardev_check_delay(int check_prio)
+{
+	struct cedarv_engine_task *task_entry;
+	int timeout_total = 0;
+	ulong flags;
+
+	spin_lock_irqsave(&cedar_spin_lock, flags);
+	list_for_each_entry(task_entry, &run_task_list, list) {
+		if ((task_entry->t.task_prio >= check_prio) || (task_entry->running == 1) || (task_entry->is_first_task == 1))							
+			timeout_total = timeout_total + task_entry->t.frametime;
+	}
+
+	spin_unlock_irqrestore(&cedar_spin_lock, flags);
+#ifdef CEDAR_DEBUG
+	printk("%s,%d,%d\n", __func__, __LINE__, timeout_total);
+#endif
+	return timeout_total;
+}
+
+#ifdef HAVE_TIMER_SETUP
+static void cedar_engine_for_timer_rel(struct timer_list *t)
+#else
+static void cedar_engine_for_timer_rel(unsigned long arg)
+#endif
+{
+	ulong flags;
+	int ret = 0;
+	spin_lock_irqsave(&cedar_spin_lock, flags);		
+
+	if (list_empty(&run_task_list)) {
+		ret = disable_cedar_hw_clk(); 
+		if (ret < 0) {
+			cedar_ve_printk(KERN_WARNING, "clk disable error!\n");
+		}
+	} else {
+		cedar_ve_printk(KERN_WARNING, "clk disable time out "
+			"but task left\n");
+		mod_timer( &cedar_devp->cedar_engine_timer, jiffies + msecs_to_jiffies(TIMER_CIRCLE));
+	}
+
+	spin_unlock_irqrestore(&cedar_spin_lock, flags);
+}
+
+#ifdef HAVE_TIMER_SETUP
+static void cedar_engine_for_events(struct timer_list *t)
+#else
+static void cedar_engine_for_events(unsigned long arg)
+#endif
+{
+	struct cedarv_engine_task *task_entry, *task_entry_tmp;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,19,0)	
+	struct kernel_siginfo info;
+#else 
+	struct siginfo info;
+#endif	
+	ulong flags;
+
+	spin_lock_irqsave(&cedar_spin_lock, flags);		
+
+	list_for_each_entry_safe(task_entry, task_entry_tmp, &run_task_list, list) {
+		mod_timer(&cedar_devp->cedar_engine_timer_rel, jiffies + msecs_to_jiffies(CLK_REL_TIME));
+		if (task_entry->status == TASK_RELEASE || 
+				time_after(jiffies, task_entry->t.timeout)) {
+			if (task_entry->status == TASK_INIT)
+				task_entry->status = TASK_TIMEOUT;
+			list_move(&task_entry->list, &del_task_list);	
+		}
+	}
+
+	list_for_each_entry_safe(task_entry, task_entry_tmp, &del_task_list, list) {		
+		info.si_signo = SIG_CEDAR;
+		info.si_code = task_entry->t.ID;
+		if (task_entry->status == TASK_TIMEOUT) {
+			info.si_errno = TASK_TIMEOUT;			
+			send_sig_info(SIG_CEDAR, &info, task_entry->task_handle);
+		} else if (task_entry->status == TASK_RELEASE) {
+			info.si_errno = TASK_RELEASE;			
+			send_sig_info(SIG_CEDAR, &info, task_entry->task_handle);
+		}
+		list_del(&task_entry->list);
+		kfree(task_entry);
+	}
+
+	if (!list_empty(&run_task_list)) {
+		task_entry = list_entry(run_task_list.next, struct cedarv_engine_task, list);
+		if (task_entry->running == 0) {
+			task_entry->running = 1;
+			info.si_signo = SIG_CEDAR;
+			info.si_code = task_entry->t.ID;
+			info.si_errno = TASK_INIT;
+			send_sig_info(SIG_CEDAR, &info, task_entry->task_handle);
+		}
+
+		mod_timer( &cedar_devp->cedar_engine_timer, jiffies + msecs_to_jiffies(TIMER_CIRCLE));
+	}
+
+	spin_unlock_irqrestore(&cedar_spin_lock, flags);
+}
+
+#ifdef CONFIG_COMPAT
+static long compat_cedardev_ioctl(struct file *filp, u32 cmd, unsigned long arg)
+{
+	int ret = 0;
+	int ve_timeout = 0;
+	/*struct cedar_dev *devp;*/
+#ifdef USE_CEDAR_ENGINE
+	int rel_taskid = 0;
+	struct __cedarv_task task_ret;
+	struct cedarv_engine_task *task_ptr = NULL;
+#endif
+	ulong flags;
+	struct ve_info *info;
+
+	info = filp->private_data;
+
+	switch (cmd)
+	{
+		case IOCTL_ENGINE_REQ:
+#ifdef USE_CEDAR_ENGINE
+			if (copy_from_user(&task_ret, (void __user *)arg,
+				sizeof(struct __cedarv_task))) {
+				cedar_ve_printk(KERN_WARNING, "USE_CEDAR_ENGINE "
+					"copy_from_user fail\n");
+				return -EFAULT;
+			}
+			spin_lock_irqsave(&cedar_spin_lock, flags);
+
+			if (!list_empty(&run_task_list) &&
+				(task_ret.block_mode == CEDAR_NONBLOCK_TASK)) {
+				spin_unlock_irqrestore(&cedar_spin_lock, flags);
+				return CEDAR_RUN_LIST_NONULL;
+			}
+			spin_unlock_irqrestore(&cedar_spin_lock, flags);
+
+			task_ptr = kmalloc(sizeof(struct cedarv_engine_task), GFP_KERNEL);
+			if (!task_ptr) {
+				cedar_ve_printk(KERN_WARNING, "get "
+					"task_ptr error\n");
+				return PTR_ERR(task_ptr);
+			}
+			task_ptr->task_handle = current;
+			task_ptr->t.ID = task_ret.ID;
+			/*ms to jiffies*/
+			task_ptr->t.timeout = jiffies +
+				msecs_to_jiffies(1000*task_ret.timeout);
+			task_ptr->t.frametime = task_ret.frametime;
+			task_ptr->t.task_prio = task_ret.task_prio;
+			task_ptr->running = 0;
+			task_ptr->is_first_task = 0;
+			task_ptr->status = TASK_INIT;
+
+			cedardev_insert_task(task_ptr);
+
+			ret = enable_cedar_hw_clk();
+			if (ret < 0) {
+				cedar_ve_printk(KERN_WARNING, "IOCTL_ENGINE_REQ "
+					"clk enable error!\n");
+				return -EFAULT;
+			}
+			return task_ptr->is_first_task;
+#else
+			cedar_devp->ref_count++;
+			if (1 == cedar_devp->ref_count)
+				enable_cedar_hw_clk();
+			break;
+#endif
+		case IOCTL_ENGINE_REL:
+#ifdef USE_CEDAR_ENGINE
+			rel_taskid = (int)arg;
+
+			ret = cedardev_del_task(rel_taskid);
+#else
+			cedar_devp->ref_count--;
+			if (0 == cedar_devp->ref_count) {
+				ret = disable_cedar_hw_clk();
+				if (ret < 0) {
+					cedar_ve_printk(KERN_WARNING, "IOCTL_ENGINE_REL "
+						"clk disable error!\n");
+					return -EFAULT;
+				}
+			}
+#endif
+			return ret;
+		case IOCTL_ENGINE_CHECK_DELAY:
+			{
+				struct cedarv_engine_task_info task_info;
+
+				if (copy_from_user(&task_info, (void __user *)arg,
+					sizeof(struct cedarv_engine_task_info))) {
+					cedar_ve_printk(KERN_WARNING, "%d "
+						"copy_from_user fail\n",
+						IOCTL_ENGINE_CHECK_DELAY);
+					return -EFAULT;
+				}
+				task_info.total_time = cedardev_check_delay(task_info.task_prio);
+#ifdef CEDAR_DEBUG
+				printk("%s,%d,%d\n", __func__, __LINE__, task_info.total_time);
+#endif
+				task_info.frametime = 0;
+				spin_lock_irqsave(&cedar_spin_lock, flags);
+				if (!list_empty(&run_task_list)) {
+
+					struct cedarv_engine_task *task_entry;
+#ifdef CEDAR_DEBUG
+					printk("%s,%d\n",__func__,__LINE__);
+#endif
+					task_entry = list_entry(run_task_list.next, struct cedarv_engine_task, list);
+					if (task_entry->running == 1)
+						task_info.frametime = task_entry->t.frametime;
+#ifdef CEDAR_DEBUG
+					printk("%s,%d,%d\n",__func__,__LINE__,task_info.frametime);
+#endif
+				}
+				spin_unlock_irqrestore(&cedar_spin_lock, flags);
+
+				if (copy_to_user((void *)arg, &task_info, sizeof(struct cedarv_engine_task_info))){
+					cedar_ve_printk(KERN_WARNING, "%d "
+						"copy_to_user fail\n",
+						IOCTL_ENGINE_CHECK_DELAY);
+					return -EFAULT;
+				}
+			}
+			break;
+		case IOCTL_WAIT_VE_DE:
+			ve_timeout = (int)arg;
+			cedar_devp->de_irq_value = 0;
+
+			spin_lock_irqsave(&cedar_spin_lock, flags);
+			if (cedar_devp->de_irq_flag)
+				cedar_devp->de_irq_value = 1;
+			spin_unlock_irqrestore(&cedar_spin_lock, flags);
+
+			wait_event_interruptible_timeout(wait_ve, cedar_devp->de_irq_flag, ve_timeout*HZ);
+			cedar_devp->de_irq_flag = 0;
+
+			return cedar_devp->de_irq_value;
+
+		case IOCTL_WAIT_VE_EN:
+
+			ve_timeout = (int)arg;
+			cedar_devp->en_irq_value = 0;
+
+			spin_lock_irqsave(&cedar_spin_lock, flags);
+			if (cedar_devp->en_irq_flag)
+				cedar_devp->en_irq_value = 1;
+			spin_unlock_irqrestore(&cedar_spin_lock, flags);
+
+			wait_event_interruptible_timeout(wait_ve, cedar_devp->en_irq_flag, ve_timeout*HZ);
+			cedar_devp->en_irq_flag = 0;
+
+			return cedar_devp->en_irq_value;
+
+#if ((defined CONFIG_ARCH_SUN8IW8P1) || (defined CONFIG_ARCH_SUN50I))
+
+		case IOCTL_WAIT_JPEG_DEC:
+			ve_timeout = (int)arg;
+			cedar_devp->jpeg_irq_value = 0;
+
+			spin_lock_irqsave(&cedar_spin_lock, flags);
+			if (cedar_devp->jpeg_irq_flag)
+				cedar_devp->jpeg_irq_value = 1;
+			spin_unlock_irqrestore(&cedar_spin_lock, flags);
+
+			wait_event_interruptible_timeout(wait_ve, cedar_devp->jpeg_irq_flag, ve_timeout*HZ);
+			cedar_devp->jpeg_irq_flag = 0;
+			return cedar_devp->jpeg_irq_value;
+#endif
+		case IOCTL_ENABLE_VE:
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)
+		        if (clk_prepare_enable(cedar_devp->mod_clk)) {
+			        cedar_ve_printk(KERN_WARNING, "IOCTL_ENABLE_VE enable ve_moduleclk failed! (%d)\n", __LINE__);
+			}
+#else /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+		        if (clk_prepare_enable(ve_moduleclk)) {
+				cedar_ve_printk(KERN_WARNING, "IOCTL_ENABLE_VE "
+					"enable ve_moduleclk failed!\n");
+			}
+#endif /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+			break;
+
+		case IOCTL_DISABLE_VE:
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)
+			if ((NULL == cedar_devp->mod_clk)||IS_ERR(cedar_devp->mod_clk)) {
+			        cedar_ve_printk(KERN_WARNING, "IOCTL_DISABLE_VE ve_moduleclk is invalid (%d)\n", __LINE__);
+				return -EFAULT;
+			} else {
+				clk_disable_unprepare(cedar_devp->mod_clk);
+			}
+#else /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+			if ((NULL == ve_moduleclk)||IS_ERR(ve_moduleclk)) {
+				cedar_ve_printk(KERN_WARNING, "IOCTL_DISABLE_VE "
+					"ve_moduleclk is invalid\n");
+				return -EFAULT;
+			} else {
+				clk_disable_unprepare(ve_moduleclk);
+			}
+#endif /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+			break;
+
+		case IOCTL_RESET_VE:
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)
+		        reset_control_assert(cedar_devp->rstc);
+		        reset_control_deassert(cedar_devp->rstc);
+#else /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+			sunxi_periph_reset_assert(ve_moduleclk);
+			sunxi_periph_reset_deassert(ve_moduleclk);
+#endif /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+			break;
+
+		case IOCTL_SET_VE_FREQ:
+			{
+				int arg_rate = (int)arg;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)
+				struct clk *const ve_moduleclk=cedar_devp->mod_clk;
+				struct clk *const ve_parent_pll_clk=cedar_devp->ahb_clk;
+				u32 ve_parent_clk_rate;
+#endif /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+				if (arg_rate >= VE_CLK_LOW_WATER &&
+						arg_rate <= VE_CLK_HIGH_WATER &&
+						clk_get_rate(ve_moduleclk)/1000000 != arg_rate) {
+					if (!clk_set_rate(ve_parent_pll_clk, arg_rate*1000000)) {
+						ve_parent_clk_rate = clk_get_rate(ve_parent_pll_clk);
+						if (clk_set_rate(ve_moduleclk, ve_parent_clk_rate)) {
+							cedar_ve_printk(KERN_WARNING, "set ve clock failed\n");
+						}
+
+					} else {
+						cedar_ve_printk(KERN_WARNING, "set pll4 clock failed\n");
+					}
+				}
+				ret = clk_get_rate(ve_moduleclk);
+				break;
+			}
+		case IOCTL_GETVALUE_AVS2:
+		case IOCTL_ADJUST_AVS2:
+		case IOCTL_ADJUST_AVS2_ABS:
+		case IOCTL_CONFIG_AVS2:
+		case IOCTL_RESET_AVS2:
+		case IOCTL_PAUSE_AVS2:
+		case IOCTL_START_AVS2:
+			cedar_ve_printk(KERN_WARNING, "do not supprot this ioctrl now\n");
+			break;
+
+		case IOCTL_GET_ENV_INFO:
+			{
+				struct cedarv_env_infomation_compat env_info;
+#if defined(USE_ION)
+				env_info.phymem_start = 0; // do not use this interface ,ve get phy mem form ion now
+				env_info.phymem_total_size = 0;//ve_size = 0x04000000 
+				env_info.address_macc = 0;
+#else				
+				env_info.phymem_start = (unsigned long)phys_to_virt(cedar_devp->ve_start); // do not use this interface ,ve get phy mem form ion now
+				env_info.phymem_total_size = (unsigned long) cedar_devp->ve_size;
+				env_info.address_macc = (unsigned long) cedar_devp->iomap_addrs.regs_macc;
+#endif	
+				if (copy_to_user((char *)arg, &env_info,
+					sizeof(struct cedarv_env_infomation_compat)))
+					return -EFAULT;
+			}
+			break;
+		case IOCTL_GET_IC_VER:
+			{
+				return 0;
+			}
+		case IOCTL_FLUSH_CACHE:
+#if !defined(MMAP_UNCACHABLE)		
+			{
+				struct cedarv_cache_range cache_range;
+				if(copy_from_user(&cache_range, (void __user*)arg, sizeof(struct cedarv_cache_range))){
+					printk("IOCTL_FLUSH_CACHE copy_from_user fail\n");
+					return -EFAULT;
+				}
+#ifndef __aarch64__				
+				flush_clean_user_range(cache_range.start, cache_range.end);
+#endif				
+			}
+#endif			
+			break;	
+		case IOCTL_SET_REFCOUNT:
+			cedar_devp->ref_count = (int)arg;
+			break;
+		case IOCTL_SET_VOL:
+			{
+
+#if defined CONFIG_ARCH_SUN9IW1P1
+				int ret;
+				int vol = (int)arg;
+
+				if (down_interruptible(&cedar_devp->sem)) {
+					return -ERESTARTSYS;
+				}
+				info->set_vol_flag = 1;
+
+				//set output voltage to arg mV
+				ret = regulator_set_voltage(regu,vol*1000,3300000);
+				if (IS_ERR(regu)) {
+					cedar_ve_printk(KERN_WARNING, \
+						"fail to set axp15_dcdc4 regulator voltage!\n");
+				}
+				up(&cedar_devp->sem);
+#endif
+				break;
+			}
+		default:
+			return -1;
+	}
+	return ret;
+}
+#endif /* CONFIG_COMPAT */
+
+/*
+ * ioctl function
+ * including : wait video engine done,
+ *             AVS Counter control,
+ *             Physical memory control,
+ *             module clock/freq control.
+ *				cedar engine
+ */ 
+static long cedardev_ioctl(struct file *filp, u32 cmd, unsigned long arg)
+{
+	int ret = 0;	
+	int ve_timeout = 0;
+	//struct cedar_dev *devp;
+#ifdef USE_CEDAR_ENGINE
+	int rel_taskid = 0;
+	struct __cedarv_task task_ret;
+	struct cedarv_engine_task *task_ptr = NULL;
+#endif
+	ulong flags;
+	struct ve_info *info;
+
+	info = filp->private_data;
+
+	switch (cmd)
+	{
+		case IOCTL_ENGINE_REQ:
+#ifdef USE_CEDAR_ENGINE
+			if (copy_from_user(&task_ret, (void __user *)arg, sizeof(struct __cedarv_task))) {
+				cedar_ve_printk(KERN_WARNING, \
+					"IOCTL_ENGINE_REQ copy_from_user fail\n");
+				return -EFAULT;
+			}
+			spin_lock_irqsave(&cedar_spin_lock, flags);
+
+			if (!list_empty(&run_task_list) && (task_ret.block_mode == CEDAR_NONBLOCK_TASK)) {
+				spin_unlock_irqrestore(&cedar_spin_lock, flags);
+				return CEDAR_RUN_LIST_NONULL;
+			}
+			spin_unlock_irqrestore(&cedar_spin_lock, flags);
+
+			task_ptr = kmalloc(sizeof(struct cedarv_engine_task), GFP_KERNEL);
+			if (!task_ptr) {
+				cedar_ve_printk(KERN_WARNING, "get mem for IOCTL_ENGINE_REQ\n");
+				return PTR_ERR(task_ptr);
+			}
+			task_ptr->task_handle = current;
+			task_ptr->t.ID = task_ret.ID;
+			task_ptr->t.timeout = jiffies + msecs_to_jiffies(1000*task_ret.timeout);//ms to jiffies
+			task_ptr->t.frametime = task_ret.frametime;
+			task_ptr->t.task_prio = task_ret.task_prio;
+			task_ptr->running = 0;
+			task_ptr->is_first_task = 0;
+			task_ptr->status = TASK_INIT;
+
+			cedardev_insert_task(task_ptr);
+
+			ret = enable_cedar_hw_clk();
+			if (ret < 0) {
+				cedar_ve_printk(KERN_WARNING, \
+					"cedar clk enable somewhere error!\n");
+				return -EFAULT;
+			}
+			return task_ptr->is_first_task;		
+#else
+			cedar_devp->ref_count++;
+			if (1 == cedar_devp->ref_count)
+				enable_cedar_hw_clk();
+			break;
+#endif	
+		case IOCTL_ENGINE_REL:
+#ifdef USE_CEDAR_ENGINE 
+			rel_taskid = (int)arg;		
+
+			ret = cedardev_del_task(rel_taskid);					
+#else
+			cedar_devp->ref_count--;
+			if (0 == cedar_devp->ref_count) {
+				ret = disable_cedar_hw_clk();
+				if (ret < 0) {
+					cedar_ve_printk(KERN_WARNING, "IOCTL_ENGINE_REL "
+						"clk disable error!\n");
+					return -EFAULT;
+				}
+			}
+#endif
+			return ret;
+		case IOCTL_ENGINE_CHECK_DELAY:		
+			{
+				struct cedarv_engine_task_info task_info;
+
+				if (copy_from_user(&task_info,
+					(void __user *)arg,
+					sizeof(struct cedarv_engine_task_info))) {
+					cedar_ve_printk(KERN_WARNING, \
+						"IOCTL_ENGINE_CHECK_DELAY copy_from_user fail\n");
+					return -EFAULT;
+				}
+				task_info.total_time = cedardev_check_delay(task_info.task_prio);
+#ifdef CEDAR_DEBUG
+				printk("%s,%d,%d\n", __func__, __LINE__, task_info.total_time);
+#endif
+				task_info.frametime = 0;
+				spin_lock_irqsave(&cedar_spin_lock, flags);
+				if (!list_empty(&run_task_list)) {
+
+					struct cedarv_engine_task *task_entry;
+#ifdef CEDAR_DEBUG
+					printk("%s,%d\n",__func__,__LINE__);
+#endif
+					task_entry = list_entry(run_task_list.next, struct cedarv_engine_task, list);
+					if (task_entry->running == 1)
+						task_info.frametime = task_entry->t.frametime;
+#ifdef CEDAR_DEBUG
+					printk("%s,%d,%d\n",__func__,__LINE__,task_info.frametime);
+#endif
+				}
+				spin_unlock_irqrestore(&cedar_spin_lock, flags);
+
+				if (copy_to_user((void *)arg, &task_info, sizeof(struct cedarv_engine_task_info))){
+					cedar_ve_printk(KERN_WARNING, \
+						"IOCTL_ENGINE_CHECK_DELAY copy_to_user fail\n");
+					return -EFAULT;
+				}
+			}
+			break;
+		case IOCTL_WAIT_VE_DE:            
+			ve_timeout = (int)arg;
+			cedar_devp->de_irq_value = 0;
+
+			spin_lock_irqsave(&cedar_spin_lock, flags);
+			if (cedar_devp->de_irq_flag)
+				cedar_devp->de_irq_value = 1;
+			spin_unlock_irqrestore(&cedar_spin_lock, flags);
+
+			//printk("[cedar-ve]: ioctl IOCTL_WAIT_VE_DE\n");
+
+			wait_event_interruptible_timeout(wait_ve, cedar_devp->de_irq_flag, ve_timeout*HZ);            
+			cedar_devp->de_irq_flag = 0;	
+
+			return cedar_devp->de_irq_value;
+
+		case IOCTL_WAIT_VE_EN:
+			ve_timeout = (int)arg;
+			cedar_devp->en_irq_value = 0;
+
+			spin_lock_irqsave(&cedar_spin_lock, flags);
+			if (cedar_devp->en_irq_flag)
+				cedar_devp->en_irq_value = 1;
+			spin_unlock_irqrestore(&cedar_spin_lock, flags);
+
+			//printk("[cedar-ve]: ioctl IOCTL_WAIT_VE_EN\n");
+
+			wait_event_interruptible_timeout(wait_ve, cedar_devp->en_irq_flag, ve_timeout*HZ);            
+			cedar_devp->en_irq_flag = 0;	
+
+			return cedar_devp->en_irq_value;
+
+#if ((defined CONFIG_ARCH_SUN8IW8P1) || (defined CONFIG_ARCH_SUN50I))
+
+		case IOCTL_WAIT_JPEG_DEC:            
+			ve_timeout = (int)arg;
+			cedar_devp->jpeg_irq_value = 0;
+
+			spin_lock_irqsave(&cedar_spin_lock, flags);
+			if (cedar_devp->jpeg_irq_flag)
+				cedar_devp->jpeg_irq_value = 1;
+			spin_unlock_irqrestore(&cedar_spin_lock, flags);
+
+			wait_event_interruptible_timeout(wait_ve, cedar_devp->jpeg_irq_flag, ve_timeout*HZ);            
+			cedar_devp->jpeg_irq_flag = 0;	
+			return cedar_devp->jpeg_irq_value;	
+#endif
+		case IOCTL_ENABLE_VE:
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)
+			if (clk_prepare_enable(cedar_devp->mod_clk)) {
+				cedar_ve_printk(KERN_WARNING, \
+					"try to enable ve_moduleclk failed!\n");
+			}
+#else /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+			if (clk_prepare_enable(ve_moduleclk)) {
+				cedar_ve_printk(KERN_WARNING, \
+					"try to enable ve_moduleclk failed!\n");
+			}
+#endif /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+			break;
+
+		case IOCTL_DISABLE_VE:
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)
+		  if ((NULL == cedar_devp->mod_clk)||IS_ERR(cedar_devp->mod_clk)) {
+				cedar_ve_printk(KERN_WARNING, \
+					"ve_moduleclk is invalid,just return!\n");
+				return -EFAULT;
+			} else {
+				clk_disable_unprepare(cedar_devp->mod_clk);
+			}
+#else /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+		  if ((NULL == ve_moduleclk)||IS_ERR(ve_moduleclk)) {
+				cedar_ve_printk(KERN_WARNING, \
+					"ve_moduleclk is invalid,just return!\n");
+				return -EFAULT;
+			} else {
+				clk_disable_unprepare(ve_moduleclk);
+			}
+#endif /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+		  break;
+
+		case IOCTL_RESET_VE:
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)
+		        reset_control_assert(cedar_devp->rstc);
+		        reset_control_deassert(cedar_devp->rstc);
+#else /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+			sunxi_periph_reset_assert(ve_moduleclk);
+			sunxi_periph_reset_deassert(ve_moduleclk);
+#endif /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+			break;
+
+		case IOCTL_SET_VE_FREQ:	
+			{
+				int arg_rate = (int)arg;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)
+				struct clk *const ve_moduleclk=cedar_devp->mod_clk;
+				struct clk *const ve_parent_pll_clk=cedar_devp->ahb_clk;
+				u32 ve_parent_clk_rate;
+#endif /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+
+				if (arg_rate >= VE_CLK_LOW_WATER &&
+						arg_rate <= VE_CLK_HIGH_WATER &&
+						clk_get_rate(ve_moduleclk)/1000000 != arg_rate) {
+					if (!clk_set_rate(ve_parent_pll_clk, arg_rate*1000000)) {
+						ve_parent_clk_rate = clk_get_rate(ve_parent_pll_clk);
+						if (clk_set_rate(ve_moduleclk, ve_parent_clk_rate)) {
+							cedar_ve_printk(KERN_WARNING, "set ve clock failed\n");
+						}
+
+					} else {
+						cedar_ve_printk(KERN_WARNING, "set pll4 clock failed\n");
+					}
+				}
+				ret = clk_get_rate(ve_moduleclk);
+				break;
+			}
+		case IOCTL_GETVALUE_AVS2:
+		case IOCTL_ADJUST_AVS2:	    
+		case IOCTL_ADJUST_AVS2_ABS:
+		case IOCTL_CONFIG_AVS2:
+		case IOCTL_RESET_AVS2:
+		case IOCTL_PAUSE_AVS2:
+		case IOCTL_START_AVS2:
+			cedar_ve_printk(KERN_WARNING, "do not supprot this ioctrl now\n");
+			break;
+
+		case IOCTL_GET_ENV_INFO:
+			{
+				struct cedarv_env_infomation env_info;
+#if defined(USE_ION)
+				env_info.phymem_start = 0; // do not use this interface ,ve get phy mem form ion now
+				env_info.phymem_total_size = 0;//ve_size = 0x04000000 
+				env_info.address_macc = 0;
+#else				
+				env_info.phymem_start = (unsigned long)phys_to_virt(cedar_devp->ve_start); // do not use this interface ,ve get phy mem form ion now
+				env_info.phymem_total_size = (unsigned long) cedar_devp->ve_size;
+				env_info.address_macc = (unsigned long) cedar_devp->iomap_addrs.regs_macc;
+#endif				
+				if (copy_to_user((char *)arg, &env_info, sizeof(struct cedarv_env_infomation)))
+					return -EFAULT;
+			}
+			break;
+		case IOCTL_GET_IC_VER:
+			{        	
+				return 0;
+			}
+		case IOCTL_FLUSH_CACHE:
+#if !defined(MMAP_UNCACHABLE)		
+			{
+				struct cedarv_cache_range cache_range;
+				if(copy_from_user(&cache_range, (void __user*)arg, sizeof(struct cedarv_cache_range))){
+					printk("IOCTL_FLUSH_CACHE copy_from_user fail\n");
+					return -EFAULT;
+				}
+#ifndef __aarch64__				
+				flush_clean_user_range(cache_range.start, cache_range.end);
+#endif				
+			}
+#endif			
+			break;		
+		case IOCTL_SET_REFCOUNT:
+			cedar_devp->ref_count = (int)arg;
+			break;
+		case IOCTL_SET_VOL:
+			{
+
+#if defined CONFIG_ARCH_SUN9IW1P1
+				int ret;
+				int vol = (int)arg;
+
+				if (down_interruptible(&cedar_devp->sem)) {
+					return -ERESTARTSYS;
+				}
+				info->set_vol_flag = 1;
+
+				//set output voltage to arg mV
+				ret = regulator_set_voltage(regu,vol*1000,3300000);
+				if (IS_ERR(regu))
+					cedar_ve_printk(KERN_WARNING, "fail to set axp15_dcdc4 regulator voltage!\n");
+				up(&cedar_devp->sem);
+#endif
+				break;
+			}
+		default:
+			return -1;
+	}
+	return ret;
+}
+
+static int cedardev_open(struct inode *inode, struct file *filp)
+{
+	//struct cedar_dev *devp;
+	struct ve_info *info;
+
+	info = kmalloc(sizeof(struct ve_info), GFP_KERNEL);
+	if (!info)
+		return -ENOMEM;
+
+	info->set_vol_flag = 0;
+	ve_file = filp;
+
+	//devp = container_of(inode->i_cdev, struct cedar_dev, cdev);
+	filp->private_data = info;
+	if (down_interruptible(&cedar_devp->sem)) {
+		return -ERESTARTSYS;
+	}
+
+	/* init other resource here */
+	if (0 == cedar_devp->ref_count) {
+		cedar_devp->de_irq_flag = 0;
+		cedar_devp->en_irq_flag = 0;
+		cedar_devp->jpeg_irq_flag = 0;
+	}
+
+	up(&cedar_devp->sem);
+	nonseekable_open(inode, filp);	
+	return 0;
+}
+
+static int cedardev_release(struct inode *inode, struct file *filp)
+{   
+	//struct cedar_dev *devp;
+	struct ve_info *info;
+	//int ret = 0;
+
+	info = filp->private_data;
+
+	if (down_interruptible(&cedar_devp->sem)) {
+		return -ERESTARTSYS;
+	}
+
+#if defined CONFIG_ARCH_SUN9IW1P1
+
+	if (info->set_vol_flag == 1) {
+		regulator_set_voltage(regu,900000,3300000);
+		if (IS_ERR(regu)) {
+			cedar_ve_printk(KERN_WARNING, \
+				"some error happen, fail to set axp15_dcdc4 regulator voltage!\n");
+			return -EINVAL;
+		}
+	}
+#endif
+
+	/* release other resource here */
+	if (0 == cedar_devp->ref_count) {
+		cedar_devp->de_irq_flag = 1;
+		cedar_devp->en_irq_flag = 1;
+		cedar_devp->jpeg_irq_flag = 1;
+	}
+	up(&cedar_devp->sem);
+
+	kfree(info);
+	ve_file = NULL;
+	return 0;
+}
+
+static void cedardev_vma_open(struct vm_area_struct *vma)
+{	
+} 
+
+static void cedardev_vma_close(struct vm_area_struct *vma)
+{	
+}
+
+static struct vm_operations_struct cedardev_remap_vm_ops = {
+	.open  = cedardev_vma_open,
+	.close = cedardev_vma_close,
+};
+
+#if defined(USE_ION)
+static int cedardev_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+	u32 temp_pfn;
+
+	if (vma->vm_end - vma->vm_start == 0)
+	{
+		cedar_ve_printk(KERN_WARNING, "vma->vm_end is equal vma->vm_start : %lx\n",\
+			vma->vm_start);
+		return 0;
+	}
+	if (vma->vm_pgoff > (~0UL >> PAGE_SHIFT))
+	{
+		cedar_ve_printk(KERN_WARNING, \
+			"the vma->vm_pgoff is %lx,it is large than the largest page number\n", vma->vm_pgoff);
+		return -EINVAL;
+	}
+
+
+	temp_pfn = MACC_REGS_BASE >> 12;
+
+
+	/* Set reserved and I/O flag for the area. */
+	vma->vm_flags |= /*VM_RESERVED | */VM_IO;
+	/* Select uncached access. */
+	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+
+	if (io_remap_pfn_range(vma, vma->vm_start, temp_pfn,
+				vma->vm_end - vma->vm_start, vma->vm_page_prot)) {
+		return -EAGAIN;
+	}
+
+
+	vma->vm_ops = &cedardev_remap_vm_ops;
+	cedardev_vma_open(vma);
+
+	return 0; 
+}
+#else
+
+static int cedardev_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+    unsigned long temp_pfn;
+    unsigned long  VAddr;
+	  struct iomap_para addrs;
+
+	  unsigned int io_ram = 0;
+    VAddr = vma->vm_pgoff << PAGE_SHIFT;
+	  addrs = cedar_devp->iomap_addrs;
+	  size_t size = vma->vm_end - vma->vm_start;
+
+
+    if (VAddr == (unsigned long)addrs.regs_macc) {
+        temp_pfn = MACC_REGS_BASE >> PAGE_SHIFT;
+        io_ram = 1;
+    } else {
+        temp_pfn = (__pa(vma->vm_pgoff << PAGE_SHIFT)) >> PAGE_SHIFT;
+        io_ram = 0;
+    }
+
+    if (io_ram == 0) {   
+        /* Set reserved and I/O flag for the area. */
+        vma->vm_flags |= /* VM_RESERVED | */VM_IO;
+
+#if defined(MMAP_UNCACHABLE)
+        /* Select uncached access. */
+    	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+# ifndef __aarch64__    	
+        if (remap_pfn_range(vma, vma->vm_start, temp_pfn,
+                            size, vma->vm_page_prot)) {
+            return -EAGAIN;
+        }
+# else        
+        dma_mmap_coherent(cedar_devp->dev, vma, cedar_devp->ve_start_virt, cedar_devp->ve_start_pa, size);
+# endif        
+#endif
+        
+    } else {
+        /* Set reserved and I/O flag for the area. */
+        vma->vm_flags |= /*VM_RESERVED |*/ VM_IO;
+        /* Select uncached access. */
+        vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+
+        if (io_remap_pfn_range(vma, vma->vm_start, temp_pfn,
+                               size, vma->vm_page_prot)) {
+            return -EAGAIN;
+        }
+    }
+    
+    vma->vm_ops = &cedardev_remap_vm_ops;
+    cedardev_vma_open(vma);
+    
+    return 0; 
+}
+#endif
+
+#ifdef CONFIG_PM
+static int snd_sw_cedar_suspend(struct platform_device *pdev,pm_message_t state)
+{	
+	int ret = 0;
+
+	printk("[cedar] standby suspend\n");
+	ret = disable_cedar_hw_clk();
+
+#if defined CONFIG_ARCH_SUN9IW1P1
+	clk_disable_unprepare(ve_power_gating);
+#endif
+
+	if (ret < 0) {
+		cedar_ve_printk(KERN_WARNING, "cedar clk disable somewhere error!\n");
+		return -EFAULT;
+	}
+
+	return 0;
+}
+
+static int snd_sw_cedar_resume(struct platform_device *pdev)
+{
+	int ret = 0;
+
+	printk("[cedar] standby resume\n");
+
+#if defined CONFIG_ARCH_SUN9IW1P1
+	clk_prepare_enable(ve_power_gating);
+#endif
+
+	if (cedar_devp->ref_count == 0) {
+		return 0;
+	}
+
+	ret = enable_cedar_hw_clk();
+	if (ret < 0) {
+		cedar_ve_printk(KERN_WARNING, "cedar clk enable somewhere error!\n");
+		return -EFAULT;
+	}
+	return 0;
+}
+#endif
+
+static const struct file_operations cedardev_fops = {
+	.owner   = THIS_MODULE,
+	.mmap    = cedardev_mmap,
+	.open    = cedardev_open,
+	.release = cedardev_release,
+	.llseek  = no_llseek,
+	.unlocked_ioctl   = cedardev_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl = compat_cedardev_ioctl,
+#endif
+};
+
+static int cedardev_init(struct platform_device *pdev)
+{
+	int ret = 0;
+	int devno;
+#ifdef CONFIG_OF
+	struct device_node *node;
+#endif /*CONFIG_OF*/
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)
+	struct device_node *np;
+#endif /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+	dev_t dev;
+
+	dev = 0;
+
+	printk("[cedar]: install start!!!\n");
+
+
+#if defined(CONFIG_OF)
+	node = pdev->dev.of_node;
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)
+	np = of_find_matching_node(NULL, sunxi_cedar_ve_match);
+	if (!np) {
+	  printk(KERN_ERR "Couldn't find the VE node\n");
+	  return -ENODEV;
+	}
+#endif /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+
+	/*register or alloc the device number.*/
+	if (g_dev_major) {
+		dev = MKDEV(g_dev_major, g_dev_minor);	
+		ret = register_chrdev_region(dev, 1, "cedar_dev");
+	} else {
+		ret = alloc_chrdev_region(&dev, g_dev_minor, 1, "cedar_dev");
+		g_dev_major = MAJOR(dev);
+		g_dev_minor = MINOR(dev);
+	}
+
+	if (ret < 0) {
+		cedar_ve_printk(KERN_WARNING, "cedar_dev: can't get major %d\n", \
+			g_dev_major);
+		return ret;
+	}
+	spin_lock_init(&cedar_spin_lock);
+	cedar_devp = kmalloc(sizeof(struct cedar_dev), GFP_KERNEL);
+	if (cedar_devp == NULL) {
+		cedar_ve_printk(KERN_WARNING, "malloc mem for cedar device err\n");
+		return -ENOMEM;
+	}		
+	memset(cedar_devp, 0, sizeof(struct cedar_dev));
+
+	cedar_devp->dev = &pdev->dev;
+	cedar_devp->pdev = pdev;
+
+#if defined(CONFIG_OF)
+	cedar_devp->irq = irq_of_parse_and_map(node, 0);
+	cedar_ve_printk(KERN_INFO, "cedar-ve the get irq is %d\n", \
+		cedar_devp->irq);
+	if (cedar_devp->irq <= 0)
+		cedar_ve_printk(KERN_WARNING, "Can't parse IRQ");
+#else
+	cedar_devp->irq = SUNXI_IRQ_VE;
+#endif
+
+#if defined(CONFIG_OF) && !defined(USE_ION)
+	// assign reserved memory for the VE
+	ret = of_reserved_mem_device_init(cedar_devp->dev);
+    if (ret) {
+        dev_err(cedar_devp->dev, "could not assign reserved memory\n");
+        return -ENODEV;
+    }
+#endif
+
+	sema_init(&cedar_devp->sem, 1);
+	init_waitqueue_head(&cedar_devp->wq);	
+
+	memset(&cedar_devp->iomap_addrs, 0, sizeof(struct iomap_para));
+
+	ret = request_irq(cedar_devp->irq, VideoEngineInterupt, 0, "cedar_dev", NULL);
+	if (ret < 0) {
+		cedar_ve_printk(KERN_WARNING, "request irq err\n");
+		return -EINVAL;
+	}
+
+	/* map for macc io space */
+#if defined(CONFIG_OF)
+	cedar_devp->iomap_addrs.regs_macc = of_iomap(node, 0);
+	if (!cedar_devp->iomap_addrs.regs_macc)
+		cedar_ve_printk(KERN_WARNING, "ve Can't map registers");
+
+	cedar_devp->sram_bass_vir = (u32*)of_iomap(node, 1);
+	if (!cedar_devp->sram_bass_vir)
+		cedar_ve_printk(KERN_WARNING, "ve Can't map sram_bass_vir registers");
+
+	cedar_devp->clk_bass_vir = (u32*)of_iomap(node, 2);
+	if (!cedar_devp->clk_bass_vir)
+		cedar_ve_printk(KERN_WARNING, "ve Can't map clk_bass_vir registers");
+#endif
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,11,0) && ((defined CONFIG_ARCH_SUN8IW7P1) || (defined CONFIG_ARCH_SUN8IW8P1) || (defined CONFIG_ARCH_SUN8IW9P1))
+#ifdef CONFIG_OF
+	if (of_device_is_compatible(np, "allwinner,sun8i-h3-video-engine")) {
+#endif /* CONFIG_OF*/
+	/*VE_SRAM mapping to AC320*/
+	  {
+	u32 val;
+	//printk(KERN_DEBUG "patching H3... %08x\n"\n, sunxi_smc_readl((void __iomem *)0xf1c00000));
+	val = sunxi_smc_readl((void __iomem *)0xf1c00000);
+	val &= 0x80000000;
+	sunxi_smc_writel(val, (void __iomem *)0xf1c00000); 
+	//printk(KERN_DEBUG "patching H3... %08x\n"\n, sunxi_smc_readl((void __iomem *)0xf1c00000));
+
+	/*remapping SRAM to MACC for codec test*/
+	val = sunxi_smc_readl((void __iomem *)0xf1c00000);
+	val |= 0x7fffffff;
+	sunxi_smc_writel(val, (void __iomem *)0xf1c00000);
+
+	//clear bootmode bit for give sram to ve
+	val = sunxi_smc_readl((void __iomem *)0xf1c00004);
+	val &= 0xfeffffff;
+	sunxi_smc_writel(val, (void __iomem *)0xf1c00004);
+	  }
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,11,0)
+	  ve_parent_pll_clk = clk_get(NULL, "pll_ve");
+	if ((!ve_parent_pll_clk)||IS_ERR(ve_parent_pll_clk)) {
+		printk("try to get ve_parent_pll_clk fail\n");
+		return -EINVAL;
+	}
+#endif /*LINUX_VERSION_CODE < KERNEL_VERSION(4,11,0)*/
+#ifdef CONFIG_OF
+	} else {
+#endif /* CONFIG_OF*/
+#if (defined CONFIG_ARCH_SUNIVW1P1) /*for 1663*/
+	  {
+	    u32 val;
+	val = readl(cedar_devp->clk_bass_vir+6);
+	val &= 0x7fff80f0;
+	val = val | (1<<31) | (8<<8);
+	writel(val, cedar_devp->clk_bass_vir+6);
+
+	/*set VE clock dividor*/
+	val = readl(cedar_devp->clk_bass_vir+79);
+	val |= (1<<31);
+	writel(val, cedar_devp->clk_bass_vir+79);
+
+	/*Active AHB bus to MACC*/
+	val = readl(cedar_devp->clk_bass_vir+25);
+	val |= (1<<0);
+	writel(val, cedar_devp->clk_bass_vir+25);
+
+	/*Power on and release reset ve*/
+	val = readl(cedar_devp->clk_bass_vir+177);
+	val &= ~(1<<0); /*reset ve*/
+	writel(val, cedar_devp->clk_bass_vir+177);
+
+	val = readl(cedar_devp->clk_bass_vir+177);
+	val |= (1<<0);
+	writel(val, cedar_devp->clk_bass_vir+177);
+
+	/*gate on the bus to SDRAM*/
+	val = readl(cedar_devp->clk_bass_vir+64);
+	val |= (1<<0);
+	writel(val, cedar_devp->clk_bass_vir+64);
+
+	/*VE_SRAM mapping to AC320*/
+	val = readl(cedar_devp->sram_bass_vir);
+	val &= 0x80000000;
+	writel(val, cedar_devp->sram_bass_vir);
+
+	/*remapping SRAM to MACC for codec test*/
+	val = readl(cedar_devp->sram_bass_vir);
+	val |= 0x7fffffff;
+	writel(val, cedar_devp->sram_bass_vir);
+
+	/*clear bootmode bit for give sram to ve*/
+	val = readl((cedar_devp->sram_bass_vir + 1));
+	val &= 0xefffffff;
+	writel(val, (cedar_devp->sram_bass_vir + 1));
+	  }
+#else
+	{
+	  u32 val;
+	/*VE_SRAM mapping to AC320*/
+	val = readl(cedar_devp->sram_bass_vir);
+	val &= 0x80000000;
+	writel(val, cedar_devp->sram_bass_vir);
+
+	/*remapping SRAM to MACC for codec test*/
+	val = readl(cedar_devp->sram_bass_vir);
+	val |= 0x7fffffff;
+	writel(val, cedar_devp->sram_bass_vir);
+
+	/*clear bootmode bit for give sram to ve*/
+	val = readl((cedar_devp->sram_bass_vir + 1));
+	val &= 0xfeffffff;
+	writel(val, (cedar_devp->sram_bass_vir + 1));
+	}
+#endif
+#ifdef CONFIG_OF
+	}
+#endif /* CONFIG_OF*/
+#endif /* ((defined CONFIG_ARCH_SUN8IW7P1) || (defined CONFIG_ARCH_SUN8IW8P1) || (defined CONFIG_ARCH_SUN8IW9P1)) */
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)
+	cedar_devp->syscon = syscon_regmap_lookup_by_phandle(cedar_devp->dev->of_node, "syscon");
+	if (IS_ERR(cedar_devp->syscon)) {
+	  dev_err(cedar_devp->dev, "syscon failed...\n");
+	  cedar_devp->syscon = NULL;
+	} else {
+		// remap SRAM C1 to the VE
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5,1,0)	
+		int ret = sunxi_sram_claim(cedar_devp->dev);
+		if (ret) {
+			dev_err(cedar_devp->dev, "Failed to claim SRAM\n");
+	  		return -EFAULT;
+		}
+		printk("[cedar]: Success claim SRAM\n");
+#else 
+
+	  	regmap_write_bits(cedar_devp->syscon, SYSCON_SRAM_CTRL_REG0,
+					      SYSCON_SRAM_C1_MAP_VE,
+					      SYSCON_SRAM_C1_MAP_VE);
+#endif
+	}
+
+	cedar_devp->ahb_clk = devm_clk_get(cedar_devp->dev, "ahb");
+       if (IS_ERR(cedar_devp->ahb_clk)) {
+               dev_err(cedar_devp->dev, "failed to get ahb clock\n");
+               return PTR_ERR(cedar_devp->ahb_clk);
+       }
+       cedar_devp->mod_clk = devm_clk_get(cedar_devp->dev, "mod");
+       if (IS_ERR(cedar_devp->mod_clk)) {
+               dev_err(cedar_devp->dev, "failed to get mod clock\n");
+               return PTR_ERR(cedar_devp->mod_clk);
+       }
+       cedar_devp->ram_clk = devm_clk_get(cedar_devp->dev, "ram");
+       if (IS_ERR(cedar_devp->ram_clk)) {
+               dev_err(cedar_devp->dev, "failed to get ram clock\n");
+               return PTR_ERR(cedar_devp->ram_clk);
+       }
+#else /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+# if defined(CONFIG_OF)
+	ve_parent_pll_clk = of_clk_get(node, 0);
+	if ((!ve_parent_pll_clk) || IS_ERR(ve_parent_pll_clk)) {
+		cedar_ve_printk(KERN_WARNING, "try to get ve_parent_pll_clk fail\n");
+		return -EINVAL;
+	}
+
+	ve_moduleclk = of_clk_get(node, 1);
+	if (!ve_moduleclk || IS_ERR(ve_moduleclk)) {
+		cedar_ve_printk(KERN_WARNING, "get ve_moduleclk failed; \n");
+	}
+# endif
+#endif /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+
+	// no reset ve module
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)
+	ret = clk_prepare_enable(cedar_devp->ahb_clk);
+	if (ret) {
+	  dev_err(cedar_devp->dev, "could not enable ahb clock\n");
+	  return -EFAULT;
+	}
+	ret = clk_prepare_enable(cedar_devp->mod_clk);
+	if (ret) {
+	  clk_disable_unprepare(cedar_devp->ahb_clk);
+	  dev_err(cedar_devp->dev, "could not enable mod clock\n");
+	  return -EFAULT;
+	}
+	ret = clk_prepare_enable(cedar_devp->ram_clk);
+	if (ret) {
+	  clk_disable_unprepare(cedar_devp->mod_clk);
+	  clk_disable_unprepare(cedar_devp->ahb_clk);
+	  dev_err(cedar_devp->dev, "could not enable ram clock\n");
+	  return -EFAULT;
+	}
+	cedar_devp->rstc = devm_reset_control_get(cedar_devp->dev, NULL);
+	if (IS_ERR(cedar_devp->rstc)) {
+		dev_err(cedar_devp->dev, "Failed to get reset control\n");
+		ret = PTR_ERR(cedar_devp->rstc);
+		return -EFAULT;
+	}
+
+	reset_control_assert(cedar_devp->rstc);
+#else /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+	sunxi_periph_reset_assert(ve_moduleclk);
+	clk_prepare(ve_moduleclk);
+#endif /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+
+	/* Create char device */
+	devno = MKDEV(g_dev_major, g_dev_minor);	
+	cdev_init(&cedar_devp->cdev, &cedardev_fops);
+	cedar_devp->cdev.owner = THIS_MODULE;
+	//cedar_devp->cdev.ops = &cedardev_fops;
+	ret = cdev_add(&cedar_devp->cdev, devno, 1);
+	if (ret) {
+		cedar_ve_printk(KERN_WARNING, "Err:%d add cedardev", ret);
+	}
+	cedar_devp->class = class_create(THIS_MODULE, "cedar_dev");
+	cedar_devp->odev  = device_create(cedar_devp->class, NULL, devno, NULL, "cedar_dev");
+#ifdef HAVE_TIMER_SETUP
+	timer_setup(&cedar_devp->cedar_engine_timer, &cedar_engine_for_events, 0);
+	timer_setup(&cedar_devp->cedar_engine_timer_rel, &cedar_engine_for_timer_rel, 0);
+#else
+	setup_timer(&cedar_devp->cedar_engine_timer, cedar_engine_for_events, (ulong)cedar_devp);
+	setup_timer(&cedar_devp->cedar_engine_timer_rel, cedar_engine_for_timer_rel, (ulong)cedar_devp);
+#endif
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)
+	of_node_put(np);
+#endif /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+
+#if !defined(USE_ION)
+	cedar_devp->ve_size = 80 * SZ_1M;
+	ret = dma_set_coherent_mask(cedar_devp->dev, DMA_BIT_MASK(32));
+	if (ret) {
+		dev_err(cedar_devp->dev, "DMA enable failed\n");
+		return ret;
+	}
+
+	cedar_devp->ve_start_virt = dma_alloc_coherent(cedar_devp->dev, cedar_devp->ve_size,
+												   &cedar_devp->ve_start_pa,
+												   GFP_KERNEL | GFP_DMA);
+
+	if (!cedar_devp->ve_start_virt) {
+		dev_err(cedar_devp->dev, "cedar: failed to allocate memory buffer\n");
+		return -ENODEV;
+	}
+	cedar_devp->ve_start = cedar_devp->ve_start_pa;
+#ifndef __aarch64__
+	printk("[cedar]: memory allocated at address PA: %08lX, VA: %08lX\n", 
+		cedar_devp->ve_start, (ulong)cedar_devp->ve_start_virt);
+#else 
+	printk("[cedar]: memory allocated at PA: %016lX, VA: %016lX, CONV: %016lX\n", 
+																					cedar_devp->ve_start, 
+																					(ulong)cedar_devp->ve_start_virt, 
+																					(ulong) phys_to_virt(cedar_devp->ve_start));
+
+	printk("[cedar]: MACC regs allocated at %016lX\n", (ulong)cedar_devp->iomap_addrs.regs_macc);
+	printk("PAGE_OFFSET = %16lx\n", PAGE_OFFSET);
+	printk("PAGE_SHIFT = %d\n", PAGE_SHIFT);
+	printk("PAGE_MASK = %16lx\n", PAGE_MASK);
+	printk("PHYS_OFFSET = %16lx\n", PHYS_OFFSET);
+#endif
+
+#endif
+
+	printk("[cedar]: install end!!!\n");
+	return 0;
+}
+
+
+static void cedardev_exit(void)
+{
+	dev_t dev;
+	dev = MKDEV(g_dev_major, g_dev_minor);
+
+	free_irq(cedar_devp->irq, NULL);
+	iounmap(cedar_devp->iomap_addrs.regs_macc);
+	//	iounmap(cedar_devp->iomap_addrs.regs_avs);
+
+	if (cedar_devp->sram_bass_vir) iounmap(cedar_devp->sram_bass_vir);
+    if (cedar_devp->clk_bass_vir)  iounmap(cedar_devp->clk_bass_vir);
+
+#if !defined(USE_ION)
+    if (cedar_devp->ve_start_virt)
+    	dma_free_coherent(cedar_devp->dev, cedar_devp->ve_size, cedar_devp->ve_start_virt, cedar_devp->ve_start_pa);      	 
+
+# if defined(CONFIG_OF)
+	of_reserved_mem_device_release(cedar_devp->dev);
+# endif	
+#endif	
+
+	/* Destroy char device */
+	if (cedar_devp) {
+		cdev_del(&cedar_devp->cdev);
+		device_destroy(cedar_devp->class, dev);
+		class_destroy(cedar_devp->class);
+	}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)
+	if (NULL == cedar_devp->mod_clk || IS_ERR(cedar_devp->mod_clk)) { cedar_ve_printk(KERN_WARNING, "ve_moduleclk handle is invalid,just return! (%d)\n", __LINE__); }
+	if (NULL == cedar_devp->ram_clk || IS_ERR(cedar_devp->ram_clk)) { cedar_ve_printk(KERN_WARNING, "ve_moduleclk handle is invalid,just return! (%d)\n", __LINE__); }
+	clk_disable_unprepare(cedar_devp->mod_clk);
+	clk_disable_unprepare(cedar_devp->ram_clk);
+	cedar_devp->mod_clk = NULL;
+	cedar_devp->ram_clk = NULL;
+#else /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+	if (NULL == ve_moduleclk || IS_ERR(ve_moduleclk)) {
+		cedar_ve_printk(KERN_WARNING, "ve_moduleclk handle "
+			"is invalid,just return!\n");
+	} else {
+		clk_disable_unprepare(ve_moduleclk);
+		clk_put(ve_moduleclk);
+		ve_moduleclk = NULL;
+	}
+#endif /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)
+	if (NULL == cedar_devp->ahb_clk || IS_ERR(cedar_devp->ahb_clk)) {
+	  cedar_ve_printk(KERN_WARNING, "ve_parent_pll_clk handle is invalid,just return!\n");
+	} else {	
+	  clk_disable_unprepare(cedar_devp->ahb_clk);
+	  cedar_devp->ahb_clk = NULL;
+	}
+#else /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+	if (NULL == ve_parent_pll_clk || IS_ERR(ve_parent_pll_clk)) {
+		cedar_ve_printk(KERN_WARNING, "ve_parent_pll_clk "
+			"handle is invalid,just return!\n");
+	} else {	
+		clk_put(ve_parent_pll_clk);
+	}
+#endif /*LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)*/
+
+#if defined CONFIG_ARCH_SUN9IW1P1
+	//put regulator when module exit
+	regulator_put(regu);
+
+	if (NULL == ve_power_gating || IS_ERR(ve_power_gating)) {
+		cedar_ve_printk(KERN_WARNING, "ve_power_gating "
+			"handle is invalid,just return!\n");
+	} else {
+		clk_disable_unprepare(ve_power_gating);
+		clk_put(ve_power_gating);
+		ve_power_gating = NULL;
+	}
+#endif
+
+	unregister_chrdev_region(dev, 1);	
+	//  platform_driver_unregister(&sw_cedar_driver);
+	if (cedar_devp) {
+		kfree(cedar_devp);
+	}
+}
+
+static int  sunxi_cedar_remove(struct platform_device *pdev)
+{
+	cedardev_exit();
+	return 0;
+}
+
+static int  sunxi_cedar_probe(struct platform_device *pdev)
+{
+	cedardev_init(pdev);
+	return 0;
+}
+
+/*share the irq no. with timer2*/
+/*
+static struct resource sunxi_cedar_resource[] = {
+	[0] = {
+		.start = SUNXI_IRQ_VE,
+		.end   = SUNXI_IRQ_VE,
+		.flags = IORESOURCE_IRQ,
+	},
+};
+
+struct platform_device sunxi_device_cedar = {
+	.name		= "sunxi-cedar",
+	.id		= -1,
+	.num_resources	= ARRAY_SIZE(sunxi_cedar_resource),
+	.resource	= sunxi_cedar_resource,
+};
+*/
+
+static struct platform_driver sunxi_cedar_driver = {
+	.probe		= sunxi_cedar_probe,
+	.remove		= sunxi_cedar_remove,
+#ifdef CONFIG_PM
+	.suspend	= snd_sw_cedar_suspend,
+	.resume		= snd_sw_cedar_resume,
+#endif
+	.driver		= {
+		.name	= "sunxi-cedar",
+		.owner	= THIS_MODULE,
+
+#if defined(CONFIG_OF)
+		.of_match_table = sunxi_cedar_ve_match,
+#endif
+	},
+};
+
+static int __init sunxi_cedar_init(void)
+{
+	//need not to gegister device here,because the device is registered by device tree 
+	//platform_device_register(&sunxi_device_cedar);
+	printk("sunxi cedar version 0.1 \n");
+	return platform_driver_register(&sunxi_cedar_driver);
+}
+
+static void __exit sunxi_cedar_exit(void)
+{
+	platform_driver_unregister(&sunxi_cedar_driver);
+}
+
+module_init(sunxi_cedar_init);
+module_exit(sunxi_cedar_exit);
+
+
+MODULE_AUTHOR("Soft-Reuuimlla");
+MODULE_DESCRIPTION("User mode CEDAR device interface");
+MODULE_LICENSE("GPL");
+MODULE_VERSION(DRV_VERSION);
+MODULE_ALIAS("platform:cedarx-sunxi");
diff -Nur -X diffexclude a/drivers/staging/media/sunxi/cedar_ve/cedar_ve.h b/drivers/staging/media/sunxi/cedar_ve/cedar_ve.h
--- a/drivers/staging/media/sunxi/cedar_ve/cedar_ve.h	1970-01-01 08:00:00.000000000 +0800
+++ b/drivers/staging/media/sunxi/cedar_ve/cedar_ve.h	2023-06-09 13:37:51.352217619 +0800
@@ -0,0 +1,99 @@
+#ifndef _CEDAR_VE_H_
+#define _CEDAR_VE_H_
+
+enum IOCTL_CMD {
+	IOCTL_UNKOWN = 0x100,
+	IOCTL_GET_ENV_INFO,
+	IOCTL_WAIT_VE_DE,
+	IOCTL_WAIT_VE_EN,
+	IOCTL_RESET_VE,
+	IOCTL_ENABLE_VE,
+	IOCTL_DISABLE_VE,
+	IOCTL_SET_VE_FREQ,
+	
+	IOCTL_CONFIG_AVS2 = 0x200,
+	IOCTL_GETVALUE_AVS2 ,
+	IOCTL_PAUSE_AVS2 ,
+	IOCTL_START_AVS2 ,
+	IOCTL_RESET_AVS2 ,
+	IOCTL_ADJUST_AVS2,
+	IOCTL_ENGINE_REQ,
+	IOCTL_ENGINE_REL,
+	IOCTL_ENGINE_CHECK_DELAY,
+	IOCTL_GET_IC_VER,
+	IOCTL_ADJUST_AVS2_ABS,
+	IOCTL_FLUSH_CACHE,
+	IOCTL_SET_REFCOUNT,
+	IOCTL_FLUSH_CACHE_ALL,
+	IOCTL_TEST_VERSION,
+
+	IOCTL_READ_REG = 0x300,
+	IOCTL_WRITE_REG,
+	IOCTL_SET_VOL = 0x400,
+
+	IOCTL_WAIT_JPEG_DEC = 0x500,
+	/*for get the ve ref_count for ipc to delete the semphore*/
+	IOCTL_GET_REFCOUNT,
+};
+
+struct cedarv_env_infomation {
+	unsigned long  phymem_start;
+	unsigned long  phymem_total_size;
+	unsigned long  address_macc;
+};
+
+struct cedarv_cache_range{
+	long start;
+	long end;
+};
+
+struct __cedarv_task {
+	int task_prio;
+	int ID;
+	unsigned long timeout;	
+	unsigned int frametime;
+	unsigned int block_mode;
+};
+
+struct cedarv_engine_task {
+	struct __cedarv_task t;	
+	struct list_head list;
+	struct task_struct *task_handle;
+	unsigned int status;
+	unsigned int running;
+	unsigned int is_first_task;
+};
+
+struct cedarv_engine_task_info {
+	int task_prio;
+	unsigned int frametime;
+	unsigned int total_time;
+};
+
+struct cedarv_regop {
+    unsigned long addr;
+    unsigned int value;
+};
+
+struct cedarv_env_infomation_compat {
+	unsigned long  phymem_start;
+	unsigned long  phymem_total_size;
+	unsigned long  address_macc;
+};
+
+struct __cedarv_task_compat {
+	int task_prio;
+	int ID;
+	u32 timeout;
+	unsigned int frametime;
+	unsigned int block_mode;
+};
+
+struct cedarv_regop_compat {
+	u32 addr;
+    unsigned int value;
+};
+/*--------------------------------------------------------------------------------*/
+
+
+#endif
diff -Nur -X diffexclude a/drivers/staging/media/sunxi/cedar_ve/ve_mem_list.h b/drivers/staging/media/sunxi/cedar_ve/ve_mem_list.h
--- a/drivers/staging/media/sunxi/cedar_ve/ve_mem_list.h	1970-01-01 08:00:00.000000000 +0800
+++ b/drivers/staging/media/sunxi/cedar_ve/ve_mem_list.h	2023-06-09 12:02:57.292923413 +0800
@@ -0,0 +1,120 @@
+/*
+ *	  Filename: ve_mem_list.h
+ *	   Version: 0.01alpha
+ * Description: Video engine driver memory list management.
+ *	   License: GPLv2
+ *
+ *		Author	: yangcaoyuan <yangcaoyuan@allwinnertech.com>
+ *		Date	: 2017/04/04
+ *
+ *	This program is free software; you can redistribute it and/or modify
+ *	it under the terms of the GNU General Public License version 2 as
+ *	published by the Free Software Foundation.
+ *
+ *	This program is distributed in the hope that it will be useful,
+ *	but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *	MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *	GNU General Public License for more details.
+ *
+ *	You should have received a copy of the GNU General Public License
+ *	along with this program;
+ *
+ */
+#ifndef _VE_MEM__LIST_H
+#define _VE_MEM__LIST_H
+
+#define ion_offsetof(TYPE, MEMBER) ((size_t) &((TYPE *)0)->MEMBER)
+
+#define aw_container_of(aw_ptr, type, member) ({ \
+const typeof(((type *)0)->member)*__mptr = (aw_ptr); \
+(type *)((char *)__mptr - ion_offsetof(type, member)); })
+
+static inline void aw_prefetch(const void *x) {(void)x; }
+static inline void aw_prefetchw(const void *x) {(void)x; }
+
+#define AW_LIST_LOCATION1  ((void *) 0x00100100)
+#define AW_LIST_LOCATION2  ((void *) 0x00200200)
+
+struct aw_mem_list_head {
+struct aw_mem_list_head *aw_next, *aw_prev;
+};
+
+#define AW_MEM_LIST_HEAD_INIT(aw_name) { &(aw_name), &(aw_name) }
+
+#define VE_LIST_HEAD(aw_name) \
+struct aw_mem_list_head aw_name = AW_MEM_LIST_HEAD_INIT(aw_name)
+
+#define AW_MEM_INIT_LIST_HEAD(aw_ptr) do { \
+(aw_ptr)->aw_next = (aw_ptr); (aw_ptr)->aw_prev = (aw_ptr); \
+} while (0)
+
+/*
+ * Insert a new entry between two known consecutive entries.
+ *
+ * This is only for internal list manipulation where we know
+ * the aw_prev/aw_next entries already!
+ */
+static inline void __aw_list_add(struct aw_mem_list_head *newList,
+	  struct aw_mem_list_head *aw_prev,
+	  struct aw_mem_list_head *aw_next)
+{
+	aw_next->aw_prev = newList;
+	newList->aw_next = aw_next;
+	newList->aw_prev = aw_prev;
+	aw_prev->aw_next = newList;
+}
+
+/**
+ * list_add - add a new entry
+ * @new: new entry to be added
+ * @head: list head to add it after
+ *
+ * Insert a new entry after the specified head.
+ * This is good for implementing stacks.
+ */
+static inline void aw_mem_list_add(struct aw_mem_list_head *newList,
+										struct aw_mem_list_head *head)
+{
+	__aw_list_add(newList, head, head->aw_next);
+}
+
+/**
+ * aw_mem_list_add_tail - add a new entry
+ * @new: new entry to be added
+ * @head: list head to add it before
+ *
+ * Insert a new entry before the specified head.
+ * This is useful for implementing queues.
+ */
+static inline void aw_mem_list_add_tail(struct aw_mem_list_head *newList,
+		 struct aw_mem_list_head *head)
+{
+	__aw_list_add(newList, head->aw_prev, head);
+}
+
+static inline void __aw_mem_list_del(struct aw_mem_list_head *aw_prev,
+		struct aw_mem_list_head *aw_next)
+{
+	aw_next->aw_prev = aw_prev;
+	aw_prev->aw_next = aw_next;
+}
+
+static inline void aw_mem_list_del(struct aw_mem_list_head *entry)
+{
+	__aw_mem_list_del(entry->aw_prev, entry->aw_next);
+	entry->aw_next = AW_LIST_LOCATION1;
+	entry->aw_prev = AW_LIST_LOCATION2;
+}
+
+#define aw_mem_list_entry(aw_ptr, type, member) aw_container_of(aw_ptr, type, member)
+
+#define aw_mem_list_for_each_safe(aw_pos, aw_n, aw_head) \
+for (aw_pos = (aw_head)->aw_next, aw_n = aw_pos->aw_next; aw_pos != (aw_head); \
+aw_pos = aw_n, aw_n = aw_pos->aw_next)
+
+#define aw_mem_list_for_each_entry(aw_pos, aw_head, member) \
+for (aw_pos = aw_mem_list_entry((aw_head)->aw_next, typeof(*aw_pos), member); \
+	aw_prefetch(aw_pos->member.aw_next), &aw_pos->member != (aw_head);  \
+	aw_pos = aw_mem_list_entry(aw_pos->member.aw_next, typeof(*aw_pos), member))
+
+#endif
--- a/drivers/staging/media/sunxi/cedar_ve/cache-v7.S   2023-06-21 14:08:41.506534776 +0800
+++ b/drivers/staging/media/sunxi/cedar_ve/cache-v7.S    2023-06-21 11:25:30.627384578 +0800
@@ -0,0 +1,146 @@
+
+#ifndef __ASSEMBLY__
+#define __ASSEMBLY__
+#endif
+#include <linux/linkage.h>
+#include <asm/assembler.h>
+
+/*
+ * c code declared as follows:
+ * int flush_clean_user_range(long start, long end);
+ */
+ENTRY(flush_clean_user_range)
+       .macro  dcache_line_size, reg, tmp
+       mrc     p15, 1, \tmp, c0, c0, 0         @ read CSIDR
+       and     \tmp, \tmp, #7                  @ cache line size encoding
+       mov     \reg, #16               @ size offset
+       mov     \reg, \reg, lsl \tmp            @ actual cache line size
+       .endm
+
+    .text
+    .globl flush_clean_user_range
+    .globl flush_dcache_all
+flush_clean_user_range:
+    dcache_line_size r2, r3
+    sub        r3, r2, #1
+    bic        r0, r0, r3
+1:
+    USER(      mcr    p15, 0, r0, c7, c14, 1   )       @ clean and flush D line to the point of unification
+    add        r0, r0, r2
+2:
+    cmp        r0, r1
+    blo        1b
+    mov        r0, #0
+    dsb
+    mov        pc, lr
+
+/*
+ * Fault handling for the cache operation above. If the virtual address in r0
+ * is not mapped, just try the next page.
+ */
+9001:
+    mov        r0, r0, lsr #12
+    mov        r0, r0, lsl #12
+    add        r0, r0, #4096
+    b  2b
+ENDPROC(flush_clean_user_range)
+
+/*
+ *     flush_dcache_all()
+ *
+ *     Flush the whole D-cache.
+ *
+ *     Corrupted registers: r0-r7, r9-r11 (r6 only in Thumb mode)
+ *
+ *     - mm    - mm_struct describing address space
+ */
+ENTRY(flush_dcache_all)
+       stmfd   sp!, {r0 - r12, lr}
+       dmb                     @ ensure ordering with previous memory accesses
+       mrc     p15, 1, r0, c0, c0, 1           @ read clidr
+       ands    r3, r0, #0x7000000              @ extract loc from clidr
+       mov     r3, r3, lsr #23                 @ left align loc bit field
+       beq     finished                @ if loc is 0, then no need to clean
+       mov     r10, #0                 @ start clean at cache level 0
+loop1:
+       add     r2, r10, r10, lsr #1            @ work out 3x current cache level
+       mov     r1, r0, lsr r2                  @ extract cache type bits from clidr
+       and     r1, r1, #7              @ mask of the bits for current cache only
+       cmp     r1, #2          @ see what cache we have at this level
+       blt     skip            @ skip if no cache, or just i-cache
+       mcr     p15, 2, r10, c0, c0, 0          @ select current cache level in cssr
+       isb                     @ isb to sych the new cssr&csidr
+       mrc     p15, 1, r1, c0, c0, 0           @ read the new csidr
+       and     r2, r1, #7              @ extract the length of the cache lines
+       add     r2, r2, #4              @ add 4 (line length offset)
+       ldr     r4, =0x3ff
+       ands    r4, r4, r1, lsr #3              @ find maximum number on the way size
+       clz     r5, r4          @ find bit position of way size increment
+       ldr     r7, =0x7fff
+       ands    r7, r7, r1, lsr #13             @ extract max number of the index size
+loop2:
+       mov     r9, r4          @ create working copy of max way size
+loop3:
+ ARM(  orr     r11, r10, r9, lsl r5    )       @ factor way and cache number into r11
+ THUMB(        lsl    r6, r9, r5               )
+ THUMB(        orr    r11, r10, r6             )       @ factor way and cache number into r11
+ ARM(  orr     r11, r11, r7, lsl r2    )       @ factor index number into r11
+ THUMB(        lsl    r6, r7, r2               )
+ THUMB(        orr    r11, r11, r6             )       @ factor index number into r11
+       mcr     p15, 0, r11, c7, c14, 2         @ clean & invalidate by set/way
+       subs    r9, r9, #1              @ decrement the way
+       bge     loop3
+       subs    r7, r7, #1              @ decrement the index
+       bge     loop2
+skip:
+       add     r10, r10, #2                    @ increment cache number
+       cmp     r3, r10
+       bgt     loop1
+finished:
+       mov     r10, #0                 @ swith back to cache level 0
+       mcr     p15, 2, r10, c0, c0, 0          @ select current cache level in cssr
+       dsb
+       isb
+
+       ldmfd   sp!, {r0 - r12, lr}
+       mov     pc, lr
+ENDPROC(flush_dcache_all)
+
+/*
+ * int flush_user_range(long start, long end);
+ *
+ * flush user space range. just flush, not write back.
+ */
+ENTRY(flush_user_range)
+       dcache_line_size r2, r3
+       sub     r3, r2, #1
+
+       tst     r0, r3
+       bic     r0, r0, r3
+       USER(mcrne p15, 0, r0, c7, c14, 1)      @ clean & invalidate D / U line
+
+       tst     r1, r3
+       bic     r1, r1, r3
+       USER(mcrne p15, 0, r1, c7, c14, 1)      @ clean & invalidate D / U line
+
+1:
+       USER(mcr p15, 0, r0, c7, c6, 1)         @ invalidate D / U line
+       add     r0, r0, r2
+2:
+       cmp     r0, r1
+       blo     1b
+       mov     r0, #0
+       dsb
+       mov     pc, lr
+
+       /*
+        * Fault handling for the cache operation above. If the virtual address in r0
+        * is not mapped, just try the next page.
+        */
+9001:
+       mov     r0, r0, lsr #12
+       mov     r0, r0, lsl #12
+       add     r0, r0, #4096
+       b       2b
+ENDPROC(flush_user_range)
+
